\chapter{Untitled}
\epigraph{\textit{``Young man, in mathematics you don't understand things. You just get used to them."}}{John Von Neumann}
\epigraph{\textit{``There is no royal road to geometry"}}{Euclid}
\section*{Setting the Stage}

It is impossible to conceive of a complete description of physics that need not make use of a geometric space in which states exists.  And even those physical degrees of freedom not tied to positions or times, such as spin, naturally lend themselves to having their states placed in a geometric space.  As such, a natural starting point for understanding physical systems is first understanding the space in which the system lives, the \textit{stage} on which physics takes place.  For many years it was merely posited that physical space had the mathematical character of $\R^3$ (Euclidean space) and that time was merely another orthogonal direction making spacetime $\R^4$.  Such assumptions were poor for two reasons.  For one, a physicist should strive to make as few assumptions as possible in the pursuit of a general theory and presuming that physical space is $\R^3$ indirectly assumes many things about how physical systems behave. Second, and much more egregious, in many cases these assumptions are simply false; that is, they are inconsistent with what we experimentally observe about the universe.  But what is the way forward, then?  First, we must explore in abstract the geometries that spaces may have and how objects behave on them.  This naturally leads us to what is at present the best mathematical conception of the stage on which physics takes place, the \textbf{manifold}\index{manifold}.\\

The term manifold describes a very general class of mathematical objects.  Moreover, without certain restrictions on their properties, they may take certain pathological and non-physical forms.  As such, we will restrict our discussion to smooth manifolds, since spacetime (and other geometric spaces we care about) appears to be such a thing.  It is worth noting that perhaps, as physics progresses, we may learn that this assumption was also a mere approximation to a more fundamental description of reality.  This is a fair point, but nonetheless, this assumption is completely compatible with the physics of interest here. \\

 In order to formally define a manifold, we must first define a \textbf{topological space}\index{topological space}.  
A topological space $X$ is a set with a family of open subsets $T$ satisfying 
 \begin{enumerate}
 	\item[1)] $\emptyset$ (empty set) and $X$ are in $T$
 	\item[2)] A finite union of members of $T$ is in $T$
 	\item[3)] A finite intersection of members of $T$ is in $T$
 \end{enumerate}
 and $T$ is called the \textbf{topology}\index{topology} on $X$.  We can talk about points $x$ in the set $X$ and we say that a collection of open sets $\{O_\alpha \}$ \textbf{covers}\index{cover} $X$ if $X$ is contained in their union.  Intuitively, the information contained in such a collection of sets should be enough to talk about all of $X$.  The last thing we need is a \textbf{chart}\index{chart}, which is an open set $O_a$ paired with a continuous function $\varphi_\alpha: O_\alpha \to \R^n$ with continuous inverse.  This map is identifying each point in the subset $O_\alpha$ with a corresponding point in $\R^n$.  We can now define a manifold\\

  An $n$-dimensional \textbf{smooth manifold} is a topological space $M$ equipped with a cover $\{O_\alpha\}$ and charts $\{\varphi_\alpha\}$ such that when $O_\alpha$ and $O_\beta$ have nontrivial intersection (they overlap), $\varphi_\alpha \circ \varphi_\beta^{-1}$ is \textbf{smooth}.  Smooth here means infinitely differentiable.  This composition is called a \textbf{transition function}\index{transition function} and its smoothness guarantees that our collection of charts (called an \textbf{atlas}\index{atlas}) will allow us to define smooth functions on our manifold with no trouble. \\
  
  It is also important to stress that the reader should not at the moment think of charts as mapping identifying each point $p$ in a manifold $M$ with $n$ coordinates in $\R^n$.  The point of this construction is to have coordinate independent notions of geometry.  When working in a chart, we are identifying each point in the manifold with a corresponding point in $\R^n$, which exists independent of a set of numbers we choose to identify it with.\\
 
This definition of a manifold may still seem too abstract.  Intuitively, this definition is saying that an $n$-dimensional Manifold is a set which \textit{locally} looks like $\R^n$.  In the context of physics, by considering general manifolds, we are laxing our stipulation that space is Euclidean in its totality to a requirement that space \textit{seems} Euclidean in a small patch around any given point. For example, we know by various means, that the surface of the earth is roughly like the surface of a spherical ball.  But, ignoring irregularities in the surface like hills and mountains, at any given point on earth the surface appears flat in some patch around you.  We could, in principle, draw lines determining a collection of relatively flat patches of earth which collectively cover all of earth and then map each of these onto part of the plane, $\R^2$.  Therefore the surface of the earth is to good approximation a perfectly fine 2-dimensional smooth manifold, but we note that as it is different from the plane.  In a similar manner, its conceivable that although in 3-dimensional patches  around us physical space appears to be Euclidean, the entirety of physical space may be described by some much more interesting geometry.  It is on these non-Euclidean manifolds that much interesting and often unintuitive physics arises.\\

As we endow our geometric spaces with more  structure it is important to keep in mind at each stage the limits of what we can say about our space.  At this point, we only have information about the smoothness and \textit{topology} of our space, but we have no notion of how distances behave on our manifold.  A concrete example of this limit is that we have not provided enough structure to our notion of manifold to differentiate between an egg and a  ping-pong ball.  The surfaces of these objects are topologically the same, and differentiating between the two requires a notion of how long different curves are on their surfaces; such differentiation is facilitated by the presence of Riemann structure. We complete the section by stating our first essential concept.  **(perhaps diagram )**\\

Moreover, the reader should not presume that they are able to visualize the totality of a manifold.  The capacity to do so for an $n$-dimensional manifold is contingent upon the ability to \textit{embed} the manifold in $\R^{n+1}$, as is commonly done with the 2-sphere; we are not generally able to do this.**(tie this up or something)**  

\begin{concept}
A smooth manifold is a set which can map itself into Euclidean space in a neighborhood of any point,
\end{concept}


SEGWAY?

\section*{Better Title} 
Surely the reader is familiar with the concept of a vector field.  The geometric intuition behind a vector field $v:\R^3 \to \R^3$  in Euclidean space is that we input a position in space and get out an object at said point with a magnitude and direction.  This may describe the velocity of a particle at the point, or a force acting at such point.  But as we generalize the geometry of the spaces in which physics is taking place, it becomes natural and necessary to utilize differently defined mathematical objects analogous to (or even in special cases reducing to) those previously used.  In order to build up our new geometric machinery, we will start simple and make an effort to intuitively motivate the need for this new formalism, often by pointing out the defunctness of old ways.  Nonetheless, this modern view of geometry may never feel as intuitive as arrows in three dimensional space, and so the reader should take seriously the epigraph at the beginning of the chapter. \\

Perhaps the simplest object to conceive of existing on a manifold is a smooth function 
\begin{equation*}
	f: M \to \R
\end{equation*}
This definition should not seem surprising to the reader.  At each point in the manifold the function assigns a value and in order for it to be smooth we require that 
\begin{equation*}
	f\circ \varphi^{-1}_\alpha: \R^n \to \R
\end{equation*}
be smooth.  The collection of such functions is denoted by $C^\infty(M)$.  Perhaps the most fundamental functions are coordinates themselves!  For example, each Cartesian coordinate in $\R^3$ ($x$, $y$, and $z$) returns a value at each point in $\R^3$ corresponding to what we call the $x$, $y$, or $z$ coordinate.  Clearly these are infinitely differentiable, defining a smooth function.  We see that, working in a chart, we can then use the local coordinate functions $\{x^\mu \}$ on $\R^n$ as arguments of our function by relating 
\begin{equation*}
	f(p) = f(x^1 \circ \varphi_a, \dots, x^n \circ \varphi_a)
\end{equation*}
We will proceed writing functions in the way on the right, and omitting the composition with the chart (i.e. just writing $f(x^1, \dots, x^n)$).\\

 Slightly less intuitive is our modern definition of a tangent vector.  In order to motivate this, we must first introduce a \textbf{curve} in $M$, which is a map $\gamma: T \to M$ where $T$ is the compact interval $[0, T] \subset \R$.  For each value of some parameter in $\R$, we get a point in $M$, and so $\gamma$ is describing a path traced out in the manifold.  The reader should be familiar with curves in $\R^3$ as described by the methods of basic multivariable calculus.  In this case we were unknowingly spoiled, as $\R^3$ can be viewed both as a manifold and a vector space.  We could sloppily interchange these viewpoints of $\R^3$, representing the points on the curve as endpoints of vectors starting from the origin.  We could then acquire the tangent vector to a curve by differentiating the position vector component-wise.  When  one conceives of such a tangent vector, they cannot help but conceive of it existing as an arrow in three dimensional Euclidean space. And so one way to explain why our old Euclidean ways fall short is this: our manifold is not in general a vector space.  \textit{We cannot linearly combine two distinct points on the manifold to get something meaningful.}**(probably worthy of stressing as a concept)**  We have not equipped our space with a structure enabling comparison of information at two distinct points and so we must not subconsciously presume one.\\
 
   Moreover, we need to talk about geometric objects only making reference to the \textit{intrinsic} properties of the manifold, and without embedding our $n$-dimensional manifold in $n+1$ - dimensional Euclidean space. How then can we view our tangent vector as an element of the vector space $\R^{m}$? We cannot in general, but I ask the reader, what good are tangent vectors?  That is, what do we want from a tangent vector?  A tangent vector to a curve provides us with the information necessary to determine the rate of change of a function as its argument moves along the curve.  In the old way mentioned above, this is done by taking a directional derivative of a function along an arrow in $\R^3$, but on our arbitrary manifold, the output of $\gamma$ is not a vector, but a point!  Let us proceed demanding the desired \textit{result} from our tangent vector and see if we can construct a consistent geometric viewpoint.  We simply demand that the tangent vector to $\gamma$ at point $p = \gamma(t)$ be the map 
   \begin{equation*}
   	\gamma^\prime(t): C^\infty(M) \to C^\infty(M)
   \end{equation*}
   sending the smooth function $f$ to its derivative 
\begin{align*}
	\frac{d}{dt}(f(\gamma(t)))
\end{align*}
the collection of such maps for all possible curves $\gamma$ at a point $\gamma(t)$ is called the Tangent Space at $\gamma(t)$, denoted $T_{\gamma(t)}M$.  We will formally define this later.  Before seeing how this lends itself to a generalized notion of vector fields on manifolds, we must first discuss how mappings between manifolds give rise to mappings between the objects that exist on them. 
\subsection*{Covariance, Contravariance, and Duality}
Covariance, contravariance, and duality are very general concepts utilized in category theory, but in order to remain geometrically grounded we will restrict ourselves to a discussion of their application to manifolds.  Consider a smooth map $\varphi: M \to N$ between manifolds $M$ and $N$.  Such a map need not be invertible in general.  If we have a smooth function $f$ defined on $N$, $\varphi$ can allow us to compute this function on $M$ by considering the composition $\varphi^*f = f\circ \varphi : M \to \R$.  This operation $\varphi^*: f \to f\circ \varphi$ is called a \textbf{pullback}\index{pullback} because it brought the function defined on $N$ \textit{back} and defined it on $M$.  Notice that since $\varphi$ is not in general invertible, we are not in general able to send functions from $M$ forward to $N$ by composing with $\varphi^{-1}$ (which may not exist).  Because the map that goes forward from $M$ to $N$ gives rise to a backward map  $\varphi*: C^\infty(N) \to C^\infty(M)$ we say that smooth functions are \textbf{contravariant}\index{contravariant} objects.  Now, there exist other objects which, unlike functions, give rise to maps that would go the same way as $\varphi$.  Consider a map $v:C^\infty(M)\to C^\infty(M)$.  This $v$ takes a smooth function on $M$ and sends yields another smooth function on $M$.  We may define $(\varphi_*v)(f) = v(f\circ\varphi)$ where $f$ is a smooth function on $N$.  Therefore, $\varphi_*$ took a $v$ defined on $M$ and yielded a map $\varphi_* v: C^\infty(N) \to C^\infty(N)$ defined on $N$.  Since $\varphi_*$ maps in the same direction as $\varphi$ we call it a \textbf{pushforward}.  In practice, when we actually perform analysis on manifolds, we will be manipulating coordinates $\{x^\mu\}$ on $\R^n$, but technically speaking we are working with the \textit{pullbacks} of these coordinate functions by our charts, since \textit{those} are what are defined on the manifold.  There exists a \textit{duality} between contravariant and covariant objects, which can be summarized abstractly.  \\

If we have a vector space $V$, we may consider the space of linear maps $v^*: V \to \R$.  This too comprises a vector space $V^*$ which we call the \textbf{dual space}\index{dual space}.  If we have a map $\varphi:V \to W$ we can see that, being careful about composing maps properly, this gives rise to $\varphi^*: W^* \to V^*$.  Maps between vector spaces produce pullbacks between the dual spaces.  With this in mind, we are prepared to discuss vector fields and differential forms.\\
**(excercise on basis inducing a corresponding dual basis and $(V^*)^* = V$ )**

\section*{TITLE VECTOR FIELDS, FORMS}
\subsection*{Vector Fields}
Given how organically the concept emerged of a tangent vector  being a map differentiating functions, it is natural to generalize the concept to vector \textit{fields} on a manifold.   A vector field will be a map which sends smooth functions to a directional derivative.  Since we define our smooth functions using local coordinate functions on a chart, we can compute this derivative as
\begin{align*}
	vf = \sum\limits_\mu v^\mu \frac{\partial}{\partial x^\mu}f(x^1, ..., x^n)
\end{align*} 
where we will call the $v^\mu$ the components of the vector $v$ written with respect to a basis $\{\partial_\mu \}$ of partial derivative operators with respect to coordinates on $\R^n$.  The $\{\partial_\mu\}$ serve as a basis for vector fields in $\R^n$ in the sense that any function $f$ on $\R^n$ can be taken to any of its possible directional derivatives by taking a unique linear combination 
\begin{align*}
	v^\mu\partial_\mu = \sum\limits_\mu v^\mu \frac{\partial}{\partial x^\mu}
\end{align*}
where the left side is short hand meant to represent the right side in accordance with Einstein summation notation.  But in the spirit of generality, we must provide a coordinate independent definition of a vector field, which will naturally be more abstract.  A \textbf{vector field}\index{vector field} is a map $v: C^\infty(M) \to C^\infty(M)$ satisfying 
\begin{enumerate}
	\item $v(\alpha f+\beta g) = \alpha v(f)+\beta v(g)$
	\item $v(fg) = v(f)g+v(g)f$
\end{enumerate}
for $f, g \in C^\infty(M)$ and a scalar $\alpha$.\\

The essence of a vector field as a derivative operator is captured by the second property, the so called Liebnitz property (essentially the chain rule).  In this way, the presence of vector fields which differentiate functions is not contingent upon the details of a coordinate system.  Given this construction, we can now generally define a tangent vector.\\

  The vector $v_p$ is a map taking $f$ to its derivative at $p$, a number which would be acquired by feeding in $p$ to the resultant function yielded by $vf$.  We say $v_p$ is a tangent vector at $p$ and the set of all such vectors is the \textbf{Tangent Space}\index{Tangent Space} at $p$, $T_pM$.  If we have an embedding of our $n$-dimensional manifold, we can identify the components $v_p^\mu$ of the vector field at a point $p$ as the components of traditional vector in $\R^n$ and as such place a copy of $\R^n$ tangent to $M$ at $p$ and draw the corresponding arrow.  In this way, we would get a field of arrows on our manifold.  But to indulge this practice is to betray the essence of our new concept of a vector field; our definition provides a useful form of a vector that does not need an embedding of the manifold in higher dimensional Euclidean space.  Nonetheless, it is nice to see that this advanced machinery is consistent with our old intuitions in special cases.\\
  

\subsection*{Forms}
In order to motivate forms, it is valuable to begin by asking how we might try to perform integration on a manifold or a submanifold (which is merely a manifold whose points are contained in a greater manifold).  The simplest example we might consider is integrating over a curve $\gamma: [0, 1] \to M$.  We want our integration to correspond to a summation of tiny scalar values, computed along a series of infinitesimal displacements conceptually given by the tangent vectors to $\gamma$.  But we cannot merely sum vector values to get a scalar, so we see already the need for a map $\omega$ from tangent vectors to real numbers.  This map $\omega$ assigns at each point in the manifold a map 
\begin{equation*}
	\omega_p: T_pM \to \R
\end{equation*} 
which lives in $T^*_pM$, the \textbf{cotangent space}\index{cotangent space}, which is dual to the tangent space as its elements are linear maps from tangent vectors to scalars.  These are also called \textbf{one-forms}\index{one-forms}, and just as we have vector fields, we can have form fields, which assign a covector in the cotangent space at each point in the manifold.\\

  More concretely, we can consider the one-form $df$ where $f$ is a smooth function on $M$.  Though there is delicate formalism which we need to develop, we still intuitively identify $df$ with the differential of $f$.  We have that $df$ is defined by 
\begin{equation*}
	df(v) = vf
\end{equation*} 
and so we can consider integration 
\begin{equation*}
	\int\limits_\gamma df = \int_{[0, 1]} df(\gamma'(t)) dt = \int\limits_{[0, 1]} \frac{d}{dt}f(\gamma(t)) dt
\end{equation*}
where we use the notation on the right to indicate that we are performing a typical Riemann sum.  So what occurs here is that we define our form $df$ and it eats a vector, yielding a real number.  Intuitively this is the infinitesimal change in $f$ for an infinitesimal change in $\gamma$.  That is to say, $df$ is \textit{not} the scalar value of small change itself, but a map which gives us this small change if we feed it a vector.  By making these identifications, we see that forms are the things which we integrate over manifolds, or more generally, manifolds with boundary which we will define later.  As such, forms merit a great deal of study and so we must proceed unpacking their machinery. \\

If we recall that a basis $\{e_i\}$ for a vector space $V$ directly induces a basis $\{e^j\}$ for the dual space $V^*$ then we recognize that their should be such a basis induced on the space of forms.  Indeed, given the basis $\{ \partial_\mu\}$ for vector fields on $\R^n$, we have a dual basis of forms $\{ dx^\nu \}$ satisfying 
\begin{align*}
	dx^\nu(\partial_\mu) = \delta^\nu_\mu
\end{align*}
where the symbol on the right is the Kronecker delta\index{Kronecker delta}, which is 1 when $\mu = \nu$ and 0 otherwise.  Being a basis, we can express any one-form in a chart as a linear combination 
\begin{equation*}
	\omega = \omega_\mu dx^\mu
\end{equation*}
But we will not stop at mere one-forms, we must generalize in order to integrate over higher dimensional manifolds.\\

 The reader should recall from multivariable calculus that when integrating over surfaces in $\R^3$ one would parameterize the surface using a map $\varphi:[0, 1]^2 \to \R^3$ and the derivatives with respect to each parameter would each define tangent vector.  We would identify with the norm of the cross product of these tangent vectors a small area element on the surface, and we could then integrate functions over these infinitesimal areas.  We can intuit that since a $k$-manifold will be parameterized by $k$-parameters, we need something that can eat $k$ tangent vectors and give us scalars to sum over our manifold.  Out of necessity we therefore develop $k$-forms.\\
 **(maybe note that mapping from unit square implies compact surface)**\\
 
 We need an operation which can allow us to build $k$-forms from our humble one-forms, and we will denote it by $\wedge$.  But how can we get a unique operation to combine them?  For one, there is a requirement we would like satisfied by these forms.  Given our new formalism, our tangent vectors merely get eaten by the forms, so some of the work that was done by interesting operations between vectors must now be done by forms.  In particular, 3-vectors had an algebra where the operation was the cross-product.  We still want to have an analog of a cross-product in our new formalism. Therefore, like the cross product, we demand antisymmetry from our $\wedge$.  That is, for any two $v, w \in V$ where $V$ is the vector space of one-forms, we demand 
 \begin{equation*}
 	v \wedge w = -w \wedge v
 \end{equation*} 
we call this operation the \textbf{wedge product} and with it we construct something called the \textbf{exterior algebra}\index{exterior algebra}, $\Lambda V$.  We can build $k$-forms by wedging any two $n$ and $m$ forms together (satisfying $n+m = k$) and taking linear combinations of $k$-forms with functions as coefficients.  This will now allow us to integrate over $k$-manifolds, but before we attempt to do so, we need to discuss another operation on forms and some additional structure on our manifold.   **(rigorous definition with rules)** **(Prove $\omega \wedge \omega = 0$ as excercise)**\\
Remaining\\

We saw before that we wanted $df$ to represent a one-form which emerged from a smooth function.  Therefore we want $d$ to be an operation which sends smooth functions to one forms.  But really, we can identify functions as 0-forms, so we are motivated to make $d$ a general operation from $k$ forms to $k+1$ forms.  We will call this the \textbf{exterior derivative} and as a derivative we would like it to obey
\begin{enumerate}
	\item $d(\alpha\omega+\beta\mu) = \alpha d\omega+\beta d\mu$ for forms $\omega, \mu$ and scalars $\alpha, \beta$
	\item $d(\omega\wedge \mu) =  d\omega \wedge \mu +(-1)^p\omega\wedge d\mu$ for $p$-form $\omega$ and one-form $\mu$.
	\item $d(d\omega) = 0$ for all forms $\omega$
\end{enumerate}
These properties uniquely define the directional derivative.  We can write a simple algorithm for taking a general exterior derivative as a result of these properties.  Consider a multi-index $I$ which is a list of $p$ indices, where each index corresponds to those labeling the coordinate functions in a chart (and hence the basis of forms and vector fields).   We can write a $p$-form as 
\begin{align*}
	\omega = \omega_I dx^I
\end{align*} 
where the summation is over all possible $\textit{combinations}$ of $p$ indices.  We chose to say combinations because the $dx^I$ for different orderings of indices in $I$ can only differ by a sign, so we just combine the functions for different permutations into one $\omega_I$ coefficient.  We then have that 
\begin{align*}
	d\omega = (\partial_\mu \omega_I)dx^\mu \wedge dx^I
\end{align*} 
Now, this generalization of the $d$ operation may at first appear contrived or useless, but we again call upon the intuition that in our new framework of geometry, we are transferring some of what was physically expressed by operations on vectors, to operations on \textit{forms}.  Also, despite this rather involved generalization to $k$-forms, we can still preserve our old notion of $d$ as representing small change.  \\

**(form wedge parallelipiped somewhere)**\\

  Consider a chart on an $n$-dimensional manifold $M$.  We introduce a \textit{fixed} index, which is just a collection $(i_1, \dots, i_p)$ where the $i$'s are ordered and correspond to the indexation of the basis of coordinate functions and hence one-forms in our chart.  We now consider a $p$-form of the form
  \begin{align*}
  	\omega = \omega_{(i_1, \dots, i_p)} dx^{(i_1, \dots, i_p)}
  \end{align*}
  where 
  \begin{equation*}
  	dx^{(i_1, \dots, i_p)} = dx^{i_1}\wedge \dots \wedge dx^{i_p}
  \end{equation*}
  We can consider 
  \begin{align*}
  	d\omega = (\partial_\mu\omega_{(i_1, \dots, i_p)})dx^\mu \wedge dx^{(i_1, \dots, i_p)}
  \end{align*}
  where the nonzero terms result in a sum over basis forms $dx^\mu$ not in the $p$-form $dx^I$ (otherwise we have a repeated wedge of basis forms which equals 0).  Now, consider a vector 
  \begin{align*}
  	v = v^\nu\partial_\nu
  \end{align*}
 where the $v^\nu$ are only nonzero for $\nu$ not in the index.  Then we see that we get
 \begin{align*}
 	d\omega(v) = v^\mu(\partial_\mu \omega_{(i_1, \dots, i_p)})dx^\mu(\partial_\mu)\wedge dx^{(i_1, \dots, i_p)}  = (v \omega_{(i_1, \dots, i_p)})dx^{(i_1, \dots, i_p)}
 \end{align*}
 so the effect of this was to essentially get a directional derivative, the change, of the form's coefficient $\omega_{(i_1, \dots, i_p)}$ in the directions excluded from the index.\\   
 
 *(Diagram of this with parallelogram moving)*\\
 
 This example is clearly not a general result, but it is nonetheless comforting to see that a notion of exterior derivative as small changed can be recovered, and that we are \textit{generalizing} our old intuitions, not completely trashing them.\\
 
 **(Introdce tensor product of vector spaces and then tensor fields )**\\
 
 **(Exterior algebra is subspace of tensor products)**
\section*{Stoke's and Integration}
We have now built up the geometric intuition and mathematical sophistication necessary to discuss a very powerful and general theorem in differential geometry.  The statement concerns a form $\omega$ defined on a mandifold with boundary, $M$. \\

A manifold with boundary differs from a manifold in that we allow the charts to map to $H^n := \{(x^1, \dots, x^n) \hspace{2mm} | \hspace{2mm}  x^n \geq 0\}$.  The requirement that transition functions be smooth becomes slippery around the boundary\\
**(define this better)**\\

The statement is
\begin{equation*}
	\int\limits_{\partial M} \omega = \int\limits_M d\omega
\end{equation*}
in English, we are saying that the integral of the form over the boundary is equal to the integral of its exterior derivative over the manifold.  Even in plain english, this may not be entirely intuitive yet.  In order to understand why this equality should make sense, we should recall that as we evaluate the integral of a form, we are summing evaluations of the form over small movements through the manifold.  We also recall that the exterior derivative of a form intuitively yields a small change in the form evaluated in the direction of those small movements, represented by tangent vectors it ``eats".  In this sense what Stoke's theorem is saying is the following: the sum of the small changes of a form \textit{throughout} a manifold is equal to the sum of the form's values over the boundary of the manifold.  That is, when we sum all the small changes throughout the manifold, the result is what's left on the boundary, just the \textit{net} change.  The power of this theorem is apparent when one realizes that the fundamental theorem of calculus, Green's theorem, and divergence theorem are all special cases of this general statement of Stoke's theorem.\\

**(Do these examples of Green's etc.)**\\

**(Possibly geometric picture with sum over cubes cancelling leaving flat parallelograms on surface)**

\section*{Coord Transforms}
Perhaps the greatest impetus for the use of this geometric formalism in physics is the notion that we need coordinate independent laws of physics.  That is, when we apply a coordinate transformation on our manifold, we want the mathematical statements or equalities which we call physical laws, to say the same thing they did before.  In the context of relativity, coordinate transformations correspond to changes of reference frame, and this condition says that we do not want the laws of physics to be different for two different observers.  Therefore, the study of coordinate transformations on manifolds merits a bit of attention.\\

There are two ways to apply coordinate transformations on a manifold, actively and passively.  We will focus on the active way.  Active coordinate transformations are represented by $\textbf{diffeomorphisms}\index{diffeomorphism}$, which are isomorphisms of manifolds.  If we have a map $\varphi: M \to N$, the condition for it to be a diffeomorphism is that $\varphi$ is smooth with smooth inverse;  this means $\varphi$ can essentially pair each point in $M$ with a point in $N$ smoothly.  And so, if two manifolds $M$ and $N$ are diffeomorphic, they are really the same manifold being represented in two different ways.  Perhaps the reader is not convinced of this, but we will show that diffeomorphisms are equivalent to our old notion of coordinate transformation.\\

  To make the equivalence of diffeomorphic manifolds more concrete we consider an example.  Suppose I have a 2-sphere on which I draw a great circle to create two hemispheres and a natural choice of two poles which I call Northern and Southern.  If I have a map switching each point $p$ in the Northern hemisphere with the closest point $q$ in the Southern hemisphere that has the same distance from the South pole as $p$ has from the North pole (a sort of "reflecting" about the equator) **(figure )**, all we have done is exchanged the two hemispheres.  We have completely preserved the structure of the manifold, just changed the way the coordinate functions choose to talk about it.  It should then be clear that the things of physical interest on our manifold are those unaffected by diffeomorphisms.  First, we will consider how a vector $v$ changes under a diffeomorphism.\\
  
  We introduce manifolds $M$ and $M'$ which are diffeomorphic, so two different representations of the same thing, and denote objects defined on each space as primed and unprimed, respectively.  As such, there exists a diffeomorphism $\varphi$ which allows us to pushforward and pullback objects.  We say the pushforward of $v$  will be sent to $v'$ and we know it obeys
  \begin{equation*}
  	v'(f') = (\varphi_* v)(f') = v(f'\circ \varphi)
  \end{equation*} 
  but since this is a diffeomorphism, we are free to use $\varphi^{-1}$ and say that $f'$ is $f\circ \varphi^{-1}$, effectively the pushforward of the function $f$ on $M$. And so
  \begin{equation*}
  	v'(f') = v(f\circ \varphi^{-1}\circ \varphi) = v(f)
  \end{equation*}
  and so the action of the vector field on functions is invariant under the diffeomorphism, and hence invariant under coordinate transformations.  It is sensible that the action of vectors on functions is invariant, but \textit{something} must be changing, right?  What changes are the the coordinate vector fields and the vectors components; they do so in essentially opposite ways so that their changing cancels out to leave the \textit{whole} object invariant.  When we are doing actual physics later on, we are going to want to know how these components change.  To do this, we will develop the tensor transformation law, but first we must define tensors.
  
  \section*{Tensors}
  The starting point for defining tensors, is defining the \textbf{tensor product}\index{tensor product} of vector spaces.  We begin with an abstract, coordinate independent definition.  Given vector spaces $V^1, \dots, V^n$ we can consider their cartesian product, $V^1 \times \dots \times V^n$, merely the set of ordered pairs $(v^1, \dots, v^n)$.  We introduce a space $	V^1 \otimes \dots \otimes V^n$ whose elements are of the form 
  \begin{equation*}
  	v^1\otimes\dots \otimes v^n
  \end{equation*}
   We define the tensor product by its universal property.  That is, when given a multilinear map $\phi: V^1 \times \dots \times V^n \to Z$ (a map linear in each "slot")
   \begin{align*}
   	&\phi(\alpha v^1+\beta w^1, \dots, \gamma v^n +\delta w^n) =\\
   	 & \alpha \gamma \phi(v^1, \dots, v^n)+\alpha \delta f(v^1, \dots, w^n)+\beta \gamma f(w^1, \dots, v^n)+\beta\delta f(w^1, \dots, w^n)
   \end{align*}
   we can find a multilinear map $\varphi: V^1 \times \dots \times V^n \to V^1 \otimes \dots \otimes V^n$ and find that there exists a unique \textit{linear} map $F:V^1 \otimes \dots \otimes V^n \to Z$ such that 
   \begin{equation*}
   	F(v^1\otimes\dots \otimes v^n) = f(v^1, \dots, v^n)
   \end{equation*}
   We say that $F$ is a map over the tensor product of these vector spaces.  We can then see that a given bases $\{e^1_i\}, \dots, \{e_i^n\}$ for these vector spaces, we have that $\{e^1_i\otimes \dots \otimes e^n_i\}$ must be a basis for the tensor product space.  
   This definition appears devoid of intuitive motivation, and so we now try to make this construction seem more natural.  \\
   
   One perspective to take on what we have done in constructing our tensor product space is to interpret tensoring as introducing \textit{coupling} between elements of the different vector spaces.  Indeed this is a useful notion when applying tensor products to quantum mechanics.  What we mean by coupling is that two tensored vectors $v\otimes v'$ behave now as a collective object, which is dependent on how each sub-object (each vector in the tensor product) changes.  To illustrate this notion more explicitly, we will compare $V\otimes W$ and $V\times W$ for vector spaces $V$ and $W$.  If I have 
   \begin{equation*}
   	(v_1+v_2, w_1+w_2) \in V\times W
   \end{equation*} 
     **(maybe even talk about how this is a direct sum)**\\
     I can decompose this as 
     \begin{equation*}
     	(v_1, w_1)+(v_2, w_2)
     \end{equation*}
     so we can independently separate each component.  I can separate vectors in $V$ and in $W$ separately without worrying about any coupling or interaction between vectors in the two spaces.  This is why we used Cartesian products to describe velocity and acceleration vectors; we have the physical intuition that forces applied in the $x$ direction of motion should not affect my $y$ direction of motion.  Now consider 
     \begin{align*}
     	(v_1+v_2)\otimes(w_1+w_2) &= v_1\otimes \omega_1+v_2\otimes w_2+v_1\otimes w_2 +v_2\otimes w_1 \\
     	&\neq v_1\otimes w_1+v_2\otimes w_2
     \end{align*}
     There are cross terms here emerging from the definition of our tensor product.  These cross terms are a manifestation of the coupling, which is the essential property of the tensor product.\\
     
     \begin{concept}
     \textit{The tensor product of vector spaces can be viewed as the space of coupled elements of each vector space.  A map which is multilinear over the Cartesian product extends to a linear map over the tensor product because the tensoring the vectors packages them into a single, pre-coupled object.}
     \end{concept}
This perspective will prove useful in our use of tensor products in quantum mechanics, but in order to consider tensors in relativity, we will need to consider the tensor products of linear maps.  Given a set of linear maps 
   \begin{equation*}
   	L^i: V^i\to W^i
   \end{equation*}  
   We have tensored maps 
   \begin{align*}
   	L^1 \otimes \dots \otimes L^n: V^1 \otimes \dots \otimes V^n \to W^1 \otimes \dots \otimes W^n
   \end{align*}
   which are linear over the tensor product space and obey 
   \begin{equation*}
   	L^1 \otimes \dots \otimes L^n(v^1\otimes\dots \otimes v^n) = L^1(v^1) \otimes \dots \otimes L^n(v^n)
   \end{equation*}
   These maps can also be viewed as multilinear maps over the Cartesian product of the vector spaces..  We will now use this concept and the duality of vector spaces to construct tensor fields.  Recall that given a vector space $V$, the dual space $V^*$ is the space of linear maps from $V$ to $\R$.  As such, we can identify an element of 
   \begin{equation*}
   	V^1\otimes \dots \otimes V^r\otimes V^{*^1}\otimes V^{*^s}
   \end{equation*}
   as a linear map 
   \begin{equation*}
   	T: V^{*^1}\otimes \dots \otimes V^{*^r}\otimes V^1\otimes V^s \to \R
   \end{equation*}
   where we note that $n$ tensor products of $\R$ is just $\R$.  Such an object is called a rank $(r, s)$ \textbf{tensor} \index{tensor}.  This construction straightforwardly extends to smooth tensor fields on manifolds.\\
   
    Given a manifold $M$, we may consider the tangent and cotangent spaces at $p$, $T_pM$ and $T^*_pM$ respectively.  These are vector spaces, and so we may consider objects in the tensor product of $r$ copies of the tangent space and $s$ copies of the cotangent space.  A tensor field will assign an element in this tensor product space at each point $p$ in the manifold $M$.  Given our coordinate bases for the tangent and cotanget spaces in a manifold, we can expand a tensor as
    \begin{equation*}
    	T = T^{\mu_1\dots \mu_r}_{\nu_1 \dots \nu_s}\partial_{\mu_1}\otimes \dots \partial_{\mu_r}\otimes dx^{\nu_1}\otimes \dots \otimes dx^{\nu_s}
    \end{equation*} \\
    where the smooth functions in front are the components of the tensor.  Now that we have tensor fields on our manifold, we can formulate the tensor transformation law.\\
    
    In order to formulate the tensor transformation law it is now more straightforward to work with passive coordinate transformations.  In this case, we merely change the coordinate functions on $\R^n$ that we use in a chart.  As we transform from the \\
    
    **(tensor transformation law)**\\
    
**(contraction etc.?)**\\

**(exterior algebra as anti-symmetric subspace?)**\\
\section*{Metric title}
In order to put manifolds to use in physics, in particular general relativity, we need to define something called a metric. In the sense of stripping away old Euclidean assumptions and generalizing, the metric allows us to generalize the notion of a scalar or dot product.  In particular, it gives us a way to talk about the magnitudes of vectors.  In Euclidean space we would multiply the components of two vectors in order to obtain their dot product, and not think much about it.  As such, the magnitude of a vector was the square-root of the sum of the squares of its components.  But implicit in this operation was a choice of metric, we are free to choose \textit{how} we want infinitesimal distances to vary across points in our manifold.  We will motivate the need for the distance function but making an effort to compute the length of a curve.\\

Consider two points $p$ and $q$ in a manifold.  Call the function yielding the distance between them for a given path $s$.  We then identify the exterior derivative $ds$ with a tiny distance change along the curve.  We would like to integrate the form $ds$ in order to get a length for the curve $\gamma: [0, 1] \to M$.  In  accordance with the integration formalism we have developed so far, we will write this as 
\begin{equation*}
	\int_\gamma ds = \int_{[0, 1]} ds(\gamma'(t)) dt = \int_{[0, 1]}\frac{d}{dt}s(\gamma(t)) dt
\end{equation*}
So, in order to compute this integral, we need a way for $ds$ to take a tangent vector and give us an infinitesimal change in distance along the curve, or a way to get the instantaneous rate of change of the distance function at each point along the curve.  There is no \textit{right} way to define the tiny changes in distance on an arbitrary manifold given only the structure that we have built so far.  In fact, the way that small distances vary on a manifold is how we differentiate between manifolds which are topologically identical.  As such, we need to define on our own how $s$ or $ds$ will determine these distances; we are actually going to define the one-form $ds$ by running two copies of $\gamma$ through a two-form $g$.  We write
\begin{equation*}
	ds(\gamma'(t)) = \sqrt{g(\gamma'(t), \gamma'(t)) }
\end{equation*}
and we call $g$ the \textbf{metric tensor}\index{metric tensor} on the manifold.  It is the object which tells us how to measure tiny distances.  Really, $g$ is the metric tensor \textit{field}, defined over the entirety of $M$.  At each point $p$ in $M$, $g$ assigns a metric tensor $g_p: T_pM \times T_pM \to \R$, taking two vectors and returning a scalar.  Before proceeding, we will now rigorously define a semi-Riemannina metric.\\

A semi-Riemannian metric is a bilinear map 
\begin{equation*}
	g: V\times V \to \R
\end{equation*}
obeying 
\begin{enumerate}
	\item[$\cdot$] $g(v, w) = g(w, v)$
	\item[$\cdot$] $g(v, w) = 0$ for all $w \in V$ if and only if $v = 0$  
\end{enumerate}
these conditions are called symmetry and non-degeneracy, respectively.  Indeed, as $g$ is bilinear in the Cartesian product, it naturally extends to being defined as a rank (0, 2) tensor.  We write
\begin{equation*}
	ds\otimes ds = g_{\mu\nu}dx^{\mu}\otimes dx^{\nu}
\end{equation*}
for the metric tensor in a basis of coordinate one-forms.  As is the case with a tensor field, the metric components can vary from point to point on the manifold.  It is this added Riemann structure on the manifold that lets us differentiate between the egg and the ping-pong ball!   On the ping-pong ball, from a given point, the change in small distances is not uniform over directions, whereas they are on the sphere.  Aside from its use as implied its motivation, the metric is a useful algebraic tool for index manipulation.  We can get a dual vector from a vector and vice versa by``raising" and ``lowering" with the metric.\\

**(excercise showing $v^a g_{ab} = v_b$ )**

**(Notion of tangent bundle?)**
 
 \section*{Derivatives Section}
 Up to now we have built up a great deal of structure on our manifold, but there still remain some physically useful notions that do not have the capacity to discuss with the tools at hand.  For example, we know how functions change as they move along curves, but what about a vector or even a tensor?  We have no notion of how to propagate a vector along a curve because we have no notion of how to take a directional derivative of a vector.  The mathematical apparatus which allows us to talk about these things is called a \textbf{connection}\index{connection} and with it comes a \textbf{covariant derivative}\index{covariant derivative}.\\ 
 
 **(Lie derivative-> what's the difference between the two?)**
 
   
 
  



