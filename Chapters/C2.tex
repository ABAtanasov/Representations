%!TEX root = /Users/Alex/Work/Representations/Master.tex
\documentclass[../master.tex]{subfiles}

\begin{document}
	
\chapter{New Horizons Developed}\thispagestyle{empty}
	
	\section{The Manifold} % (fold)
	\label{sec:the_manifold}
	\index{Manifold|(}
	Several thousand years ago, the first sentient human beings noticed that the landscape of the earth looked flat, and seemed to stretch out infinitely far in every direction. It is perhaps from this observation that the Euclidean plane was first conceived, and indeed it is from the fact that the earth looked like Euclid's 2D plane that geometry got its name to literally mean ``measuring the earth''. But the fact is that the earth is \emph{not} a flat plane, stretching out infinitely. It turned out to be a sphere. What is true, however, is that \emph{locally}, the geometry of the earth looks very similar to that of Euclidean space. 
	
	And now in modern times, as we look out into the cosmos and see them stretching out in every direction, our first human bias creeps in and tells us ``this thing must be infinite, stretching out in every direction". Just as people thought the world was $\mathbb R^2$ in ancient times, in this age we entertain the thought that our universe could be three-dimensional Euclidean space $\mathbb R^3$. Indeed, most of the time when we do simple classical physics, we embed our system into a space that is $\mathbb R^3$ and work there. It is an easy space to work in. 
	
	But just as the earth's surface looked \emph{locally} like Euclidean 2-space but in fact turned out to globally be wildly different, we should not be surprised if it turns out that the universe, despite locally looking Euclidean, has wildly different global structure. 
	
	This is exactly what a manifold\index{Manifold} intuitively is: an object that at each point locally resembles Euclidean space. The property of being locally Euclidean is similar to the property that differentiable functions have of being locally linear. It allows us to use calculus on them to reduce nonlinear objects to linear ones locally. 
	
	\begin{concept}
	\textit{A manifold $M$ is a set of points which, in the neighborhood of every point, locally looks like euclidean space}
	\end{concept}
	
	A line is a one-dimensional manifold (in fact it \emph{is} a Euclidean space).  The circle is a one-dimensional manifold locally resembling a line, and so are ellipses, parabolas, hyperbolas, and the graph of any smooth function. A sphere is the two dimensional manifold that ancient humans mistook for the Euclidean plane itself. The Mobius strip is also a two-dimensional manifold. Although globally it is a twisted band, locally it too looks like two-dimensional Euclidean space. Every geometric object referred to as a ``curve'' or a ``surface'' has been an example of a manifold this whole time. 
	
	In this chapter, we work towards building the language necessary to formally define what we mean by a manifold. First, we reinforce the intuitive ideas through examples.
	
	% section the_manifold (end)
	
	\section{Examples of Manifolds} % (fold)
	\label{sec:examples_of_manifolds}
	
	\begin{example}
		The sphere is a two-dimensional manifold
	\end{example}\index{Sphere}
	
		This is the classic example of a manifold that isn't just $\mathbb R^n$. As we have said before, at every point on the sphere, things look locally like $\mathbb R^2$. Throughout this text, the sphere will often be our first go-to setting when we want to take concepts from Euclidean space and generalize them to manifolds. It is easy to visualize things on this space, so often the first question to ask when generalizing something is ``well, how would this thing look on the sphere?''. Of course for this reason ellipsoids, paraboloids, and hyperboloids are all also manifolds. Similarly familiar objects like cylinders, tori, or tori with multiple holes are also manifolds.
		
		Every point on the sphere looks exactly the same as any other. This is a highly symmetric manifold, that will later be referred to as a \emph{homogenous space}\index{Homogenous Space} because of this property.
	
	\begin{example}
		Manifolds need not be connected. The disjoint union of two manifolds is also a manifold.
	\end{example}
	
	Consider the set consisting of two separate spheres. Since each sphere individually is a manifold, then any point in this disjoint union belongs to one of the spheres, and so it has a locally euclidean neighborhood.
	
	\begin{example}
		The graph of the curve $y=f(x)$ where $f$ is a smooth function defines a one-dimensional manifold.
	\end{example}
	
	Because $f$ is smooth, we know that every point of the graph of the curve will locally look like its tangent line. Since the tangent line is precisely one-dimensional Euclidean space, every point on the curve looks locally Euclidean. Indeed, we only needed $f$ to be differentiable.
	
	\begin{example}
		The set of points forming the graph of $y = f(x_1, \dots, x_n)$ defines an $n$-dimensional manifold when $f$ is smooth.
	\end{example}
	
	The exact same argument as before holds, except with a tangent line generalized to a tangent plane, etc. Locally at each point the set looks like Euclidean $n$-space. The idea of an $n$-dimensional ``tangent \emph{something}" at each point $p$ on a manifold $M$ that generalizes the notion of a tangent line or plane to higher dimensions, will be made precise in coming sections by talking about the \emph{tangent space}\index{Tangent Space} of $M$ at $p$, $T_p M$.

	
	\begin{example}
		The figure-eight and the cone are not manifolds.
	\end{example}
	
	\todofig{Figure 8 and cone aren't manifolds}
	
	Both of these objects have a ``cusp-like" point where multiple lines intersect. Zooming in near that point will preserve these cusps, and so the space does not look like Euclidean space near that point. There is no tangent space for the figure eight at the point of intersection that looks like a line: it would look like two lines intersecting. Similarly for the cone, there would not be a tangent space at the cusp that looks like a plane. At that point, the manifold looks like a continuum of intersecting lines. Intuitively, then, manifolds cannot have sharp cusps. As a result, the cube and triangle are not examples of manifolds. \\
	
	The power of manifolds lies in the fact that not only are geometric objects manifolds, but so are many of the algebraic objects that we have been working with. 
	
	\begin{example}\label{ex:secret_torus}
		Consider a two-dimensional parallelogram, with opposite sides identified as the same. This is a manifold.
	\end{example}\index{Torus}
	
	\todofig{Parallelogram with opposite sides identified}
	
	This is like a room, where if you exit on one side, you come back on the other side. The one dimensional version of this is a circle, and we will show how this space can be thought of as a ``product of circles'' or a ``circle of circles'' in two dimensions. Clearly this parallelogram locally looks like euclidean two-space in the neighborhood of any point on the interior. The only possible problem is at the edges. Because each edge is identified with its opposite one, however, a neighborhood of a point at the edge of the parallelogram will simple wrap around to the other side, and look just as euclidean as the neighborhood around any other point.
	
	\begin{example}\label{ex:M_n}
		The set of $n \times n$ matrices forms a real manifold of dimension $n^2$.
	\end{example}
	Note that any $n \times n$ matrix can be equivalently viewed as a vector living in $\mathbb R^{n^2}$. Given any matrix, locally we can go in each of $n^2$ direction by appropriately varying one of the $n^2$ components of the matrix. So this manifold not only locally looks like $\mathbb R^{n^2}$ but can in fact be identified as being \emph{the same} as $\mathbb R^{n^2}$.
	
	\begin{example}\label{ex:GL_n}
		The set of \emph{invertible} $n\times n$ matrices forms a manifold of dimension $n^2$
	\end{example}
	It is not obvious that this is a manifold. We need to know that near any point on this set $M$, we can go in each of $n^2$ linearly independent directions. The way we can see this is that invertible matrices are precisely those matrices $A$ with \emph{nonzero determinant}, $\det A \neq 0$. If I pick a given point $A$ on this manifold corresponding to a matrix with nonzero determinant, then say it has determinant $d \neq 0$. Because the determinant is a \emph{continuous} function, then changing any of the $n^2$ components of $A$ by a small number $\delta$ will change the determinant by some small amount as well. We just need to pick $\delta$ small enough so that the resulting change has magnitude less than $d$, and therefore keeps the determinant away from $0$. As long as we pick the neighborhood around $A$ small enough, we still can vary in any of $n^2$ directions, and so it still \emph{locally} looks like $\mathbb R^{n^2}$. 
	
	\begin{example}\label{ex:SO_n}
		The set of rotations in Euclidean $3$-space is a manifold. 
	\end{example}
	From mechanics and engineering, or just by playing with an object for a little bit, it is known that there are three independent ways to rotate something (about each of the three spacial axes). Any given rotation can be specified by three ``Euler Angles'' that describe how much to rotate about each axis, in a specific order. For a specific rotation given by these three angles, we intuitively expect that we can \emph{perturb} this rotation in three independent ways by slightly changing one of the three angles. There would then be a three-dimensional neighborhood of the rotations \emph{near} the original one. So locally, we look like Euclidean 3-space. 
	
	\begin{prop}
		Manifolds are a powerful and useful idea in both engineering and applied mathematics.
	\end{prop}
	Oftentimes, we care about studying the possible states that a system can have, like the ways that an object can rotate, as in the prior example. This goes much beyond possible rotations, and can go as far as 
	
	\todoadd{AARON EXPLAIN WHY MANIFOLDS ARE USEFUL I DONT KNOW ANYTHING PRACTICAL}
	
	
	% section examples_of_manifolds (end)
	
	\section{Elementary Topology} % (fold)
	\label{sec:elementary_topology}
	
	\index{Topology|(}
	
	Before we can begin to do geometry on a space, we need to go even more fundamental still, into the real of topology. Geometry takes its name from the ``study of the earth'', while topology takes its name from the more abstract ``study of places'' (\emph{topos}). Topology deals with ideas that, to a geometer or a physicist, border on the unphysical. In fact, it is likely easier to work to work with topological objects by relying on pure logical reasoning rather than geometrical intuition. Topology is in many ways an extension of set theory, and is a central tool in mathematical analysis of functions when defining and studying ideas like continuity.
	
	Elementary topology begins with \textbf{point-set topology}\index{Topology!Point-Set}, in which the central objects of study are the open and closed subsets of a main set $X$. This should in fact be a familiar idea from studying the real line $\mathbb{R}$, on which there is the \textbf{Euclidean Topology}\index{Euclidean Space!Topology of}\index{Topology!Euclidean}.
	
	On the real line, intervals of the form $(a,b)$ consisting of all $x$ so that $a<x<b$ were called \textbf{open intervals}. One of the interesting things about open intervals is that on an interval like $(a,b)$ there is no \emph{greatest} number in that set. 
	\begin{prop}
		There is no maximum number on the open set $(a,b)$
	\end{prop} 
	\begin{proof}
		This will be a proof by contradiction. If there were a maximal number $x \in (a,b)$ then it must necessarily be \emph{strictly} less than $b$ (Note since $b$ is not strictly less than itself, $b \notin (a,b)$). Therefore, since $x<b$ we can consider $(x+b)/2$. This average is strictly greater than $x$ and less than $b$ and so is in $(a,b)$, contradicting the fact that $x$ was the maximal element in $(a,b)$.
	\end{proof}
	By the same argument, there is no minimal element in $(a,b)$. This set has the interesting property that for a point $x \in (a,b)$, there are points both to the right and left of $x$ that are still in $(a,b)$. Such subsets of $\mathbb{R}$ are called open
	
	\begin{defn}
		A subset $S$ of $\mathbb{R}$ is called open iff for any point $x \in S$, we can pick an $\varepsilon$ sufficiently small so that the open interval $(x-\varepsilon, x+\varepsilon) \subseteq S$ as well.
	\end{defn}
	For example, any point in the interval $(-1,3)$ satisfies this. For example $2.99$ is contained in the interval $(2.99 - 0.005, 2.99+0.005)$ which is a subset of  $(-1,3)$. In other words, any point in an open set has that all the points sufficiently close to it are also in that set.
	
	
	The notion of an ``open interval'' of size $\varepsilon$ around a point $x$ generalizes into higher dimensions by talking about an ``open ball'' of radius $\varepsilon$ around a point $p$
	\begin{equation}
		B_{\varepsilon} (p) := \{ q \in \mathbb{R}^n : |p-q|<\varepsilon \}
	\end{equation}
	that is, the set of all points $q$ within $\varepsilon$ of $p$. So a subset $S$ of $\mathbb{R}^n$ is called open iff every point $p \in S$ has a ball $B_\varepsilon(p)$ of some sufficiently small positive radius $\varepsilon$ is contained in $S$.
	
	So on the real line open intervals are open sets, and in fact any union of open intervals is still an open set (in fact an exercise will show that all open sets on the real line can be expressed as a (possibly infinite) union of open intervals). Intersections of open intervals are also open, but only when there are \emph{finitely many} such intersections. On the other hand, infinite unions of open sets are still open:
	
	\begin{prop}
		In the Euclidean topology of $\mathbb{R}^n$, any arbitrary union $V = \bigcup_\alpha U_\alpha$ of open sets $U_\alpha$ is open 
	\end{prop}
	\begin{proof}
		Let $p$ be a point in this union $V$, then since it belongs to the union, it must belong to at least one of the $U_\alpha$. This means that some $B_\varepsilon(p) \subseteq U_\alpha$, implying $B_\varepsilon (p) \subseteq V$ as well.
	\end{proof}
	Hopefully this proof illustrates something that is common in many topology proofs: working with simple logic works better than appealing to geometric intuition. Similarly
	\begin{prop}
		In the Euclidean topology, any \emph{finite} intersection $V = \bigcap_\alpha U_\alpha$ of open sets is open.
	\end{prop}
	\begin{proof}
		Let $p$ be a point in the intersection $V$, then since it belongs to the intersection, it must belong to \emph{all} of the $U_\alpha$. For each $U_\alpha$ there a positive $\varepsilon$ so that $B_\varepsilon(p) \subseteq U_\alpha$. Since the intersection is finite, pick the minimum such epsilon and it will still be positive. Moreover $B_\varepsilon(p)$ will then be contained in each $U_\alpha$ so will be contained in the intersection $V$ as well.
	\end{proof}
	It's clear to see that if the intersection were \emph{not} finite, then the set of $\varepsilon$s that we take the minimum over may give us a minimum value of zero. This is illustrated in an example as one of the exercises. We are now in a good place to define what we mean by a \textbf{topology} on a set.
	
	\begin{defn}\label{def:Topology}
		A \emph{topology}\index{Topology} on a set $X$ is a family $\mathcal T$ of subsets of $X$ so that
		\begin{itemize}
			\item Both the empty set $\emptyset$ and $X$ are in $\mathcal T$
			\item Any finite/infinite union of sets in $\mathcal T$ is in $\mathcal T$
			\item Any \emph{finite} intersection of sets in $\mathcal T$ is in $\mathcal T$
		\end{itemize}
		The elements of the topology $\mathcal T$ are called the \textbf{\emph{open sets}}\index{Topology!Open Set} of $X$. Complements of open sets are called \textbf{\emph{closed sets}}\index{Topology!Closed Set}. $X$ is then called a \textbf{\emph{topological space}}\index{Topology!Topological Space}.
	\end{defn}
	Note that on $\mathbb{R}$, the closed intervals $[a,b]$ are the complements of the unions of open sets informally denoted by $(-\infty, a) \cup (b, \infty)$.
	
	We can always arbitrarily define a topology and call ``these sets, and all their  arbitrary unions'' open, but it is better to work with a natural topologies like the Euclidean one discussed above.\\
	\todoadd{Possibly talk about the algebra of open/closed sets?}
	
	
	Why do we care about open sets so much? They allow us to define a notion of continuity for functions between any topological spaces:
	
	\begin{defn}[Continuity]\label{def:Continuity}
		A function between two topological spaces $$f: X \rightarrow T$$ is \textbf{\emph{continuous at a point}} \index{Topology!Continuous Functions} $p \in X$ iff for every open set $V\subseteq T$ containing $f(p)$, there is an open set $U \subseteq X$ so that $f(U) \subseteq V$.
		
		
		A function is \textbf{\emph{continuous}} if it is continuous at every point $p \in X$.
	\end{defn}
	\textbf{DRAW THIS}\\
	
	On the Euclidean topology, this is exactly equivalent to the $\varepsilon-\delta$ definitions taught in calculus class. An exercise will sketch out a simple proof of this fact. This is one of the reasons that open sets are worth studying: it allows us to turn language using numerical epsilons and deltas into a ``coordinate-free" language of maps between sets. Intuitively, a continuous function is one that doesn't locally mess with the space too much around any point. 
	
	\begin{defn}[Homeomorphism]\label{def:Homeomorphism}
		A function between two topological spaces $$f: X \rightarrow T$$ is a \textbf{\emph{homeomorphism}}\index{Homeomorphism}\index{Topology!Homeomorphism} iff it is bijective, with inverse $$f^{-1}: T \rightarrow X$$ so that both $f$ and $f^{-1}$ are continuous.
	\end{defn}
	
	An example of a homeomorphism that is one of the most common ideas associated with topology is that something like a coffee cup is homeomorphic to a doughnut as a topological space. \\
	\todoadd{ELABORATE HERE: coffee cup = doughnut}
	
	In order to talk about manifolds \emph{locally} looking like Euclidean space, we need to be able to have a rigorous way of talking about things \emph{locally}. We want a good way of defining a \emph{neighborhood} around a point $p$. Certainly an open ball containing $p$ is a neighborhood of $p$ because it contains ``All the points near $p$''. In the Euclidean topology, any open set $U$ containing $p$ also contains a ball of some radius around $p$, and thus contains a neighborhood of $p$. We could say that a neighborhood is any open set containing $p$, or more generally:
	
	\begin{defn}[Neighborhood]\label{def:Neighborhood}
		A \index{Topology!Neighborhood}\textbf{\emph{neighborhood}} $V$ of a point $p$ is a subset of $X$ that contains an open set containing $p$. That is $p \in U \subseteq V$.
	\end{defn}
	
	This way, there is no need for $V$ to be open, but it does \emph{contain} an open set containing all the points close to $p$.
	
	\textbf{Draw a sphere, with a patch.}
	
	\begin{defn}[The Manifold]\label{def:Manifold}
		A \textbf{\emph{manifold}}\index{Manifold} $M$ of dimension $n$ is a topological space such that for every point $p \in M$, there is a neighborhood $U$ of $p$ that is homeomorphic to an open  subset of $\mathbb{R}^n$.
	\end{defn}
	
	This is exactly what we have been speaking about intuitively the whole time: for any point, a neighborhood around that point looks like Euclidean space. On a manifold, then, these neighborhoods are open sets that can be mapped in a one-to-one manner to open sets in $\mathbb{R}^n$. We use this notion to define coordinate charts: 
	
	\begin{defn}[Coordinate Charts]\label{def:coordinate_charts}
		A \textbf{\emph{coordinate chart}}\index{Manifold!Coordinate Chart} $(U, \varphi)$ is an open set $U \subseteq M$ together with a ``coordinate" map $\varphi: U \rightarrow \mathbb{R}^n$ that is a homeomorphism of $U$ to and open subset of $\mathbb{R}^n$.
	\end{defn}
	
	The revolutionary idea of Descartes has been translated beyond Euclidean space, onto manifolds. Coordinate charts are vital to going from geometric data into algebraic calculations. They allow physicists to lay down a coordinate system that is valid for at least a \emph{part} of the manifold, on which numerical calculations can be done. Because $\varphi$ is one-to-one, we can parameterize the part of the manifold on $U$ by $n$ parameters.
	
	\begin{example}[Charts on the Sphere]
		The sphere has a \textbf{\emph{stereographic projection}}\index{Sphere!Stereographic Projection} to the plane that covers every point of the sphere except the north pole. 
	\end{example}
	\todoadd{Charts on the sphere}
	
	We would like to be able to patch up the whole manifold with coordinate charts, so that we can work globally. Such a patch that covers the entire manifold is called an \emph{atlas}.
	
	\begin{defn}[Atlas]\label{def:Atlas}
		An \textbf{\emph{atlas}}\index{Manifold!Atlas} on a manifold is a set of coordinate charts $(U_\alpha, \varphi_\alpha)$ whose union $\bigcup_\alpha U_\alpha = M$.
	\end{defn}
	
	Note that because each $\varphi_\alpha$ is a homeomorphism, meaning its inverse is also continuous, then if $U_\alpha \cap U_\beta \neq \emptyset$, we have $\varphi_\alpha \circ \varphi_\beta^{-1}$ is a continuous function as well, from $\mathbb{R}^n$ to $\mathbb{R}^n$. These are the \emph{transition maps}\index{Manifold!Transition Map} between coordinate patches.
	
	A \textbf{smooth manifold} is one where the $\varphi_\alpha$ are not only continuous but in fact infinitely differentiable (i.e. \textbf{smooth}). This makes the transition maps smooth functions on $\mathbb{R}^n$. Essentially all the examples we deal with for the remainder of this book will be smooth manifolds.
	
	\index{Topology|)}
	% section elementary_topology (end)
	
	\section{Embeddings vs. Intrinsic Geometry} % (fold)
	\label{sec:embeddings_vs._intrinsic_geometry}
	
	\todoadd{The whole embeddings section. Talk about how we always picture manifolds as embedded into Euclidean space, but nothing about them \emph{intrinsically demands it}}
	
	\todoadd{Mention any $n$-dimensional manifold can be embedded in $\mathbb{R}^{2n}$}

	
	\todoadd{Talk about how the torus is exactly the space in Example \ref{ex:secret_torus}}
	
	% section embeddings_vs._intrinsic_geometry (end)
	
	\section{Vectors Reimagined} % (fold)
	\label{sec:vectors_reimagined}
	
	\index{Vector!As a Derivation|(}
	
	Let us go back to $\mathbb{R}^3$\index{Euclidean Space}. Studying the point-set topology of $\mathbb{R}^3$ as we have been doing for manifolds is a very easy thing to do, and isn't so rewarding. The reason we do it is so we can apply it to studying the \emph{functions} on $\mathbb{R}^3$. A function on $\mathbb{R}^3$ takes three a tuple of three real inputs $(x,y,z)$ and outputs a real number $f(x,y,z)$.
	
	As you should be aware of by now, $(x,y,z)$ is not a point, but instead a \emph{coordinate representation} of some point. The manifold of Euclidean 3-space $\mathbf{E}^3$ can be modelled by $\mathbb{R}^3$ once a coordinate representation is chosen. This distinction is so slight that it is barely noted, but it is worth noting. A function $f: \mathbf{E}^3 \rightarrow \mathbb{R}$ in fact takes a point $p \in \mathbf{E}^3$ and outputs a real number $f(p)$.
	
	We also study vector fields on Euclidean space\index{Euclidean Space!Fields on}. Before thinking too hard about coordinate co-variance and contra-variance, we would just pick an orthogonal reference frame, and label an orthonormal basis by three vectors $\hat{\mathbf{i}},\hat{\mathbf{j}},\hat{\mathbf{k}}$. Then, the form of a vector field looked like
	
	\begin{equation*}
		\mathbf{F} = P(x,y,z)\hat{\mathbf{i}} + Q(x,y,z)\hat{\mathbf{j}} + R(x,y,z)\hat{\mathbf{k}}
	\end{equation*}
	
	and again, $(x,y,z)$ is in fact a coordinate representation of the invariant point $p = x \hat{\mathbf{i}} + y \hat{\mathbf{j}} + z \hat{\mathbf{k}}$.\\
	
	In higher dimensions, a function (i.e. a scalar field) could be specified in terms of a coordinate representation by $f(x_1, \dots, x_n)$. Usually, it is customary to replace $\hat{\mathbf{i}}, \hat{\mathbf{j}}, \hat{\mathbf{k}}$ by the notation $\hat{\mathbf{e}}_1, \hat{\mathbf{e}}_2, \dots, \hat{\mathbf{e}}_n$ to denote an orthonormal frame in $n$ dimensions, just so that we do not run out of alphabet characters.
	\begin{equation*}
		\mathbf{F} = \sum_{i = 1}^n \mathbf{F}^i(x^1, \dots , x^n) ~ \hat{\mathbf{e}}_i = \mathbf{F}^i(x^j) ~\hat{\mathbf{e}}_i
	\end{equation*}
	where we are now beginning to use Einstein summation convention\index{Summation Convention}. Of course, we do not \emph{need} an orthonormal frame to describe a vector. We can use any basis $\mathbf e_i$ and write $p = x^i \mathbf e_i$, $\mathbf{F}(p) = \mathbf{F}^i(p) \mathbf e_i$.
	
	The first important thing to note is that a scalar field is an invariant, \emph{physical} thing on $\mathbb{R}^n$ (and on manifolds in general). When we talk about the temperature at each point in space, or classically when we talk about the energy density of the electric field at each point in space, that means there is a specific invariant number at each point. It doesn't matter whether we set up coordinates $x^i$ on the space or not. The temperature at a point does not depend on what coordinate system we represent that point in. 
	
	Similarly, a \emph{vector field} is also an invariant physical thing! The components $\mathbf F^i$ of the vector field, of course, \emph{will} depend on the coordinate system we use, but the field $\mathbf{F}$ itself will \emph{not}. Just like the wind doesn't change its motion across the earth when we change the coordinate system we use for measuring it, $\mathbf{F} = \mathbf{F}^i \mathbf e_i$ will remain invariant at every point. Only the components $\mathbf{F}^i$ will contra-vary against our co-variant $\mathbf e_i$.
	
	Using these ideas from $\mathbb{R}^n$, consider a manifold $M$. A scalar function on $M$ is no difficult thing to define: it associates to each point $p \in M$ a value $f(p)$. Because we can form an atlas of charts $(U_\alpha, \varphi_\alpha)$, we can locally form a function $f \circ \varphi_\alpha^{-1} : \mathbb{R}^n \rightarrow \mathbb R$ on Euclidean $\mathbb{R}^n$. Then we can say $f$ is continuous/differentiable/smooth if every $f \circ \varphi_\alpha^{-1}$ is. In physics, we will frequently be concerned with smooth functions in particular. They are natural objects of study. The set of smooth, infinitely differentiable functions on a manifold $M$ will be denoted by $C^\infty(M)$, while the space of continuous functions will be denoted by $C^0(M)$.
	
	The $\varphi_\alpha^{-1}$ allow us to take a tuple of $n$ coordinates $q^1, \dots, q^n$ on $\mathbb{R}^n$ and associate that to a unique point on $M$ that is part of $U$. So we see how a scalar field $f: M \rightarrow R$ can descend to a function on the local coordinates. 
	
	\todofig{Show a graph of the curves for coordinates $q_i$}
	
	The question now is what about a \emph{vector field} $\mathbf{F}$ on our manifold $M$? What does this thing mean? We want to associate to each point on $M$ a vector at that point. Note that this doesn't mean we want to associate to each point on $M$ just some tuple in $\mathbb{R}^n$, because a vector is \emph{not} just a tuple of numbers (that's coordinate dependent). We want to associate a physical vector at that point. 
	
	 Intuitively it should make sense what we mean by this: 
	
	\todofig{Ball with a vector field on it}
	
	In Section~\ref{sec:nonlinear_coordinate_systems_are_locally_linear}, we showed how when adopting polar coordinates, the corresponding vectors $\hat{\mathbf{r}},\hat{\mathbf{\theta}}$ change direction depending on the point $p$ we're describing. Each of them points in the direction of an increase in the corresponding coordinate. So for a general curvilinear coordinate system on a manifold $q^i$, the associated vectors $\mathbf e_i$ would be tangent to the paths traced out by varying $q_i$. 
	
	The vectors $e_i$ are tangents to the curves traced out by letting each $q^i$ vary (i.e. $\phi^{-1} (q^i)$). 
	
	At each point $p \in M$, the local tangent vector $\mathbf e_i$ corresponding to coordinate $q^i$ is associated with increasing $q^i$ while holding all other coordinates fixed. This already gives us enough information to know one thing: the rate of change of a scalar function $f$ along $\mathbf e_i$. It is exactly the partial derivative with respect to $q^i$
	
	\begin{equation}
		D_{\mathbf e_i} = \frac{\partial f}{\partial q^i}
	\end{equation}
	
	We already know from Section~\ref{sec:einstein's_summation_convention} that partial derivative operators with respect to coordinates are \emph{co}-variant, just like the vectors $\mathbf e_i$ themselves. So for any coordinate system $q^i$, we have a set of differential operators $\partial/\partial q^i$ corresponding to directional derivatives along the basis vectors $\mathbf e_i$. So we have a correspondence between coordinate frames $\mathbf e_i$ at a point and the differential operators associated with their coordinate system $q^i$.
	
	\begin{align*}
		&\text{Covariant Quantities:}&  \mathbf e_i \longleftrightarrow \frac{\partial}{\partial q^i}
	\end{align*}
	
	 The set of vectors at $p$ correspond to all the possible directions that paths on $M$ can pass through $p$. If we have a path $\gamma \in M$ parameterized by $t$ so that $\gamma(t_0) = p$ then we can compute derivative along the direction $\mathbf v$ tangent to $\gamma$ as
	 \begin{equation}
	 	D_\mathbf v f = \frac{d}{dt} \left[ f\left(\gamma(t) \right) \right]
	 \end{equation}
	 So although even though we don't have an idea of how to represent $\mathbf v$ in terms of our coordinate system $q^i$, we know how to represent its directional derivative operator. That is just:
	 \begin{equation}
		D_\mathbf v =  \frac{dq^i}{dt} \frac{\partial}{\partial q^i}
	 \end{equation}
	 where $dq^i/dt$ is the change in $q^i = \varphi^i(\gamma(t))$.
	 \begin{equation}
	 	\frac{dq^i}{dt} = \frac{d}{dt} \big[ \varphi^i(\gamma(t)) \big]
	 \end{equation}
	 This gives us the ``component'' $v^i$ associated with $\partial/\partial q^i$ for our directional derivative in our coordinate basis:
	 \begin{equation}
	 	D_\mathbf v = \frac{d}{dt} \big[ \varphi^i (\gamma(t))\big] \frac{\partial}{\partial q^i} = v^i \frac{\partial}{\partial q^i}
	 \end{equation}
	 
	But this is a correspondence between two physical, invariant objects associated with a direction:
	\begin{align*}
		\text{Tangent Vectors at $p$} &\rightleftharpoons \text{Directional Derivatives at $p$}\\
		\mathbf v = v^i \mathbf e_i &\longleftrightarrow D_\mathbf v = v^i \frac{\partial}{\partial q^i} 
	\end{align*}
	Independent of the coordinates $q^i$ that we use, the differential operator $D_{\mathbf v}$ will give the same invariant value when acting on a physical scalar field $f$, namely $v^i \frac{\partial f}{\partial q^i}$. So this differential operator is an invariant just like its corresponding vector $\mathbf v$. Moreover, in the case of Euclidean space, the components $v^i$ of the directional derivative operator are exactly the same as the components of $\mathbf v$ itself:
	\begin{equation}
		D_\mathbf v = \mathbf v \cdot \nabla = v^i \frac{\partial}{\partial x^i}
	\end{equation}
	
	Just like with vector addition, the sum $\mathbf u + \mathbf v$ corresponds to the operator $ D_{\mathbf u + \mathbf v} = D_\mathbf u + D_\mathbf v$, and a multiple $c \mathbf u$ gives the multiple of the original derivative $D_{c \mathbf u} = c D_\mathbf u$. Directional derivatives form a vector space in one-to-one correspondence with the vectors at $p$.  For this reason we say that these directional derivatives \emph{are} the vectors at $p$. Vectors, at their core, represent flows along direction, which is no different from what a directional derivative operator along that point represents. We will sometimes abbreviate it as $v^i \partial_i$.
	
	This leads us to define the \textbf{tangent space} of the vectors at a point $p$ of a manifold $M$.
	\begin{defn}[Tangent Space]
		The tangent space\index{Manifold!Tangent Space} at a point $p$ of a manifold $M$, $T_p M$ is the set of first order derivative operators at $p$.
	\end{defn}

	Going back to $\mathbb{R}^n$, this means that this whole time we could have treated $\hat{\mathbf{i}}$ as $\partial/\partial x$, $\hat{\mathbf{j}}$ as $\partial/\partial y$, etc. Using this idea is powerful, because now just from having a coordinate patch, $q^i$ on a manifold, we obtain the full set of tangent vectors $\partial/\partial q^i$ at each point on the manifold. 
	
	The fundamental observation is that vectors are in one-to-one correspondence with the \emph{first order behaviour} of curves passing through $p$ (i.e. the instantaneous velocity along that curve). This in turn corresponds to first-order derivative operators at $p$ (corresponding to moving along the curve's direction instantaneously). Vectors can be viewed as equivalence classes of curves through $p$, where two curves $\gamma_1, \gamma_2$ are equivalent if they share the same tangent at $p$. \\
	
	The guiding philosophy of this section is that everything vectors represent: namely direction and magnitude, can be derived from the way that we compute changes in a scalar function via directional derivatives. From multivariable calculus, it is easy to see that the first order change in a function along a curve is the sum of the changes due to each coordinate:
	\begin{equation}
		\frac{df}{dt} = \frac{\partial f}{\partial q^i} \frac{dq^i}{dt}
	\end{equation}
	Given a function $f$ and given a specific direction of first order changes $dq^i/dt$, this gives a \emph{real, invariant} quantity. Note that this quantity is dependent on two objects: the \emph{function} that we are differentiating and the \emph{direction} along which we differentiate.
	
	When we focus a specific direction along a curve: $dq^i/dt = v^i$, and allow the function to be arbitrary we get a vector corresponding to change along our path:
	
	\begin{equation}
		\mathbf v  = v^i \partial_i
	\end{equation}
	This needs to be \emph{fed} a \underline{function} $f$ to give a real number value.
	
	On the other hand, if we pick a specific \emph{function} with first order behaviour given by $\partial_i f = \omega_i$ and allow for the \emph{direction} to be arbitrary, we get something else
	
	\begin{equation}
		df = \omega_i dq^i
	\end{equation}
	This needs to be \emph{fed} a \underline{direction} $dq^i$ to give an associated real value. We will call this new object a \textbf{covector}\index{Vector!Covector} or a \textbf{1-form}\index{Differential Form!1-Form}, and it is often denoted by the greek letter $\omega$. It is an entirely different object from a vector. Where a vector represents a direction that we can differentiate functions alone, this represents a \emph{function differential} which we can take changes of, along a direction. It is the dual to the notion of a vector, but behaves in very much the same way.
	
	The sum of two one-forms is still a one form, as are scalar multiples, so the one forms at a point $p$ form a vector space called the \textbf{cotangent space} at $p$.
	
	\begin{defn}[Cotangent Space]
		The cotangent space\index{Manifold!Cotangent Space} at a point $p$ of a manifold $M$, $T_p^* M$ is the set of all first order differentials at $p$.
	\end{defn}
	
	Vectors were defined in terms of curves: they represent the first order approximations to curves (namely their tangents, and the derivatives along them). 1-Forms are defined in terms of \emph{functions}: they represent the first order approximations of functions (namely their differentials). At first glance, you may think that for a dimension $n$ manifold, there are only $n$ directions to go in, whereas we have seemingly infinite freedom in picking the functions passing through $p$ so there are many more 1-forms than there are vectors, but this isn't true.
	
	Just as there are a huge number of functions that we can define at $p$, there are a huge number of curves that go through $p$, but because we only care about \underline{first order behaviour} at $p$ for the curves, many curves give the \emph{same} tangent vector. Similarly, many functions will give the \emph{same} 1-form, if they have the same \underline{first order behaviour}. It should start to become clear that there is duality between these two ideas
	
	\begin{concept}
	The first-order behaviour of curves at a point $p$ (i.e. vectors) is dual (in a sense that will be discussed in the next chapter) to the first-order behaviour of the functions at $p$. On a manifold of dimension $n$, both of these are vector spaces of dimension $n$. 
	\end{concept}
	
	\begin{center}
	 \begin{tabular}{|p{5cm} | p{5cm}|} 
	 \hline
	 Vectors & 1-Forms  \\ [0.5ex] 
	 \hline \hline
	 First-order approximations\newline to \underline{curves} & 
	 First-order approximations\newline to \underline{functions} \\
	 \hline
	 To differentiate\newline \underline{functions} along &
	 To be integrated\newline along \underline{curves} \\
	 \hline
	 Act on 1-Forms &
	 Act on Vectors \\
	 \hline
	 Contravariant components & Covariant components \\
	 \hline
	 Space of dimension $n$ & Space of dimension $n$ \\ [1ex] 
	 \hline
	\end{tabular}
	\end{center}
	
	This is all well and good, but how can we \emph{visualize} these two concepts. Visualizing vectors is easy, we've been doing it since calculus class, if not before. They are ``arrows'', tangent to curves, showing the first-order behaviour of that curve. 1-forms on the other hand are a different story. If we pick a point $p$ in $2$D, then the first order behavior of a function can be visualized by adding in a dimension and showing the local tangent plane approximating that function. Alternatively, we can stay in 2-D and just show the local \emph{level curves} (these are lines for tangent plane). 
	
	\todofig{GRAPHIC: Vector on the left, form on the right}
	
	In $3$D vectors would look the same, and forms could be visualized by the local level-planes of the function at $p$.
	
	\todoex{Exercise on constructing a dual basis of forms to a vector}
	
	If we are given a coordinate system $q^i$ then on a given vector $\mathbf v = v^i \partial_i$ representing a direction, a form $\omega_i dx^i$ representing the first order behavior of a function can \emph{act} on $\mathbf v$ to give the first order change along $\mathbf v$. 
	
	\begin{lemma}
		For a basis $\frac{\partial}{\partial q^i}$ of the tangent space $T_p M$ at a point $p$, the associated first order behavior corresponding to first order change only along one coordinate $dq^i$ forms a basis for the cotangent space \emph{and} we have $\partial_i dq^j = \delta^j_i$
	\end{lemma}
	\begin{proof}
		The fact that $dq^i$ form a basis for the set of differentials of functions at $p$ is clear since every function has the first order approximation $df = \partial_i f dq^i = \omega_i dq^i$, exactly in the span of the $dq^i$. \\
		For the second part, for a specific coordinate $q^i$, $\partial_i$ corresponds to finding a change along $q^i$, holding all other $q^j$ fixed. If the local behavior of a function was just $dq^j$ for a specific $j$ different from $i$, then since this does not result in a change by varying $i$, we get that $\partial_i dq^j = 0$ if $i\neq j$. On the other hand for $dq^i$, representing the local behavior of a function that goes up by $1$ unit for a unit change in $dq^i$, we'd get $\partial_i$ of such a function is exactly $1$, so we write $\partial_i dq^i = 1$ (no summation) for any specific $i$, so that in general $\partial_i dq^j = \delta_i^j$ exactly.
	\end{proof}
	
	From this, we know how forms act on vectors $\omega(\mathbf v)$:
	\begin{equation}
		\omega(\mathbf v) = \omega_i dq^i (v^j \partial_j) = \omega_i v^j ~ dq^i \partial_j = \omega_i v^j \delta^j_i = \omega_i v^i
	\end{equation}
	
	For a real vector space $V$ (in this case our tangent space), linear maps $\alpha: V \rightarrow \mathbb R$ that send vectors in $\mathbf v \in V$ to real numbers $\alpha(\mathbf v)$ are called \textbf{linear functionals}\index{Functional, Linear} on the vector space. Note that a linear combination of two linear functionals $a \alpha + b \beta$ is still a linear functional that acts by $(a \alpha + b \beta)(\mathbf v) = a \alpha(\mathbf v) + b \beta(\mathbf v) \in \mathbb R$. This is exactly what one forms are, they act on vectors and send them to real numbers corresponding to the first order change along $\mathbf v$ of $\omega$. In general, for a vector space $V$, the vector space of linear functionals on $V$ is called the \textbf{dual space}\index{Dual Space}, denoted $V^*$.
	\begin{defn}
		For a finite-dimensional vector space $V$, the set of all linear maps $\alpha: V \rightarrow \mathbb R$ forms a vector space $V^*$ called the dual space of $V$. 
	\end{defn}
	\todoex{EXERCISE: show $(V^*)^*=V$}
	
	In linear algebra, perhaps this idea is \emph{too} simple. If our vector space is represented as column vectors, the linear functionals are the row vectors.
	
	\begin{prop}
		An element in $V^*$ is determined entirely by its action on a basis $\mathbf v_i$ of $V$. In particular this means that $\dim V^* = \dim V$.
	\end{prop}
	\begin{proof}
		Say two functionals $\alpha, \beta$ act exactly the same on the basis $\mathbf v_i$, then by their linearity they will act exactly the same everywhere: 
		\begin{align*}
			\alpha(\mathbf v)= \alpha(a^i \mathbf v_i) = a^i \alpha(\mathbf v_i) &= a^i \beta(\mathbf v_i)= \beta(a^i \mathbf v_i) = \beta(\mathbf v)\\
			\Rightarrow \alpha &= \beta
		\end{align*}
		This means a basis for the dual space can be built by the $(\mathbf v^*)^i \in V^*$ defined so that $(\mathbf v^*)^i$ acts as zero on all basis vectors $\mathbf v_j$ except $\mathbf v_i$, on which it's action gives $1$, i.e. $(\mathbf v^*)^i \mathbf v_j = \delta^i_j$. 
	\end{proof}
	
	\index{Vector!As a Derivation|)}
	\index{Manifold|)}
	\section{What Follows} % (fold)
	\label{sec:what_follows}

	\todochange{Fix the ``What Follows'' section according to the course of the book}
	
	The rest of this book will expand both on the geometry of fields and manifolds, and also on the larger ideas of groups, homogenous spaces, and representations. \\
	
	In Chapter 3, we will continue studying the fields that live on manifolds. We'll prove the General Stokes' theorem, an elegant generalization of the divergence, curl, and line integral theorems that have been taught in multivariable calculus. From there, we will study more thoroughly the concept of distance on a vector space and on a manifold using a metric, and how this relates vector fields to differential forms. The notion of a derivative will be extended to manifolds, and will take the form of a ``Lie Derivative''.\\
	
	In Chapter 4, we will introduce Fourier Analysis as a powerful tool for studying functions on the real line and in Euclidean space. We'll see how the set of functions on a manifold naturally forms a vector space (of infinite dimension) and consider the Fourier Transform as a change of basis. \\
	
	In Chapter 5, we will shift to looking at the representation theory of \emph{finite} groups and illustrate the parallels. We will then return to the study of continuous group actions on especially symmetric ``homogenous'' spaces, and show how Fourier analysis is related to their representation theory. Towards the end, we will expand on the idea behind a group actions on manifolds and look at the representation theory, giving a small glimpse into harmonic analysis: the Fourier transform on manifolds. Just as in the first chapter, we'll recognize the importance of the underlying differential geometry of the group action. The underlying differential structure is known as the ``Lie Algebra'' of the group, and we will discuss that.\\
	
	In Chapter 6, we introduce some background behind Lie Algebras. We put almost all of our focus on one special case: the Lie algebra $\frak{sl}_2 (\mathbb C)$. The relationship between this algebra and the symmetries of the sphere are explored, as well as its applications in quantum physics for studying angular momentum. The representation theory of a variant of this algebra gives rise to the concept of spin. \\
	
	In Chapter 7, we move further into physics, going over classical Lagrangian and Hamiltonian Mechanics. We discuss Noether's theorem in both the Lagrangian and Hamiltonian Pictures, and then we move to study Hamiltonian mechanics using the language of differential geometry that we have developed. This will give rise to \emph{symplectic geometry}. In chapter 7, combining this with representation theory gives rise to \emph{quantum mechanics}.\\
	
	In Chapter 8, we apply differential geometry first to the study of electromagnetism, and then to gravitation. We shall arrive at Einstein's theory of gravity. Along the way, we study in even greater detail the notion of a metric, a connection, and curvature. \\
	
	In Chapter 9, we use the representation theory and differential geometry that we have developed so far to study how quantum mechanics can arise from quantizing a symplectic manifold.\\
	
	Finally, Chapter 10 studies Lie algebras in greater detail, working towards the \emph{classification of complex semisimple Lie algebras}. Along the way, we will look at the relationship between representation theory of Lie algebras and modern physics. 
	
	% section what_follows (end)
	
	% section the_field (end)
	
	\section{Exercises} % (fold)
	\label{sec:exercises2}
	
	\todoex{Exercises: topology, connectedness, holonomy? definitions etc. }
	
	\todoex{EXERCISE: Define fiber bundle here and in particular the principal bundle... Extend to gauge?}
	% section exercises (end)
	
\end{document}