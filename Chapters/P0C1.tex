\documentclass[../master.tex]{subfiles}
 
 
\begin{document}
	
	\chapter{Old Notions Revisited}
	\section{The Cartesian Coordinate System} % (fold)
	\label{sec:Cartesian}
	
	
	
	One of the most important revolutions in mathematics and physics was ushered in by an idea of the seventeenth century mathematician and philosopher Ren\'{e} Descartes. The idea was that any point $P$ in the Euclidean plane could be represented by a pair of numbers $(x,y)$. The numbers themselves represented distances along two perpendicular axes that met at a point $(0,0)$, called the origin. By introducing this concept, he had done something amazing. He had related the \emph{geometry} of the plane to the \emph{algebra} of variables and equations. Algebra could be \emph{represented} geometrically, and conversely geometric problems could be solved by going into the realm of algebra. 
	
	\textbf{Insert 2D Plane with Coordinates}
	
	Coordinates soon became more than just pairs of numbers $(x,y)$. Their use was extended to 3D space, and later to arbitrarily high dimensions. They would subsequently be used to lay the foundations for modern physics and mathematics. Linear algebra, multivariable calculus, and all the connections between algebra and geometry begin with the concept of a coordinate.
	
	Since then, the use of coordinate systems has proven indispensable to physicists and mathematicians throughout history. Newton used Descarte's coordinate system to formulate his infinitesimal calculus. Maxwell used it to analyze electromagnetic fields, discovering mathematically that light is a wave in the electromagnetic field. Einstein, going further, made use of coordinate systems to formulate his theory of gravitation. Today, physicists and engineers do their calculations within the frame of coordinate systems. In mathematics, Descarte's idea planted the roots for what would turn into the modern field of algebraic geometry. \\
	
	When studying a geometric phenomenon in some $n$-dimensional space, say $\mathbb{R}^n$, we pick an origin and axes to form our coordinate system. For a ball falling, we could set the origin at some point on the ground, and pick one axis parallel to the ground, and one perpendicular. We can decide to measure the axes in meters, or we could decide to do it in feet (nothing stops us from making bad choices). The physical point $P$ where the ball lies is represented by $(x,y)=(0~ \mathrm m,10~ \mathrm m)$. The coordinate $y$ is a natural choice of coordinate, as it corresponds to our intuitive notion of height. 
	
	\textbf{Insert Ball Falling}
	
	We can now study $y$, free of geometry, as just a function which we can do arithmetic and calculus on. If we are given an equations of motion, say 
	\begin{equation*}
		\frac{d^2y}{dt^2} = -g, \hspace{5mm} \frac{d^2x}{dt^2} = 0
	\end{equation*} 
	with initial conditions,
	\begin{equation*}
		\frac{dy}{dt}=0, \hspace{5mm} \frac{dx}{dt}=0
	\end{equation*}
	 then we can perform our well-known kinetic calculations for the system, and see how the system evolves in the \emph{time} direction. **A recurring theme will be that dynamics of a system in $n$-dimensional space can be thought of just a special type of geometry in $n+1$ dimensional space, putting time as an added dimension**.\\
	
	Because the purpose of this text is to study the ways in which geometry, algebra, and physics connect, it is worthwhile to dwell on the \emph{philosophy} behind coordinate systems.
	
	The ball will fall from 10 meters, according to the force of gravity. That is the way the world works. It doesn't matter what coordinate system we set up to do that calculation, we should get the \emph{exact same result}. Plainly: nature doens't \emph{care} what coordinate system we use. This fact, obvious as it may be, is worth thinking about: No matter what coordinate system we use, the equation of motion should give the same dynamics. The laws of physics should be \emph{independent of any coordinate system}.
	
	Newton's law $\mathbf F = m \mathbf a$ relates the force vector to the acceleration vector. The vector representing the force $\mathbf F$ that you apply on a surface is an object independent of coordinate system, and so is the resulting acceleration vector. The \emph{components} of these vectors ($F_x, F_y, F_z$) and ($a_x, a_y, a_z$), however, depend on what you have chosen for the $x,y,z$ axes. These components \emph{represent} a real physical vector, but only once we pick a coordinate system. If we were to pick a different coordinate system, the numbers representing the vector would change. 
	
	When we write an equation describing a physical law, it should be valid regardless of the coordinate system we use. $\mathbf F = m \mathbf a$ will always be true whether we rotate our frame of reference or not. On the other hand, if Newton's law of motion \emph{only} said that the \emph{first} `x' component of the Force was equal to the \emph{first} `x' component of the acceleration, and said nothing about the other $2$ components, then in different coordinate systems since `x' means different things, we would get totally different equations of motion. No physical law will ever say something just about the first or just about the second components of two vectors: it must equate the entirety of the two vectors. 
	
	
	As another example if the equation for work looked like $F_x dx = dW$, then would give different results in different coordinate systems, because it puts emphasis on just one of the three components (the first `x' coordinate) over the others. While in some coordinate system $dx$ may point in the direction of the displacement and be nonzero, there may be a different coordinate system where $dx=0$, making the work done zero. So the equation for work would be coordinate dependent: it would be wrong. The need for such invariance is why the true formula uses all three spacial dimensions and looks like:
	
	\begin{equation*}
		\mathbf F \cdot  d \mathbf r = F_x dx + F_y dy + F_z dz = d W.
	\end{equation*}
	
	 Although it isn't obvious yet that this is a quantity that is invariant regardless of the coordinate system used, at the very least it doesn't put one component above any of the others.

	
	% section Descartes (end)
	
	\section{Linear Algebra \& Coordinates} 
	\label{sec:Linear Algebra & Coordinates}% (fold)
	
	The traditional concept of a coordinate system, a series of perpendicular lines that together associate ordered tuples of numbers to each point in $n$-dimensional space, is not representative of all coordinate systems. For one, we do not need the requirement that the lines be perpendicular. Our coordinate system could instead look like this:
	
	\textbf{Graphic of non-perp lines and representing a point like that}
	
	In the language of linear algebra: once we choose an origin, choosing a set of coordinate axes is the same as choosing a basis for the space (a coordinate basis). For any point in space, we can relate coordinates $x_i'$ in the new system in terms of coordinates $x_i$ in the old system by matrix multiplication: $x_i' = \sum_{j=1}^n \mathbf A_{ij} x_j$. This is exactly what's called a change of basis in linear algebra. Transformations between coordinate bases are exactly the invertible \textbf{linear transformations} \index{Linear Algebra!Linear Transformation}.
	
	As in linear algebra, we need our coordinate system to both \textbf{span}\index{Linear Algebra!Span} the space so that we can represent any point, and be \textbf{linearly independent}\index{Linear Algebra!Linear Independence} so that every point that we can represent in our coordinate system will have a unique representation. That's all that a basis is: it specifies a good coordinate system. 
	
	\begin{defn}
		A set of vectors is said to span a space $\mathbb{R}^n$ if every point $P$ can be represented as $a_1 \mathbf v_1 + \dots a_n \mathbf v_n$
	\end{defn}
	
	\begin{defn}
		A set of vectors $\{\mathbf v_1, \dots , \mathbf v_k \}$ is called linearly independent if there is only one way to represent the zero vector $\mathbf 0$ as a combination of them, namely as $\mathbf 0 = 0v_1 + \dots + 0 v_k$.
	\end{defn}
	
	This second definition is the same as saying every point that we can represent in our system has a unique representation. Let's make this clear. If there were two ways to represent a point $P$: as \begin{equation*}
		a_1 \mathbf v_1 + \dots + a_n \mathbf v_n
	\end{equation*} 
	and 
	\begin{equation*}
		b_1 \mathbf v_1 + \dots + b_n \mathbf v_n
	\end{equation*} 
	then subtracting these two different combinations would give a nonzero way to represent zero.  Conversely, if there were a nonzero combination of vectors summing to zero, then we could add that combination to the coordinate representation of any point and get a \emph{different} representation of the same point. So coordinate representations for all vectors are unique as long as long as there is only one representation for zero the one where each component equals zero.
	
	Intuitively, linear independence means that there is no superfluous information in the set of vectors.  We cannot linearly combine vectors in some subset to get another vector in the set; each vector is adding its own unique additional piece of information, making the set able to span in an additional direction.
	
	Bases that don't span, or are not linearly independent, would lead to coordinate systems like these:
	
	\textbf{Show a 2-D basis in a 3-D space, and a basis of 3 vectors in 2-D space}

	Very often in mathematics, we ask ``does a solution exist?'', and ``if there is a solution, is it unique?''. These two questions are dual to one another. If a set of vectors spans the space, then there \emph{exists} a way to represent any point (at least one way to represent any point). If a set of vectors is linearly independent, then \emph{if} you can represent a point, that representation is \emph{unique} (no more than one way to represent any point).
	
	Now to stress the same idea again: because points in $\mathbb R^n$ and vectors are essentially the same thing, the idea that points in space are invariant of a coordinate system applies just as well to vectors. If we choose a basis for our vector space $\mathbf v_1, \dots \mathbf v_n$, then we can express any vector $\mathbf u$ by a unique combination $\mathbf u = a_1 \mathbf v_1 + \dots + a_n \mathbf v_n$. We then say that in this basis, we can represent $\mathbf u$ by a list of numbers. Often, it is written:
	\begin{equation*}
		\mathbf u = \begin{pmatrix} a_1 \\ \vdots \\a_n	\end{pmatrix}.
	\end{equation*}
	
	But in some sense, writing this as an equality is wrong. The vector $\mathbf u$ is something physical: a velocity, a force, the flow of water. It doesn't depend on the coordinate system. On the other hand, the right hand side is just a list of numbers that depend entirely on the coordinate system chosen. If we change coordinate systems, the right hand side changes. Because $\mathbf u$ exists (say, in the real world) independently of coordinates used, it does not change.
	
	A geometric vector like $u$ is \emph{not} a list of numbers. Once we pick a basis, $u$ can be \emph{represented by} a list of numbers, but if we change into a different basis, those numbers all have to change as well. This exact same idea will be the reason why a tensor is \emph{not} just a multi-dimensional array (like the ones encountered in computer science). It can be \emph{represented by} a multi-dimensional array once a coordinate system is chosen, but the numbers in each entry will differ depending on the coordinate system we pick. 
	
	This is very confusing (and will also be part of the reason why it's so hard to understand tensors as an undergraduate). In most math courses, we can freely call any list of numbers a `vector'. After all, you can add lists and scale them so they do form a `vector space'. This is a really unfortunate linguistic degeneracy in mathematics terminology. The type of vectors that we see in physics (acceleration, force, electric field, etc.) are \emph{geometric vectors}\index{Vector!Geometric} that have nothing \emph{a priori} to do with lists of numbers until we represent them as such by using coordinate systems. On the other hand, abstract structures that we can add and multiply by scalars are \emph{algebraic vectors}\index{Vector!As Lists}, and lists are an example of that. To avoid confusing lists of numbers with the geometric vectors in the physical world, we will call lists of numbers \emph{tuples}\index{Vector!Tuple} rather than vectors.
	
	So returning to the geometric vector $\mathbf u$, a more careful way to write it would be:
	
	\begin{equation}\label{eq:RepU}
		\mathbf u = \begin{pmatrix}
			\mathbf v_1  \dots \mathbf v_n
		\end{pmatrix}\begin{pmatrix} a_1 \\ \vdots \\a_n	\end{pmatrix} = a_1 \mathbf v_1 + \dots + a_n \mathbf v_n.
	\end{equation}	
	Once we pick a basis, that column of coordinates means something. If we denote our basis $\{\mathbf v_1, \dots, \mathbf v_n \}$ by $B$, then we will use the notation 
	\begin{equation*}
		\mathbf u = \begin{pmatrix} a_1 \\ \vdots \\a_n	\end{pmatrix}_B = a_1 \mathbf v_1 + \dots + a_n \mathbf v_n.
	\end{equation*}
	
	
	Let us do a very simple example to start. In the 2-D plane, say we have our original basis $\mathbf v_1, \mathbf v_2$ and we rotate it by $\pi/4$ radians to get a new basis. Say we have a point $P$ whose coordinate representation was $\mathbf v_1 + \mathbf v_2$, or $\begin{pmatrix} 1 \\ 1 \end{pmatrix}$ in the original basis.
	
	Now our new basis is the old one rotated by $\pi/4$ so 
	\begin{align*}
		\mathbf v_1' &= ~~ \frac{\sqrt 2}{2} \mathbf v_1 + \frac{\sqrt 2}{2} \mathbf v_2\\
		\mathbf v_2' &= - \frac{\sqrt 2}{2}\mathbf v_1 + \frac{\sqrt 2}{2}\mathbf v_2.
	\end{align*}
	\textbf{PUT GRAPHIC HERE}
	
	
	As a matrix transform, we can write this as\footnote{The tuple of basis vectors $\mathbf v_i$ is written as a row rather than a column to be consistent with Equation~\eqref{eq:RepU}. Then the coordinates are represented in a column. Because of the way we do matrix multiplication, then the matrix acts on the right. It's an issue of styling and indexing, and not physically meaningful. If we were to write this coordinate transform using columns \& not rows, we'd get a matrix that's the transpose of the one above, and matrix transposes would appear in subsequent equations, making them less tidy.}:
	\begin{equation*}
		\begin{pmatrix}
			\mathbf v_1' & \mathbf v_2'
		\end{pmatrix}
		= 
		\begin{pmatrix}
			\mathbf v_1 & \mathbf v_2
		\end{pmatrix}
		\begin{pmatrix}
					 \frac{\sqrt 2}{2} &  -\frac{\sqrt 2}{2} \\
					 \frac{\sqrt 2}{2} &  \frac{\sqrt 2}{2}
		\end{pmatrix}
	\end{equation*}
	This relates the actual basis vectors themselves. On the other hand, if we wanted to see the \emph{coordinates} representing $\mathbf v_1'$ and $\mathbf v_1'$, then in the new basis they would simply be represented in coordinates as: 
	\begin{equation*}
		\mathbf v_1' = \begin{pmatrix}
			1 \\ 0
		\end{pmatrix}_{\mathrm{new}}, ~
		\mathbf v_2' = \begin{pmatrix}
			0 \\ 1
		\end{pmatrix}_{\mathrm{new}}
	\end{equation*}
	
	and in the old basis they'd be represented as:
	
	\begin{equation*}
		\mathbf v_1' = \begin{pmatrix}
			\frac{\sqrt 2}{2} \\ \frac{\sqrt 2}{2}
		\end{pmatrix}_{\mathrm{old}}, ~
		\mathbf v_2' = \begin{pmatrix}
			-\frac{\sqrt 2}{2} \\ \frac{\sqrt 2}{2}
		\end{pmatrix}_{\mathrm{old}}
	\end{equation*}
		
	If we know that we can describe a point as $\begin{pmatrix}
					x \\ y
		\end{pmatrix}_{\text{new}}$
	in the new basis, then we can easily get its description in the old basis as:
	\begin{equation}\label{eq:ContravariantTransform}
		\begin{aligned}
			\begin{pmatrix}
						x \\ y
			\end{pmatrix}_{\mathrm{new}} 
			&= x \mathbf v_1' + y \mathbf v_2' \\
			&= x \begin{pmatrix}
				\frac{\sqrt 2}{2} \\ \frac{\sqrt 2}{2}
			\end{pmatrix}_{\mathrm{old}} 
			+ y \begin{pmatrix}
				-\frac{\sqrt 2}{2} \\ \frac{\sqrt 2}{2}
			\end{pmatrix}_{\mathrm{old}}
			\\ &=\left( \begin{pmatrix}
				 \frac{\sqrt 2}{2} &  -\frac{\sqrt 2}{2} \\
				 \frac{\sqrt 2}{2} &  \frac{\sqrt 2}{2}
			\end{pmatrix}_{\mathrm{old} \leftarrow \mathrm{new}}
			\begin{pmatrix}
						x \\ y
			\end{pmatrix}_{\mathrm{new}} \right)_{\mathrm{old}} 
		\end{aligned}
	\end{equation}
	
	% Note carefully that the matrix letting us go from new to old coordinates is the \emph{transpose}\index{Matrix!Transpose} of the one relating the basis vectors themselves. Since we do our calculations with coordinates, it is this matrix that we care more about.
	This is the same matrix that related the basis vectors. We'll call it $\mathbf A$. $\mathbf A$ takes the new coordinate representations $(x,y)$ and tells us how they'd look like in the \emph{old} basis. 

	
	So then it is the \emph{inverse} $\mathbf A^{-1}$ that tells us how our old coordinate representations of a point $P$ will look like in our new basis.
	
	For our point $P$, represented as $(1,1)$ in our original basis, in the new basis, we would have:
	\begin{equation*}
		P = \begin{pmatrix}
			1 \\ 1
		\end{pmatrix}_{\mathrm{old}}
		= \left( \begin{pmatrix}
			 \frac{\sqrt 2}{2} &  -\frac{\sqrt 2}{2} \\
			 \frac{\sqrt 2}{2} &  \frac{\sqrt 2}{2}
		\end{pmatrix}^{-1}_{\mathrm{new} \leftarrow \mathrm{old}} 		\begin{pmatrix}
			1 \\ 1
		\end{pmatrix}_{\mathrm{old}}\right)_\mathrm{new}
		= \begin{pmatrix}
			\sqrt 2 \\ 0
		\end{pmatrix}_{\mathrm{new}}
	\end{equation*}
	Indeed, $\mathbf v_1 + \mathbf v_2 = \sqrt 2 \mathbf v_1' + 0 \mathbf v_2'$.\\
	
	That is the central idea. If we \emph{vary}\index{Covariance} the \underline{basis} $\mathbf v_i$ to a different basis, $\mathbf v_i'$, then the \underline{coordinates} $a_i'$ will vary \index{Contravariance} the \emph{other} way, so that the geometric vector
	\begin{equation*}
		\mathbf u = a_1 \mathbf v_1 + \dots + a_n \mathbf v_n = a_1' \mathbf v_1' + \dots + a_n' \mathbf v_n'
	\end{equation*} is \emph{invariant} regardless of coordinate choice.
	
	Let's make this precise in the general case. If we start with a set of basis vectors $\{\mathbf v_1, \dots, \mathbf v_n\}$ and we make the linear transformation to a new basis $\{\mathbf v_1', \dots, \mathbf v_n' \}$ so that, as before:
	\begin{equation}\label{eq:CovariantTransform2}
		(\mathbf v_1', \dots, \mathbf v_n') = (\mathbf v_1, \dots, \mathbf v_n ) \mathbf A
	\end{equation}
	then since the vector $\mathbf u$ should not change when we change our basis, we must have: 
	\begin{equation}\label{eq:ContravariantTransform2}
		\begin{pmatrix}
			a_1' \\ \vdots \\ a_n'
		\end{pmatrix}
		= 
		\mathbf{A}^{-1}
		\begin{pmatrix}
			a_1 \\ \vdots \\ a_n
		\end{pmatrix}
	\end{equation}
	so that 
	\begin{equation*}
		(\mathbf v_1', \dots, \mathbf v_n') 
		\begin{pmatrix}
			a_1' \\ \vdots \\ a_n'
		\end{pmatrix} 
		= (\mathbf v_1, \dots, \mathbf v_n ) \mathbf A \mathbf A^{-1} 
		\begin{pmatrix}
			a_1 \\ \vdots \\ a_n
		\end{pmatrix}
		= (\mathbf v_1, \dots, \mathbf v_n ) 
		\begin{pmatrix}
			a_1 \\ \vdots \\ a_n
		\end{pmatrix}
	\end{equation*}
	as desired. \textbf{THIS WOULD BE A GOOD EXERCISE: PROVE IT HAS TO BE A INVERSE}
	
	We say that the basis vectors $\mathbf v_i$ \textbf{co-vary}\index{covariant} and the coordinates $a_i$ \textbf{contra-vary}\index{contravariant} with the change of basis. The idea, although it sounds simple, is rather hard to get the feel of. It's worth thinking a good bit about how coordinates and bases need to vary in opposite ways so that the physical object represented by the coordinates stays the same regardless of how we look at it. 
	
	\textbf{This will be a caption for a sketch of a 3-D rotation}
	
	 When you rotate your character in a video game (and in real life too, by the way), the world rotates \emph{contrary} to the direction that you've rotated in. That's because the coordinates of what you see have \emph{contra-varied} while your basis vectors, given by the direction you face have \emph{co-varied}. The end result is that despite changing your coordinate system, physics stays the same: invariant. The universe did not rotate itself just because you did. This extends beyond just rotations to \emph{all} linear transformations point. \\
	
	\section{The Notion of Length on Vector Spaces} % (fold)
	\label{sec:the_notion_of_length_on_vector_spaces}
	
	Let us consider the property of orthogonality. It's well known that for geometric vectors, there's more that we can do than just add, scale, and transform them: we can take dot products\footnote{Of course in 3-D we can also take a cross product. This will be discussed in the following chapters.} between them. When we have two geometric vectors in space, their dot product is a well defined number. If it is zero, then the vectors are orthogonal to one another. From the dot product and the magnitudes, it is possible to calculate the angle between two given vectors. 
	
	When vectors are represented in terms of tuples of numbers, the dot product was taught to us as ``multiply component by component, and then sum that up". This is not, in general, what the dot product really is. Consider a basis transformation as below:
	\begin{align*}
		\mathbf v_1' &= 2 \mathbf v_1 + \mathbf v_2 \\
		\mathbf v_2' &=  \mathbf v_1 + 2 \mathbf v_2.
	\end{align*}
	It's easy to compute the inverse of this matrix and see how the new coordinates should work, but it worthwhile looking at this geometrically. It is not a rotation, but more of a ``stretching''. Notice that while in our original perspective, if we viewed $\mathbf v_1$ and $\mathbf v_2$ as orthogonal vectors, in the \emph{new} perspective, they are \emph{no longer} orthogonal. This is very important: linear transformations in general do not preserve orthogonality.
	
	\textbf{Include graphic here}
	
	In particular, this linear transformation has \emph{stretched} our vector space and changed the notion of distance. Even though rotations keep distances preserved, general linear transformations don't care about a notion of distance. 
	
	If we were to take the ``dot product" $\mathbf v_1' \cdot \mathbf v_2'$ just by multiplying corresponding coordinates and summing them up, then in the new basis we'd get zero, but in the original basis we would \emph{not}. This dot product actually changes depending on the coordinate system that we use! In some sense, this is expected: all we're doing is multiplying contravariant coordinates together and summing them up. The result should be contravariant as well (in fact doubly contravariant).
	
	% But then does that mean that the state of being orthogonal is coordinate-dependent? Two vectors can be orthogonal in one coordinate system and not in another? Immediately, you should say ``No, the state of being perpendicular is a real-world observation that doesn't depend on what coordinate system you use.'' So what is going on? The only possibility is that we're not doing the dot product right. We can't just multiply the contravariant coordinates: we need to make use of both the coordinates and the vectors.
	
	The failure of the dot product to be invariant is intimately related to the fact that transformations can change lengths. This should not be too surprising. After all, the length of a vector is defined by the square root of its dot product with itself. If the dot product we learned is not invariant under general coordinate transformations, what is the right way to measure length? 
	
	It is here that there is a big subtlety. A vector space on its own does not have a notion of length. We've just seen choosing different bases would give rise to different length scales and notions of ``perpendicular'' as well. Endowing a vector space with a way to universally tell what the length of a vector is, or whether two vectors are perpendicular is actually adding \emph{extra structure} to the space. It picks a whole class of specific coordinate systems and says ``these are the orthogonal reference frames; the others are skewed and stretched perspectives''. This allows us to measure length using an invariant \emph{inner product}\index{Inner Product}. 
	
	Euclidean space, as well as the world in which we live in, both have an natural way to measure length between two points that is invariant of the coordinate system used. A vector space on its own does not, and so it is called an \emph{affine space}\index{Affine Space}. In affine space, although there are notions like ``parallel'', there is not a notion of distance. Adding an inner product to affine space gives rise to Euclidean space\index{Euclidean Space}. This will be discussed in much greater detail in the following two chapters. 
	
	
	%
	% In the succeeding chapter, we will study how to define these \emph{inner products}\index{Inner Product} on vectors. To obtain the invariant scalar from the contravariant coordinates, we will need to introduce a doubly-covariant quantity called the \emph{metric tensor}\index{Metric Tensor}.
	%
	%  As soon as we define a product operation between vectors $\mathbf v \cdot \mathbf w$, we have added structure to our vector space. An inner product makes it so that not all bases of vectors are equal. There are bases where the basis vectors satisfy $v_i \cdot v_j = 0$ unless $i = j$, meaning the basis vectors are all orthogonal. The orthogonal bases are special among the set of all bases. Not all linear transformations, therefore, will preserve orthogonal bases: only orthogonal ones will.
	%

	% section the_notion_of_length_on_vector_spaces (end)

	\section{Nonlinear Coordinate Systems are Locally Linear} % (fold)
	\label{sec:nonlinear_coordinate_systems_are_locally_linear}
	
	Perhaps you may be wondering why we've spent so much time on changing between coordinate systems represented by basis vectors centered at a fixed origin. Consider the change between cartesian and polar coordinates. What does this have to do with the linear changes of coordinates that we've been discussing?
	
	We could use something like a polar system of $(r,\theta)$ or a spherical system $(r, \phi, \theta)$. These coordinate systems are not representable in terms of axes, but instead look like this:
	
	\textbf{Graphic of polar coordinate system/spherical} 
	
	This is an example of a non-linear coordinate transformation. They are more commonly referred to as \textbf{curvilinear}. Whereas linear ones map lines to lines, curvilinear ones more generally map lines to curves. The idea for making sure that the equations of physics still stay true for non-linear coordinate transformations is to note that just like a curve locally looks like a line, a \emph{non-linear} transformation locally looks like a \emph{linear} one. The linear transformation that it locally looks like is called the \textbf{Jacobian} \index{Jacobian} $J$. If the laws of physics are invariant under linear transformations locally at each point, then \emph{globally}, they will be invariant under non-linear ones as well. That is why we cared about studying covariance and contravariance for linear transformations: more complicated cases can be reduced to their local linear behavior.
	
	As an example, consider going from a cartesian to a polar coordinate system. We have $x = r \cos \theta$ and $y = r \sin \theta$. Certainly, this is not a linear transformation of coordinates. There is sinusoidal dependence on $\theta$ in this transformation. Physics and geometry, however, do not have laws in terms of absolute coordinates (it doesn't make sense to say ``That object is located at 50 meters'') but only in terms of relative distances (you'd instead say ``That object is located 50 meters \emph{relative to} me"). It is the changes over relative distances between points that we care about, and these are obtained by integrating the \emph{infinitesimal} changes at each point. 
	
	So although $x,y$ do not depend linearly on theta, through the use of the chain rule, we have a local linear relationship in their infinitesimal changes:
	\begin{align*}
		dx = \cos \theta ~dr - r \sin \theta ~d\theta\\
		dy = \sin \theta ~dr + r \cos \theta ~d\theta
	\end{align*}
	At any given point, this relationship can be written as a linear change of basis.
	\begin{equation*}
		\begin{pmatrix}
			dx \\ dy
		\end{pmatrix}
		 = 
		 \begin{pmatrix}
		 	\cos \theta & - r \sin \theta \\
			\sin \theta & r \cos \theta
		 \end{pmatrix}
		 \begin{pmatrix}
		 	dr \\
			d\theta
		 \end{pmatrix}
	\end{equation*}
	So every nonlinear transformation from some coordinate system $x_1 \dots x_n$ to $x' \dots x_n'$ has the local linear transformation law:
	\begin{align*}
		\begin{pmatrix}
			dx_1 \\ \vdots \\ dx_n 
		\end{pmatrix}_{\text{old}}
		=
	 \begin{pmatrix}
	 	\frac{\partial x_1}{\partial x'_1} & \dots & \frac{\partial x_1}{\partial x'_n} \\
		\vdots & \ddots & \vdots \\
		\frac{\partial x_n}{\partial x'_1} & \dots & \frac{\partial x_n}{\partial x'_n}
	 \end{pmatrix}_{\text{old} \leftarrow \text{new}}
	 \begin{pmatrix}
	 	dx'_1 \\ \dots \\ dx'_n
	 \end{pmatrix}_{\text{new}}\\
	 \Rightarrow 
	 \begin{pmatrix}
	 	dx'_1 \\ \dots \\ dx'_n
	 \end{pmatrix}_{\text{new}}
		=
	 \begin{pmatrix}
	 	\frac{\partial x_1}{\partial x'_1} & \dots & \frac{\partial x_1}{\partial x'_n} \\
		\vdots & \ddots & \vdots \\
		\frac{\partial x_n}{\partial x'_1} & \dots & \frac{\partial x_n}{\partial x'_n}
	 \end{pmatrix}^{-1}_{\text{new} \leftarrow \text{old}}
	\begin{pmatrix}
		dx_1 \\ \vdots \\ dx_n 
	\end{pmatrix}_{\text{old}}
	\end{align*}
	This is exactly the analogue of Equation~\eqref{eq:ContravariantTransform2}, so indeed the changes in coordinates $dx_i$ can be called \emph{contravariant}, just like the coordinates were for the linear transformation case. All of this is just an extension of the principle of local linearity from calculus.
	
	Similarly, we can express the new derivative operators in terms of the old ones by using the chain rule. For polar coordinates we have
	
	\begin{align*}
		\frac{\partial f}{\partial r} = \frac{\partial x}{\partial r}\frac{\partial f}{\partial x}  +  \frac{\partial y}{\partial r} \frac{\partial f}{\partial y} \\
		\frac{\partial f}{\partial \theta} = \frac{\partial x}{\partial \theta}\frac{\partial f}{\partial x}  + \frac{\partial y}{\partial \theta}\frac{\partial f}{\partial y}
	\end{align*}
	or more compactly we can relate just the differential operators themselves: 
	\begin{align*}
		\begin{pmatrix}
			\frac{\partial}{\partial r} \\ \frac{\partial}{\partial \theta}
		\end{pmatrix}
		 = 
		 \begin{pmatrix}
		 	\frac{\partial x}{\partial r} & \frac{\partial y}{\partial r} \\
			\frac{\partial x}{\partial \theta} & \frac{\partial y}{\partial \theta}
		 \end{pmatrix}
		 \begin{pmatrix}
		 	\frac{\partial}{\partial x} \\ \frac{\partial}{\partial y} \\
		 \end{pmatrix}
	\end{align*}
	so that more generally: 
	\begin{equation}\label{eq:partial_transform}
		\begin{pmatrix}
			\frac{\partial}{\partial x'_1} \\ \vdots \\ \frac{\partial}{\partial x'_n} 
		\end{pmatrix}_{\text{new}}
		=
	 \begin{pmatrix}
	 	\frac{\partial x_1}{\partial x'_1} & \dots & \frac{\partial x_1}{\partial x'_n} \\
		\vdots & \ddots & \vdots \\
		\frac{\partial x_n}{\partial x'_1} & \dots & \frac{\partial x_n}{\partial x'_n}
	 \end{pmatrix}_{\text{new} \leftarrow \text{old}}^T
	 \begin{pmatrix}
	 	\frac{\partial}{\partial x_1} \\	\vdots \\ \frac{\partial}{\partial x_n}
	 \end{pmatrix}_{\text{old}}
	\end{equation}
	If you look carefully, you will see that this accordingly mirrors the covariant change of vectors in Equation~\eqref{eq:CovariantTransform2}. So while the infinitesimal changes in the coordinates themselves are contravariant, just like the linear coordinates themseles, the \emph{differential operators} corresponding to changes in these coordinates become \emph{covariant}, just like the vectors $\mathbf v_i$ in the linear case. This is the first correspondence that will hint that our basis vectors $\mathbf v_i$ actually \emph{correspond} to the differential operators $\frac{\partial}{\partial x_i}$.
	
	Indeed, as we begin to move away from 3-D and $n$-D Euclidean space, we will see why the old notions of unit vectors $\mathbf{\hat i}, \mathbf{\hat j}, \mathbf{\hat k}$ are better viewed as the operators $\frac{\partial}{\partial x}, \frac{\partial}{\partial y},$ and $ \frac{\partial}{\partial z}$, and in general $\mathbf v_i \rightarrow \frac{\partial}{\partial x_i}$. This change of notation will allow us to easily pass onto far more general spaces than the Euclidean ones we've gotten used to. 
	
	% section nonlinear_coordinate_systems_are_locally_linear (end)
	
	\section{Einstein's Summation Convention} % (fold)
	\label{sec:einstein_s_summation_convention}
	
	Row tuples, column tuples, matrices representing basis transformations (old to new and new to old), co-variance and contra-variance. These ideas have constituted the entirety of this first chapter, and although hopefully they have not been too difficult conceptually, the matrix manipulation even at this early level is already a pain. We have to write everything in terms of tuples, and we need to arbitrarily decide which ones are rows and which ones are columns, and which matrices are transposed so that all the matrix multiplications make sense, as defined in linear algebra. A young physicist named Albert Einstein used a convention of writing all these equations so that we did not have to explicitly write out tuples of abstract basis vectors and coordinates.
	
	The first step is to avoid explicitly writing out row tuples and column tuples. To do this, instead of writing out a whole tuple to represent a vector like $(v_1, \dots, v_n)$ we will simply write $\mathbf v_i$. $v_i$ should be viewed as the whole vector, rather than an $ith$ component. The index $i$ is \emph{free} and can be anything. 
	
	The reason it is preferable to view $v_i$ as the whole vector rather than a specific $i$th component of it is because of the same reason given at the end of Section~\ref{sec:Cartesian}, that we never care about just a specific component, but rather the vector as a whole. 
	
	The second step is to be able to differentiate between \emph{covariant} quantities and contravariant quantities. The convention is this: if the quantity is covariant, like a basis vector, then write its index \emph{downstairs}: $\mathbf v_i$. On the other hand, of a quantity is contravariant (like a coordinate) then write its index \emph{upstairs} as $a^i$ instead\footnote{If you are worried that this will be confused with exponentiation, don't be. In practice, such confusion rarely arises.}. Then we can write Equation~\eqref{eq:RepU} as 
	
	\begin{equation}
		\mathbf u = \sum_{i} a^i \mathbf v_i
	\end{equation}
	
	For another example, the gradient operator was written $\nabla = (\partial/\partial x_1, \dots, \partial/\partial x_n)$ and is now written simply as $\partial/\partial x^i$ (adopting upper indices for $x^i$ since coordinates contra-vary). This means that Equation~\eqref{eq:partial_transform} can be written as
	\begin{equation}\label{eq:partial_transform2}
		\frac{\partial}{\partial {x'}^i} = \sum_{j} \frac{\partial x^j}{\partial {x'}^i} \frac{\partial}{\partial x^j}.
	\end{equation}
	
	Since $i$ is free to be anything (while $j$ is bound since it is being summed over), this equation holds for every $i$ in $1,\dots, n$ and so is indeed an equation relating two \emph{vectors} on both sides. Note also that this means the transformation matrix can be written as $\frac{\partial x^j}{\partial {x'}^i}$. By writing just a summation and not any explicit matrices, we avoid having to worry about unnecessary troubles like transposes, etc. Note also that this matrix has both upper and lower indices so that the upper index in $\frac{\partial x^j}{\partial {x'}^i}$ gets multiplied with the lower index $\frac{\partial}{\partial x^j}$ to cancel out, and all that remains is the lower index $i$. All the trouble of seeing what's covariant, what's contravariant, and what's invariant is washed away into just seeing how the upper indices and lower indices cancel out in the end.
	
	Sometimes in the mathematics literature in places where co- and contra-variance are of lesser importance, upper and lower indices are ignored and the notation describing something like a matrix-vector multiplication looks like this:
	\begin{equation*}
		(\mathbf A \vec v)_i = \sum_{j} \mathbf A_{ij} \vec v_j.
	\end{equation*}
	That is, the $i$th component of the product $\mathbf A \vec v$ is the sum over $j$ of the $ij$th component of $\mathbf A$ with the $j$th component of $\vec v$. For a matrix-matrix multiplication this would look like
	\begin{equation*}
		(\mathbf A \mathbf{B})_{ik} = \sum_{j} \mathbf A_{ij} \mathbf B_{jk}.
	\end{equation*}
	Notice the pattern: when we wish to multiply these objects, we sum over a common index ($j$ in the above equations) that we make both objects share. The point is \emph{an index is always repeated and summed over} when we do a multiplication. 
	
	Einstein took the bold, but ultimately brilliant step of making the convention that if we ever write an index twice, that \emph{automatically means} that we are summing over it (unless we explicitly say we aren't). The above equations, in Einstein's scheme, now become
	\begin{align*}
		(\mathbf A \vec v)_i & =  \mathbf A_{ij} \vec v_j \\
		(\mathbf A \mathbf{B})_{ik} & = \mathbf A_{ij} \mathbf B_{jk}.
	\end{align*}
	A dot product between $v$ and itself would simply be $\vec v\cdot \vec v = \vec v_i \vec v_i$ in Einstein's convention.
	
	Now returning back to caring about co-variance and contra-variance, this scheme makes every equation in the chapter shockingly short. Equation~\eqref{eq:RepU} becomes $\mathbf u = a^i \mathbf v_i$, Equation~\eqref{eq:CovariantTransform2} becomes $\mathbf v_i' = A^j_i \mathbf v_j$, Equation~\eqref{eq:ContravariantTransform2} becomes $a_i' = (A^{-1})^j_i a_j$. Equation~\eqref{eq:partial_transform2} is further reduced to just 
	\begin{equation*}
		\frac{\partial}{\partial {x'}^i} = \frac{\partial x^j}{\partial {x'}^i} \frac{\partial}{\partial x^j}.
	\end{equation*}
	Similarly the transformation of infinitesimal coordinate changes is now just
	\begin{equation*}
		dx'^i = \frac{\partial {x'}^i}{\partial x^j} dx^j.
	\end{equation*}
	
	\textbf{EXERCISE: Check invariance quickly using this method.}

	By doing exercises, this method is very quick and steady to get the hang of. We will adopt it for the rest of the book.
	
	% section einstein_s_summation_convention (end)
	
\end{document}