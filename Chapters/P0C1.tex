\documentclass[../master.tex]{subfiles}
 
 
\begin{document}
	
	\chapter{Old Notions}
	\section{The Cartesian Coordinate System} % (fold)
	\label{sec:Cartesian}
	
	
	
	One of the most important revolutions in mathematics and physics was ushered in by an idea of the seventeenth century mathematician and philosopher Ren\'{e} Descartes. The idea was that any point $P$ in the Euclidean plane could be represented by a pair of numbers $(x,y)$. The numbers themselves represented distances along two perpendicular axes that met at a point $(0,0)$, called the origin. By introducing this concept, he had done something amazing. He had related the \emph{geometry} of the plane to the \emph{algebra} of variables and equations. Algebra could be \emph{represented} geometrically, and conversely geometric problems could be solved by going into the realm of algebra. 
	
	\textbf{Insert 2D Plane with Coordinates}
	
	Coordinates soon became more than just pairs of numbers $(x,y)$. Their use was extended to 3D space, and later to arbitrarily high dimensions. They would subsequently be used to lay the foundations for modern physics and mathematics. Linear algebra, multivariable calculus, and all the connections between algebra and geometry begin with the concept of a coordinate.
	Since then, the use of coordinate systems has proven indispensable to physicists and mathematicians throughout history. Newton used Descarte's coordinate system to formulate his infinitesimal calculus. Maxwell used it to analyze electromagnetic fields, discovering mathematically that light is a wave in the electromagnetic field. Einstein, going further, made use of coordinate systems to formulate his theory of gravitation. Today, physicists and engineers do their calculations within the frame of coordinate systems. In mathematics, Descarte's idea planted the roots for what would turn into the modern field of algebraic geometry. \\
	
	When studying a geometric phenomenon in some $n$-dimensional space, say $\mathbb{R}^n$, we pick an origin and axes to form our coordinate system. For a ball falling, we could set the origin at some point on the ground, and pick one axis parallel to the ground, and one perpendicular. We can decide to measure the axes in meters, or we could decide to do it in feet (nothing stops us from making bad choices). The physical point $P$ where the ball lies is represented by $(x,y)=(0~ \mathrm m,10~ \mathrm m)$. The coordinate $y$ is a natural choice of coordinate, as it corresponds to our intuitive notion of height. 
	
	\textbf{Insert Ball Falling}
	
	We can now study $y$, free of geometry, as just a function which we can do arithmetic and calculus on. If we are given an equations of motion, say 
	\begin{equation*}
		\frac{d^2y}{dt^2} = -g, \hspace{5mm} \frac{d^2x}{dt^2} = 0
	\end{equation*} 
	with initial conditions,
	\begin{equation*}
		\frac{dy}{dt}=0, \hspace{5mm} \frac{dx}{dt}=0
	\end{equation*}
	 then we can perform our well-known kinetic calculations for the system, and see how the system evolves in the \emph{time} direction. **A recurring theme will be that dynamics of a system in $n$-dimensional space can be thought of just a special type of geometry in $n+1$ dimensional space, putting time as an added dimension**.\\
	
	Because this book will, in large part, be concerned with studying the ways in which geometry, algebra, and physics connect, it is worthwhile to dwell on the \emph{philosophy} behind coordinate systems.
	
	The ball will fall from 10 meters, according to the force of gravity. That is the way the world works. It doesn't matter what coordinate system we set up to do that calculation, we should get the \emph{exact same result}. Plainly: nature doens't \emph{care} what coordinate system we use. This fact, obvious as it may be, is worth thinking about: No matter what coordinate system we use, the equation of motion should give the same dynamics. The laws of physics should be \emph{independent of any coordinate system}.
	
	Newton's law $\mathbf F = m \mathbf a$ relates the force vector to the acceleration vector. The vector representing the force $\mathbf F$ that you apply on a surface is an object independent of coordinate system, and so is the resulting acceleration vector. The \emph{components} of these vectors ($F_x, F_y, F_z$) and ($a_x, a_y, a_z$), however, depend on what you have chosen for the $x,y,z$ axes. These components \emph{represent} a real physical vector, but only once we pick a coordinate system. If we were to pick a different coordinate system, they would change. If a physical law ever looked like $F_x dx = dW$ it would not be true in every coordinate system, because it puts emphasis on just one of the components over the others. While in some coordinate system this may be nonzero, in another it would be zero, so the work done, $dW$, is not an invariant. So such a law would not be universally valid regardless of what coordinate system we pick: it would be wrong. The need for such invariance is why the true formula looks like
	\begin{equation*}
		\mathbf F \cdot  d \mathbf r = F_x dx + F_y dy + F_z dz = d W.
	\end{equation*}
	 Although its not yet obvious that this is invariant under change of coordinates, at the very least it doesn't put one component above any of the others.
	
	
		% In antiquity, Pythagoras discovered that for a right triangle with side lengths $a,b$ and hypotenuse $c$, the following equation related the lengths:
	% 	\begin{equation*}
	% 		a^2 + b^2 = c^2
	% 	\end{equation*}
	% 	Nowadays to us, this equation is interesting to know, and not too hard to prove. To Pythogoras and his students, however, it was absolutely stunning. The reason was that at that time, mathematics was divided into two fields: geometry and arithmetic. Geometry reasoned with measurements and constructions of figures in the plane, while arithmetic dealt with studying equalities of how numbers combined.
	%
	% 	Although equality played a central role in both fields,
	
	% section Descartes (end)
	
	\section{Linear Algebra \& Coordinates} 
	\label{sec:Linear Algebra & Coordinates}% (fold)
	
	The traditional conception of a coordinate system, a series of perpendicular lines that together associate ordered tuples of numbers to each point in space, is not representative of all coordinate systems. For one, we do not need the requirement that the lines be perpendicular (we'll show later that for general spaces, there's not even a good notion for what perpendicular \emph{means}). Our coordinate system could instead look like this:
	
	\textbf{Graphic of non-perp lines and representing a point like that}
	
	In the language of linear algebra, once we choose an origin point, choosing a set of coordinate axes is the same as choosing a basis for the space (a coordinate basis). We can relate the new system of coordinates $x_i'$ in terms of the old system $x_i$ by matrix multiplication: $x_i' = \sum_{j=1}^n \mathbf A_{ij} x_j$. This is exactly what's called a change of basis in linear algebra. Transformations between coordinate bases are exactly the invertible \textbf{linear transformations} \index{Linear Algebra!Linear Transformation}.
	
	As in linear algebra, we need our coordinate system to both \textbf{span}\index{Linear Algebra!Span} the space so that we can represent any point, and be \textbf{linearly independent}\index{Linear Algebra!Linear Independence} so that every point has exactly \emph{one} representation in our coordinate system. That's what a basis \emph{is}: it specifies a good coordinate system. 
	
	\begin{defn}
		A set of vectors is said to span a space $\mathbb{R}^n$ if every point $P$ can be represented as $a_1 \mathbf v_1 + \dots a_n \mathbf v_n$
	\end{defn}
	
	\begin{defn}
		A set of vectors $\{\mathbf v_1, \dots , \mathbf v_k \}$ is called linearly independent if there is only one way to represent the zero vector $\mathbf 0$ as a combination of them, namely as $\mathbf 0 = 0v_1 + \dots + 0 v_k$.
	\end{defn}
	
	This second definition is the same as saying every point has a unique representation. If there were two ways to represent a point $P$: as \begin{equation*}
		a_1 \mathbf v_1 + \dots + a_n \mathbf v_n
	\end{equation*} 
	and 
	\begin{equation*}
		b_1 \mathbf v_1 + \dots + b_n \mathbf v_n
	\end{equation*} 
	then subtracting these two different combinations would give a nonzero way to represent zero.  Conversely, if there were a nonzero combination of vectors summing to zero, then we could add that combination to a coordinate representation of any point and get a \emph{different} representation of the same point. So coordinate representations for all vectors are unique as long as long as there is only one representation for zero: the trivial one where each component is zero.
	
	Intuitively, linear independence means that there is no superfluous information in the set of vectors.  We cannot linearly combine vectors in some subset to get another vector in the set; each vector is adding its own unique additional piece of information, making the set able to span in an additional direction.
	
	Bases that don't span, or are not linearly independent, would lead to coordinate systems like these:
	
	\textbf{Show a 2-D basis in a 3-D space, and a basis of 3 vectors in 2-D space}

	Very often in mathematics, we ask ``does a solution exist?'', and ``if there is a solution, is it unique?''. These two questions are dual to one another. If a set of vectors spans the space, then there \emph{exists} a way to represent any point (at least one way to represent any point). If a set of vectors is linearly independent, then \emph{if} you can represent a point, that representation is \emph{unique} (no more than one way to represent any point).
	
	Now to stress the same idea again: because points in $\mathbb R^n$ and vectors are essentially the same thing, the idea that points in space are invariant of a coordinate system of course applies to vectors. If we choose a basis for our vector space $\mathbf v_1, \dots \mathbf v_n$, then we can express any vector $\mathbf u$ by a unique combination $\mathbf u = a_1 \mathbf v_1 + \dots + a_n \mathbf v_n$. We then say that in this basis, we can represent $\mathbf u$ by a list of numbers:
	\begin{equation*}
		\mathbf u = \begin{pmatrix} a_1 \\ \vdots \\a_n	\end{pmatrix}.
	\end{equation*}
	
	In some sense, writing this as an equality is wrong. The vector $\mathbf u$ represents something physical: a velocity, a force, the flow of water. It doesn't depend on the coordinate system. On the other hand, the right hand side is just a list of numbers that depend entirely on the coordinate system chosen. If we change coordinate systems, the right hand side changes. Because $\mathbf u$ exists (say, in the real world) independently of coordinates used, it does not change.
	
	A vector is \emph{not} a list of numbers. Once we pick a basis, a vector can be \emph{represented by} a list of numbers, but if we change into a different basis, those numbers all have to change as well. This exact same idea will be the reason why a tensor is \emph{not} just a multi-dimensional array (like the ones you see in computer science). It can be \emph{represented by} a multi-dimensional array once we pick a coordinate system, but the numbers in entries will change depending on the coordinate system we pick. 
	
	A more careful way to write $\mathbf u$ would be:
	
	\begin{equation}\label{eq:RepU}
		\mathbf u = \begin{pmatrix}
			\mathbf v_1  \dots \mathbf v_n
		\end{pmatrix}\begin{pmatrix} a_1 \\ \vdots \\a_n	\end{pmatrix} = a_1 \mathbf v_1 + \dots + a_n \mathbf v_n.
	\end{equation}	
	Once we pick a basis, that column of coordinates means something. If we denote our basis $\{\mathbf v_1, \dots, \mathbf v_n \}$ by $B$, then we will use the notation 
	\begin{equation*}
		\mathbf u = \begin{pmatrix} a_1 \\ \vdots \\a_n	\end{pmatrix}_B = a_1 \mathbf v_1 + \dots + a_n \mathbf v_n.
	\end{equation*}
	
	
	Let us do a very simple example to start. In the 2-D plane, say we have our original basis $\mathbf v_1, \mathbf v_2$ and we rotate it by $\pi/4$ radians to get a new basis. Say we have a point $P$ whose coordinate representation was $\mathbf v_1 + \mathbf v_2$, or $\begin{pmatrix} 1 \\ 1 \end{pmatrix}$ in the original basis.
	
	Now our new basis is the old one rotated by $\pi/4$ so 
	\begin{align*}
		\mathbf v_1' &= ~~ \frac{\sqrt 2}{2} \mathbf v_1 + \frac{\sqrt 2}{2} \mathbf v_2\\
		\mathbf v_2' &= - \frac{\sqrt 2}{2}\mathbf v_1 + \frac{\sqrt 2}{2}\mathbf v_2.
	\end{align*}
	\textbf{PUT GRAPHIC HERE}
	
	
	As a matrix transform, we can write this as\footnote{We're going to put the basis vectors in row-tuples instead of column tuples. The reason is due to Equation~\eqref{eq:RepU}, where we've decided to have the coordinates be represented in a column. Because of the way we do matrix multiplication, we then want the basis vectors to be rows for the multiplication to make sense. It's really just an issue of styling and indexing, and not a physical issue. If we were to write the basis vector transform in terms of columns, we'd get a matrix that's the transpose of the one above, and matrix transposes would appear in subsequent equations, making them less clean.}:
	\begin{equation*}
		\begin{pmatrix}
			\mathbf v_1' & \mathbf v_2'
		\end{pmatrix}
		= 
		\begin{pmatrix}
			\mathbf v_1 & \mathbf v_2
		\end{pmatrix}
		\begin{pmatrix}
					 \frac{\sqrt 2}{2} &  -\frac{\sqrt 2}{2} \\
					 \frac{\sqrt 2}{2} &  \frac{\sqrt 2}{2}
		\end{pmatrix}
	\end{equation*}
	
	This relates the actual basis vectors themselves. On the other hand, if we wanted to see the \emph{coordinates} representing $\mathbf v_1'$ and $\mathbf v_1'$, then in the new basis they would simply be represented in coordinates as: 
	
	\begin{equation*}
		\mathbf v_1' = \begin{pmatrix}
			1 \\ 0
		\end{pmatrix}_{\mathrm{new}}, ~
		\mathbf v_2' = \begin{pmatrix}
			0 \\ 1
		\end{pmatrix}_{\mathrm{new}}
	\end{equation*}
	
	and in the old basis they'd be represented as:
	
	\begin{equation*}
		\mathbf v_1' = \begin{pmatrix}
			\frac{\sqrt 2}{2} \\ \frac{\sqrt 2}{2}
		\end{pmatrix}_{\mathrm{old}}, ~
		\mathbf v_2' = \begin{pmatrix}
			-\frac{\sqrt 2}{2} \\ \frac{\sqrt 2}{2}
		\end{pmatrix}_{\mathrm{old}}
	\end{equation*}
		
	If we know that we can describe a point through coordinates as $\begin{pmatrix}
					x \\ y
		\end{pmatrix}$
	in the new basis, then we can easily get its description in the old basis as:
	\begin{equation}\label{eq:ContravariantTransform}
		\begin{aligned}
			\begin{pmatrix}
						x \\ y
			\end{pmatrix}_{\mathrm{new}} 
			&= x \mathbf v_1' + y \mathbf v_2' \\
			&= x \begin{pmatrix}
				\frac{\sqrt 2}{2} \\ \frac{\sqrt 2}{2}
			\end{pmatrix}_{\mathrm{old}} 
			+ y \begin{pmatrix}
				-\frac{\sqrt 2}{2} \\ \frac{\sqrt 2}{2}
			\end{pmatrix}_{\mathrm{old}}
			\\ &=\left( \begin{pmatrix}
				 \frac{\sqrt 2}{2} &  -\frac{\sqrt 2}{2} \\
				 \frac{\sqrt 2}{2} &  \frac{\sqrt 2}{2}
			\end{pmatrix}_{\mathrm{old} \leftarrow \mathrm{new}}
			\begin{pmatrix}
						x \\ y
			\end{pmatrix}_{\mathrm{new}} \right)_{\mathrm{old}} 
		\end{aligned}
	\end{equation}
	
	% Note carefully that the matrix letting us go from new to old coordinates is the \emph{transpose}\index{Matrix!Transpose} of the one relating the basis vectors themselves. Since we do our calculations with coordinates, it is this matrix that we care more about.
	This is the same matrix that related the basis vectors. We'll call it $\mathbf A$. $\mathbf A$ takes the new coordinate representations $(x,y)$ and tells us how they'd look like in the \emph{old} basis. 
		%
	% So when we expressed $\mathbf v_1'$ in terms of the $\mathbf v_i$ above, that gives us the column vector representing $\mathbf v_1'$ in terms of our initial basis. The coefficients of that linear combination form the first \emph{column} of $\mathbf A$. On the other hand, the linear relationship between the vectors themselves must therefore be written in terms of the transpose
	% \begin{equation*}
	% 	\begin{pmatrix} \mathbf v_1' \\ \mathbf v_2'
	%
	% 		\end{pmatrix} = \mathbf A^T
	% 		\begin{pmatrix} \mathbf v_1 \\ \mathbf v_2
	%
	% 	\end{pmatrix}
	% \end{equation*}
	% or equivalently,
	% \begin{equation*}
	% 	(\mathbf v_1', \mathbf v_2')
	% 	=
	% 	( \mathbf v_1, \mathbf v_2 ) \mathbf A
	% \end{equation*}
	% We can view the change of basis as a matrix acting on the right of the row made out of the basis vectors.
	
	So then it is the \emph{inverse} $\mathbf A^{-1}$ that tells us how our old coordinate representations of a point $P$ will look like in our new basis.
	
	For our point $P$, represented as $(1,1)$ in our original basis, in the new basis, we would have:
	\begin{equation*}
		P = \begin{pmatrix}
			1 \\ 1
		\end{pmatrix}_{\mathrm{old}}
		= \left( \begin{pmatrix}
			 \frac{\sqrt 2}{2} &  -\frac{\sqrt 2}{2} \\
			 \frac{\sqrt 2}{2} &  \frac{\sqrt 2}{2}
		\end{pmatrix}^{-1}_{\mathrm{new} \leftarrow \mathrm{old}} 		\begin{pmatrix}
			1 \\ 1
		\end{pmatrix}_{\mathrm{old}}\right)_\mathrm{new}
		= \begin{pmatrix}
			\sqrt 2 \\ 0
		\end{pmatrix}_{\mathrm{new}}
	\end{equation*}
	Indeed, $\mathbf v_1 + \mathbf v_2 = \sqrt 2 \mathbf v_1' + 0 \mathbf v_2'$, as we wanted. \\
	
	This is the idea. If we \emph{vary}\index{Covariance} the $\mathbf v_i$ to a different basis, $\mathbf v_i'$, then the coordinates $a_i'$ will vary \index{Contravariance} the \emph{other} way, so that 
	\begin{equation*}
		\mathbf u = a_1 \mathbf v_1 + \dots + a_n \mathbf v_n = a_1' \mathbf v_1' + \dots + a_n' \mathbf v_n'
	\end{equation*} is \emph{invariant} regardless of coordinate choice.
	
	Let's make this precise in the general case. If we start with a set of basis vectors $\{\mathbf v_1, \dots, \mathbf v_n\}$ and we make the linear transformation to a new basis $\{\mathbf v_1', \dots, \mathbf v_n' \}$ so that, as before:
	\begin{equation}\label{eq:CovariantTransform2}
		(\mathbf v_1', \dots, \mathbf v_n') = (\mathbf v_1, \dots, \mathbf v_n ) \mathbf A
	\end{equation}
	then since the vector $\mathbf u$ should not change when we change our basis, we must have: 
	\begin{equation}\label{eq:ContravariantTransform2}
		\begin{pmatrix}
			a_1' \\ \vdots \\ a_n'
		\end{pmatrix}
		= 
		\mathbf{A}^{-1}
		\begin{pmatrix}
			a_1 \\ \vdots \\ a_n
		\end{pmatrix}
	\end{equation}
	so that 
	\begin{equation*}
		(\mathbf v_1', \dots, \mathbf v_n') 
		\begin{pmatrix}
			a_1' \\ \vdots \\ a_n'
		\end{pmatrix} 
		= (\mathbf v_1, \dots, \mathbf v_n ) \mathbf A \mathbf A^{-1} 
		\begin{pmatrix}
			a_1 \\ \vdots \\ a_n
		\end{pmatrix}
		= (\mathbf v_1, \dots, \mathbf v_n ) 
		\begin{pmatrix}
			a_1 \\ \vdots \\ a_n
		\end{pmatrix}
	\end{equation*}
	as desired. \textbf{THIS WOULD BE A GOOD EXERCISE: PROVE IT HAS TO BE A INVERSE}
	
	We say that the basis vectors $\mathbf v_i$ \textbf{co-vary}\index{covariant} and the coordinates $a_i$ \textbf{contra-vary}\index{contravariant} with the change of basis. The idea, although it sounds simple, is rather hard to get the feel of. It's worth thinking a good bit about how coordinates and bases need to vary in opposite ways so that the physical object represented by the coordinates stays the same regardless of how we look at it. 
	
	\textbf{This will be a caption for a sketch of a 3-D rotation}
	
	 When you rotate your character in a video game (or in real life too...), the world rotates \emph{contrary} to the direction you've rotated in. The coordinates of what you've seen have \emph{contra-varied} while your basis vectors have \emph{co-varied}. The end result is that despite changing your coordinate system, physics stays the same: invariant. This extends beyond just rotations to \emph{all} linear transformations point. 
	
	As one further example, consider the following transformation:
	\begin{align*}
		\mathbf v_1' &= 2 \mathbf v_1 + \mathbf v_2 \\
		\mathbf v_2' &=  \mathbf v_1 + 2 \mathbf v_2.
	\end{align*}
	It's easy to compute the inverse of this matrix and see how the new coordinates should work, but it worthwhile looking at how this looks geometrically. It is not a rotation, but more of a ``stretching''. Notice that while in our original perspective, $\mathbf v_1$ and $\mathbf v_2$ were orthogonal vectors, in the \emph{new} perspective, they are \emph{no longer} orthogonal. This is very important: linear transformations in general do not preserve orthogonality.
	
	\textbf{Include graphic here}
	
	 If we were to take the ``dot product" $\mathbf v_1' \cdot \mathbf v_2'$ just by multiplying corresponding coordinates and summing them up, then in the new basis we'd get zero, but in the original basis we would \emph{not}. This dot product actually changes depending on the coordinate system that we use! In some sense, this is expected: all we're doing is multiplying contravariant coordinates together and summing them up. The result should be contravariant as well (in fact doubly contravariant).
	
	But then does that mean that the state of being orthogonal is coordinate-dependent? Two vectors can be orthogonal in one coordinate system and not in another? Immediately, you should say ``No, the state of being perpendicular is a real-world observation that doesn't depend on what coordinate system you use.'' So what is going on? The only possibility is that we're not doing the dot product right. We can't just multiply the contravariant coordinates: we need to make use of both the coordinates and the vectors. 
	
	In the succeeding chapter, we will study how to define these \emph{inner products}\index{Inner Product} on vectors. To obtain the invariant scalar from the contravariant coordinates, we will need to introduce a doubly-covariant quantity called the \emph{metric tensor}\index{Metric Tensor}.
	
	 As soon as we define a product operation between vectors $\mathbf v \cdot \mathbf w$, we have added structure to our vector space. An inner product makes it so that not all bases of vectors are equal. There are bases where the basis vectors satisfy $v_i \cdot v_j = 0$ unless $i = j$, meaning the basis vectors are all orthogonal. The orthogonal bases are special among the set of all bases. Not all linear transformations, therefore, will preserve orthogonal bases: only orthogonal ones will. 
	
	
	
	% Let's be more careful: we want to calculate the inner product:
%
% 	\begin{equation*}
% 		\mathbf v_1' \cdot \mathbf v_2' = (2 \mathbf v_1 + \mathbf v_2) \cdot (\mathbf v_1 + 2 \mathbf v_2) = 2 \mathbf v_1 \cdot \mathbf v_2 + \mathbf v_1 = 2 \mathbf v_1 \cdot \mathbf v_1 + 1 \mathbf v_1 \cdot \mathbf v_2 + 4 \mathbf v_1 \cdot \mathbf v_2 + 2 \mathbf v_2 \cdot \mathbf v_2
% 	\end{equation*}
	
	% section Linear Algebra & Coordinates (end)
	
	\section{Curvilinear Coordinates: Polar \& Beyond} % (fold)
	\label{sec:curvilinear_coordinates_polar_beyond}
	
	Perhaps you may be wondering why we've spent so much time on changing between coordinate systems represented by basis vectors centered at a fixed origin. You've seen change between cartesian and polar coordinates, and that has little to do with the linear change of basis that we've just discussed, right? 
	
	We could use something like a polar system of $(r,\theta)$ or a spherical system $(r, \phi, \theta)$. These are now not representable in terms of three axes, but instead look like this:
	
	\textbf{Graphic of polar coordinate system/spherical} 
	
	This is an example of a non-linear coordinate transformation. They are more commonly referred to as \textbf{curvilinear}. Whereas linear ones map lines to lines, curvilinear ones more generally map lines to curves. The idea for making sure that the equations of physics still stay true for non-linear coordinate transformations is to note that just like a curve locally looks like a line, a \emph{non-linear} transformation locally looks like a \emph{linear} one. The linear transformation that it locally looks like is called the \textbf{Jacobian} \index{Jacobian} $J$. If the laws of physics are invariant under linear transformations locally at each point, then \emph{globally}, they will be invariant under non-linear ones as well. That is why we cared about studying covariance and contravariance for linear transformations: more complicated cases can be reduced to their local linear behavior.
	
	As an example, consider going from a cartesian to a polar coordinate system. We have $x = r \cos \theta$ and $y = r \sin \theta$. Certainly, this is not a linear transformation of coordinates. There is sinusoidal dependence on $\theta$ in this transformation. Physics and geometry, however, do not have laws in terms of absolute coordinates (it doesn't make sense to say ``That object is located at 50 meters'') but only in terms of relative distances (you'd instead say ``That object is located 50 meters \emph{relative to} me"). It is the changes over relative distances between points that we care about, and these are obtained by integrating the \emph{infinitesimal} changes at each point. 
	
	So although $x,y$ do not depend linearly on theta, through the use of the chain rule, we have:
	\begin{align*}
		dx = \cos \theta dr - r \sin \theta d\theta\\
		dy = \sin \theta dr + r \cos \theta d\theta
	\end{align*}
	This is a linear relationship now:
	\begin{equation*}
		\begin{pmatrix}
			dx \\ dy
		\end{pmatrix}
		 = 
		 \begin{pmatrix}
		 	\cos \theta & - r \sin \theta \\
			\sin \theta & r \cos \theta
		 \end{pmatrix}
		 \begin{pmatrix}
		 	dr \\
			d\theta
		 \end{pmatrix}
	\end{equation*}
	So every nonlinear transformation from some coordinate system $x_1 \dots x_n$ to $x' \dots x_n'$ has the local linear transformation law:
	\begin{align*}
		\begin{pmatrix}
			dx_1 \\ \vdots \\ dx_n 
		\end{pmatrix}_{\text{old}}
		=
	 \begin{pmatrix}
	 	\frac{\partial x_1}{\partial x'_1} & \dots & \frac{\partial x_1}{\partial x'_n} \\
		\vdots & \ddots & \vdots \\
		\frac{\partial x_n}{\partial x'_1} & \dots & \frac{\partial x_n}{\partial x'_n}
	 \end{pmatrix}_{\text{old} \leftarrow \text{new}}
	 \begin{pmatrix}
	 	dx'_1 \\ \dots \\ dx'_n
	 \end{pmatrix}_{\text{new}}\\
	 \Rightarrow 
	 \begin{pmatrix}
	 	dx'_1 \\ \dots \\ dx'_n
	 \end{pmatrix}_{\text{new}}
		=
	 \begin{pmatrix}
	 	\frac{\partial x_1}{\partial x'_1} & \dots & \frac{\partial x_1}{\partial x'_n} \\
		\vdots & \ddots & \vdots \\
		\frac{\partial x_n}{\partial x'_1} & \dots & \frac{\partial x_n}{\partial x'_n}
	 \end{pmatrix}^{-1}_{\text{new} \leftarrow \text{old}}
	\begin{pmatrix}
		dx_1 \\ \vdots \\ dx_n 
	\end{pmatrix}_{\text{old}}
	\end{align*}
	This is exactly the analogue of Equation~\eqref{eq:ContravariantTransform2}, so indeed the changes in coordinates $dx_i$ can be called \emph{contravariant}, just like the coordinates themselves were for the linear case. This is all an extension of the principle of local linearity learned in calculus.
	
	Similarly, we can express the new derivative operators in terms of the old ones by using the chain rule. For polar coordinates we have
	
	\begin{align*}
		\frac{\partial f}{\partial r} = \frac{\partial x}{\partial r}\frac{\partial f}{\partial x}  +  \frac{\partial y}{\partial r} \frac{\partial f}{\partial y} \\
		\frac{\partial f}{\partial \theta} = \frac{\partial x}{\partial \theta}\frac{\partial f}{\partial x}  + \frac{\partial y}{\partial \theta}\frac{\partial f}{\partial y}
	\end{align*}
	or more compactly we can relate just the differential operators themselves: 
	\begin{align*}
		\begin{pmatrix}
			\frac{\partial}{\partial r} \\ \frac{\partial}{\partial \theta}
		\end{pmatrix}
		 = 
		 \begin{pmatrix}
		 	\frac{\partial x}{\partial r} & \frac{\partial y}{\partial r} \\
			\frac{\partial x}{\partial \theta} & \frac{\partial y}{\partial \theta}
		 \end{pmatrix}
		 \begin{pmatrix}
		 	\frac{\partial}{\partial x} \\ \frac{\partial}{\partial y} \\
		 \end{pmatrix}
	\end{align*}
	so that more generally: 
	\begin{equation*}
		\begin{pmatrix}
			\frac{\partial}{\partial x'_1} \\ \vdots \\ \frac{\partial}{\partial x'_n} 
		\end{pmatrix}_{\text{new}}
		=
	 \begin{pmatrix}
	 	\frac{\partial x_1}{\partial x'_1} & \dots & \frac{\partial x_1}{\partial x'_n} \\
		\vdots & \ddots & \vdots \\
		\frac{\partial x_n}{\partial x'_1} & \dots & \frac{\partial x_n}{\partial x'_n}
	 \end{pmatrix}_{\text{new} \leftarrow \text{old}}^T
	 \begin{pmatrix}
	 	\frac{\partial}{\partial x_1} \\	\vdots \\ \frac{\partial}{\partial x_n}
	 \end{pmatrix}_{\text{old}}
	\end{equation*}
	If you look carefully, you will see that this accordingly mirrors the covariant change of vectors in Equation~\eqref{eq:CovariantTransform2}. So while the infinitesimal changes in the coordinates themselves are contravariant, just like the linear coordinates themseles, the \emph{differential operators} corresponding to changes in these coordinates become \emph{covariant}, just like the vectors $\mathbf v_i$ in the linear case. This is the first correspondence that will hint that our basis vectors $\mathbf v_i$ actually \emph{correspond} to the differential operators $\frac{\partial}{\partial x_i}$.
	
	Indeed, as we begin to move away from 3-D and $n$-D Euclidean space, we will see why the old notions of unit vectors $\mathbf{\hat i}, \mathbf{\hat j}, \mathbf{\hat k}$ are better viewed as the operators $\frac{\partial}{\partial x}, \frac{\partial}{\partial y},$ and $ \frac{\partial}{\partial z}$, and in general $\mathbf v_i \rightarrow \frac{\partial}{\partial x_i}$. This change of notation will allow us to easily pass onto far more general spaces than the Euclidean ones we've gotten used to. 
	
	% section curvilinear_coordinates_polar_beyond (end)
	
\end{document}