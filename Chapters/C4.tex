\chapter[Harmonics: Fourier Analysis]{Harmonics:\\ Fourier Analysis}

	This chapter develops the ideas of Fourier analysis, an idea whose reach now extends across mathematics and physics, and moreover has great influence in the engineering disciplines. There is no argument against the fact that it is one of the most powerful tools that mathematics has ever developed, not just for real-world application, and not just for formulating physical theories, but going further and influencing the way mathematics itself is done. 
	
	Another mathematical idea that has had similar impact across the scientific fields is linear algebra, the in-depth reach of studying the phenomenon of linearity. Aside from allowing us to go between coordinate representations of physical space and datasets in science (which, as philosophically important as it may be, is not so central in application), linear algebra gives another powerful tool that, as of yet, has not been touched upon: that of the eigenvector. Eigenvectors give us a \emph{natural} way of looking at an object that is represented by a matrix or linear operator, whether that object is the stress tensor of a material (principal axes), a high dimensional dataset (principal-component analysis), or the long-term behavior of a system (eigenvectors for markov chains and stochastic processes). 
	
	In this section, we will not just reflect on the uses and power of linear algebra and Fourier analysis in engineering, physics, and mathematics, but we will go further and show that they are one and the same: Fourier analysis, viewed through the right lense, is plain linear algebra. This will set a solid stage for which the next chapter will give us the real spectacle: generalizing the ideas of Fourier analysis and linear algebra to get something much more unified and powerful, which it can be said constitutes the backbone of modern mathematics: Representation Theory. 



\section{Vector Spaces of Functions} % (fold)
\label{sec:vector_spaces_of_functions}



	We will work with functions just over $\mathbb R$ at first, the setting that we are most used to. It is no great surprise that functions $f:\mathbb{R} \rightarrow \mathbb R$ form a vector space, since we can add and scale any two functions $a f + b g$ and get another function as their linear combination. We can therefore try to apply ideas from linear algebra to this vector space. Note first that it is infinite dimensional. 
	
	There is an important subspace worth looking at: $C^{\infty} (\mathbb R)$ of smooth functions. Since finite linear combinations of smooth functions are still smooth, $C^{\infty} (\mathbb R)$ is a subspace. We only allow for finite linear combinations of functions for now, as otherwise we need to figure out what we mean by infinite sequences and series of functions to ``converge'' to a limit in this vector space. 
	
	\begin{example}
		The set of polynomials up to degree $n$ on $\mathbb{R}$, $P^n(\mathbb R)$, is a vector space. The set of polynomials up to degree $n$ restricted to an interval $[a,b]$, $P^n[a,b]$, is also a vector space. 
	\end{example}
	Note we need to include all the polynomials of degree $\leq n$ as well, not just degree $n$. Otherwise something like $(x^{5} + x^{3}) - x^{5}$ would not be in the space. 
	
	In particular, now, let's consider the interval $[0,1]$ to be of interest. We care about the space of \emph{all functions} on $[0,1]$. Again, this is still infinite dimensional, but say we restricted \emph{further} and only cared about the function at the points $0, 1/4, 1/2, 3/4$ and $1$. These are $5$ points of interest. A function restricted to these $5$ points can be represented just as a vector of $5$ numbers: $[f(0) ~ f(1/4) ~ f(1/2) ~ f(3/4) ~ f(1)]$.
	
	The linear combinations of functions on $[0,1]$ when restricted to these $5$ points are then just linear combinations of these vectors in $\mathbb{R}^5$. In numerics, this would be a \emph{low resolution} way to view the function, equivalent to something like the graphic.

	\todofig{5 box functions interpolating $f$}
	
	If we increased the number of points we sampled at, say $f(k/n)$ as $k$ runs from $0$ to $n$, then we represent functions by vectors in $\mathbb{R}^{n+1}$. As $n$ gets big our resolution gets better and better. In general, though, infinitely many different functions will get sent to the same representation as a vector in $\mathbb{R}^{n+1}$ (so this discrete approximation from the set of all functions on $[0,1]$ to just $n+1$ evenly-spaced points has a huge kernel).
	
	This type of idea is what lies behind Riemann summation for defining the integral operator. In fact, the discrete approximation of the integral operator on functions, when viewed as vectors in $\mathbb{R}^{n+1}$
	\begin{example}[Riemann Sums]
	\end{example}
	\begin{equation*}
		\int_{approx} f dx = \sum_{k=0}^n f\left(\frac{k}{n}\right) \frac{1}{n} = 
		\begin{pmatrix}
			\frac{1}{n} & \dots & \frac{1}{n}
		\end{pmatrix}
		\begin{pmatrix}
			f(0) \\
			\vdots \\
			f(1)
		\end{pmatrix}
	\end{equation*}
	The Riemann sum in this discrete space is just the linear functional $\begin{pmatrix}
			\frac{1}{n} & \dots & \frac{1}{n}
		\end{pmatrix}$ acting on the vector representing the function to give the desired value. If the full function $f$ is Riemann-integrable, then as $n\rightarrow \infty$  this will approach the true value $\int_0^1 f dx$. 
		
	This approximation in $\mathbb R^{n+1}$ is not just any vector space. It has an notion of \emph{length} for a vector
	\begin{equation}
		||f_n||^2 = \frac{1}{n} \sum_{i=0}^{n+1} f(k/n)^2 
	\end{equation}
	As the sum of the squares of the function values at every point. The factor of $1/n$ out front is so that in the limit of $n\rightarrow \infty$ this gives a \textbf{norm}\index{Norm}, a notion of size, on the space of functions whose squares are Riemann-integrable (such functions are called \textbf{square-integrable}\index{Square-Integrable}) on $[0,1]$: 
	\begin{equation}
		||f||^2 = \int_{0}^1 f(x)^2 dx
	\end{equation}
	On $[0,1]$, the norm of a function is its root mean square value. In fact this notion of length also gives a natural notion of inner product (both on the discrete space and the full space):
	\begin{equation}
		\left< f | g \right> = \int_{0}^1 f(x) g(x) dx
	\end{equation}
	Looking at it this way, it is \emph{exactly} the same as how we defined a dot product: multiply the corresponding components together and sum that up. The difference is that now ``components'' have become ``functional values'' and the sum has become an integral.
	
	Naturally, we can define an inner product on any interval $[a,b]$ between square integrable functions on that interval:
	\begin{equation}
		\left< f | g\right> = \int_a^b f(x)g(x) dx
	\end{equation}
	 It is similarly possible to define an inner product for square-integrable functions on the \emph{whole} real line:
	\begin{equation}
		\left< f | g \right> = \lim_{L \rightarrow \infty} \int_{-L}^L f(x) g(x) dx = \int_{\mathbb R} f(x) g(x) dx
	\end{equation}
	
	As in Chapter~\ref{ch:diffgeo}, an inner product on a real vector space is as follows. 
	\begin{defn}[Real Inner Product]
		For a real vector space $V$, an inner product $\left<-|-\right>$ on $V$ is a bilinear map into $\mathbb R$ so that:
		\begin{enumerate}
			\item (Linear) $\left<a \mathbf x + b \mathbf y | \mathbf z\right> = a \left< \mathbf x | \mathbf z \right> + b \left< \mathbf y | \mathbf z \right>$
			\item (Symmetric) $\left<\mathbf x | \mathbf y\right> = \left< \mathbf y | \mathbf x\right>$
			\item (Positive-definite) $\left< \mathbf x | \mathbf x \right> \geq 0$ and equality holds iff $\mathbf x = 0$. A vector space that possesses an inner product is called an \emph{\textbf{inner product space}}.
		\end{enumerate}
	\end{defn}
	
	Inner product spaces are specific examples of \textbf{Metric Spaces}\index{Metric Space}. 
	\begin{defn}[Metric Space]
		A set $X$ is a metric space if there is a non-negative function $d:X\times X \rightarrow \mathbb R$ that satisfies
		\begin{enumerate}
			\item (Symmetry) $d(x,y)=d(y,x)$
			\item (Positive Definite) $d(x,y)$=0 iff $x=y$
			\item (Triangle Inequality) $d(x,z) \leq d(x,y) + d(y,z)$
		\end{enumerate}
	\end{defn}
	Just as on the real line and on $\mathbb{R}^n$, having a notion of distance allowed us to define closed and open sets, the same is true on metric spaces. In particular, inner product spaces have a natural topology defined by open and closed balls defining neighborhoods, and so as vector spaces that are also topological spaces, they are called \textbf{Topological Vector Spaces}.\\
	
	Returning back to the inner product space of functions on $\mathbb R$ this inner product is far from useless. If both $|f|=1$ and $|g|=1$ have length then the inner product, in some well-defined sense, measures how \emph{correlated} $f$ is with $g$. If the integral of $f$ against $g$ is zero then there's no way to predict the behavior of $g$ based on the behavior of $f$. If it is positive, then a positive $f$ means a positive $g$ \emph{more often than not}, while if the integral is negative then a positive $f$ means a \emph{negative} $g$ more often than not. Such a quantity in general: 
	\begin{equation*}
		\frac{\left< f | g \right>}{||f|| ~ ||g||}
	\end{equation*}
	is geometrically what the cosine of the angle between two vectors was in $\mathbb R^{n+1}$. Functions that are ``in the same direction'' would have a positive cosine, while functions ``pointing in opposite directions'' would have a negative one. Uncorrelated, orthogonal functions would have a cosine of zero.
	\todoex{Exercise on correlation of functions and the inner product}
	
	So the vector space of square-integrable functions on $[0,1]$ is an inner product space\index{Inner Product Space}. It is denoted\footnote{\textbf{HERE SAY SOMETHING ABOUT LEBESGUE VS RIEMANN INTEGRATION}\todoadd{Riemann vs Lebesgue in a footnote}} by $L^2[0,1]$\index{Hilbert Space!$L^2$}. Similarly, for a general interval $[a,b]$ we denote the square integrable functions as $L^2 [a,b]$, and for functions square-integrable on the \emph{entire real line}, we write $L^2 (\mathbb R)$
	
	The space $L^2[0,1]$ is more than just an inner product space. It is a \textbf{Hilbert space}\index{Hilbert Space}, an object that is of central importance in quantum mechanics, signal processing, and mathematical analysis. A Hilbert space is a vector space that allows us to take limits of vectors, too, not just finite sums, so that we can do analysis on it.
	
	If we have a sequence of vectors $x_n$ on our Hilbert space that \emph{accumulates}, in the sense that for any $\varepsilon$ we can pick an $N$ large enough so that all the $x_i$ after $x_N$ are within $\varepsilon$ of $x_N$
	\begin{equation}
		|x_N - x_i| < \varepsilon
	\end{equation}
	\todoex{Ex: Show this is the same as forming a Cauchy sequence}
	\todofig{Figure: All vectors being within a neighborhood of $x_N$, accumulating.}
	Then, if we were working in $\mathbb R^n$, this would mean that the sequence should converge within some closed ball $B_{\varepsilon}(x_N)$. In fact, convergent sequences are \emph{exactly} the ones that accumulate like this. 
	
	We want a Hilbert space $H$, infinite-dimensional or otherwise, to have this property of being \textbf{complete} in the inner product. That is, whenever a sequence of vectors accumulate, that sequence will in fact \textbf{converge}\index{Convergence} to a \textbf{limit}\index{Limit} vector \emph{still in $H$}.
	
	\begin{defn}[Real Hilbert Space]
		A real Hilbert space $H$ is a real inner product space that is \textbf{complete}\index{Complete}\index{Metric!Complete} with respect to its inner product. 
	\end{defn}
	\begin{example}
		The rationals $\mathbb Q$ are not a complete metric space. The reals are (they are called the \textbf{completion} of the rationals). 
	\end{example}
	\begin{example}
		The space of continuous functions is not complete. Consider the sequence of functions:
		\begin{equation}
			f_n(x) = \begin{cases}
    -1 ,& \mathrm{if} ~ x \leq  -1/n \\
    n x,  &\mathrm{if} ~ -1/n < x < 1/n \\
	1, & \mathrm{otherwise}
\end{cases}
		\end{equation}
		While each $f_n$ is continuous, the limit is the sign function of $x$, which is \emph{not} continuous. Similar arguments show that the space of smooth functions is also not complete, by considering $f_n (x) = e^{-n \pi x^2}$ approaching the zero function with a jump discontinuity at zero. 
	\end{example}
	
	\todoex{Ex: Being complete is with respect to a metric, not a topology (reals are homeomorphic to incomplete $[0,1]$)}
	
	\begin{theorem}
		$L^2[a,b]$ is complete, as is $L^2(\mathbb R)$.
	\end{theorem}
	\begin{proof}
		Any introductory text in real analysis and measure theory proves this result after developing the necessary background in proper Lebesgue integration, c.f. \todoadd{Provide a measure theory reference for $L^2$ being complete, like Stein, but probably not Stein tb$\hbar$.}
	\end{proof}
	
% section vector_spaces_of_functions (end)

	This is an extraordinary property, which places $L^2$ spaces above all others in their importance to classical analysis. This property will allow us to talk about sequences on these spaces, and hence do analysis on these spaces by talking about sequences of functions converging to a limit point ``in the $L^2$ norm''. 
	
	The two properties of having an inner product as well as having this notion of convergence in a norm are beautifully useful in formulating the original ideas of Fourier. 

\section{Trigonometric Series} % (fold)
\label{sec:trigonometric_series}

	Let us investigate the phenomenon of a wave in a 1-dimensional medium, like a string vibrating in one dimension. At a point $x_0$, the string has some displacement $u(x_0)$. Its neighbors have displacements $u(x_0 + h)$ and $u(x_0 - h)$. 
	\todofig{Draw A String for the Wave equation}
	Because of \textbf{Hooke's Law}\index{Hooke's Law} $\mathbf F = k \mathbf x$, the rightward neighbor will exert a force $\mathbf F$ on the string at $x_0$ proportional to the difference in displacement $u(x_0 + h) - u(x_0)$. Similarly, the leftward neighbor will exert a force proportional to $u(x_0 - h) - u(x_0)$. The total force at $x_0$ then is some scalar multiple times the sum of both these contributions:
	\begin{equation}
		\mathbf F_{x_0} = k [u(x_0 + h) - u(x_0) + u(x_0 - h) - u(x_0)].
	\end{equation}
	The mass constant $k$ between successive neighbors can be related to the total spring constant $K$ by $k = K L / h$. This is a simple exercise. \todoex{Make an exercise for the total spring constant}. 
	
	We also know from \textbf{Newton's Second Law}\index{Newton's Second Law} that the force is equal to the mass times the acceleration of the displacement of the string at $x_0$
	\begin{equation}
		\mathbf F_{x_0} = m \frac{\partial^2 u}{\partial t^2}.
	\end{equation}
	The mass of the particle at $x_0$, because it is only a small section of the string, becomes infinitesimal as $h \rightarrow 0$ and can be expressed as $\rho h$ where $\rho$ is the mass density of the string. Together, then:
	\begin{equation}
		\rho h \frac{\partial^2 u}{\partial t^2} = \frac{KL}{h} [u(x_0 + h) + u(x_0 - h) - 2 u(x_0)]
	\end{equation}
	This gives rise to the equation:
	\begin{equation}
		\frac{\partial^2 u}{\partial t^2} = \frac{KL}{\rho} ~ \frac{u(x_0 + h) + u(x_0 - h) - 2 u(x_0)}{h^2}
	\end{equation}
	The constant $KL/\rho$ on the right hand side has units of length squared per second squared, and can be interpreted as a velocity squared $c^2$. The more interesting quantity on the right hand side is what succeeds that. Note that it can be re-written as:
	\begin{align*}
		\frac{1}{h} \left[\frac{u(x_0 + h) - u(x_0)}{h}  - \frac{u(x_0) - u(x_0 - h)}{h}\right]
	\end{align*}
	As $h \rightarrow 0$ the inside approaches a difference of the derivatives $\frac{\partial u}{\partial x}$ at neighboring points $x_0 + h/2, x_0 - h/2$, so that the whole term becomes the \emph{second derivative} $\partial^2 u/\partial x^2$. The equation for the wave on the string is then:
	\begin{equation}
		\frac{1}{c^2} \frac{\partial^2 u}{\partial t^2} = \frac{\partial^2 u}{\partial x^2}
	\end{equation}
	This is the \textbf{wave equation}\index{Wave Equation} of central importance across the subfields of physics. 
	
	The wave equation in one dimension is important for studying simple vibrations of strings, metal beams, and waves of current in trasmission lines and cavity resonators. However, the higher dimensional generalizations of the wave equation find even more use, especially in studying the propagation of wave energy through physical space. It is worth deriving this as well. The waves propagate through a medium with speed $c$ depending on the properties of the medium. 
	
	One way is just to note that now we have $u(x,y)$, and as before we will have that $u(x\pm h, y \pm h)$ will contribute a term that now looks like
	\begin{equation*}
		\frac{u(x+h,y) + u(x-h,y) + u(x,y+h) + u(x,y-h) - 4 u(x,y)}{h^2}
	\end{equation*}
	\begin{align*}
		 =  &\frac{u(x+h,y) + u(x-h,y)- 2u(x,y)}{h^2}\\ 
		 &+ \frac{u(x,y+h) + u(x,y-h) - 2 u(x,y)}{h^2} 
	\end{align*}
	\begin{equation*}
		\rightarrow \frac{\partial^2 u}{\partial x^2} + \frac{\partial^2 u}{\partial y^2} ~~~~
	\end{equation*}
	so that the wave equation in $2$ dimensions is
	\begin{equation}
		\frac{1}{c^2} \frac{\partial^2 u}{\partial t^2} = \frac{\partial^2 u}{\partial x^2} + \frac{\partial^2 u}{\partial y^2}
	\end{equation}
	This quantity, $\frac{\partial^2 u}{\partial x^2}$ in one dimension and $\frac{\partial^2 u}{\partial x^2} + \frac{\partial^2 u}{\partial y^2}$ in two, and more generally
	\begin{equation*}
		\sum_{k=1}^n \frac{\partial^2 u}{\partial x_i^2}
	\end{equation*}
	in $n$ dimensions, is called the \textbf{Laplacian}\index{Laplacian} of $u$. It is the divergence of the gradient, and so is often denoted $\nabla^2 u$, or otherwise just $\triangle u$. 
	
	Now for the 2-dimensional argument, you could ask why we did not take into account diagonal neighbors like $u(x+h, y+h)$. One argument could be that they are not ``directly connected'' to $(x,y)$, but a better answer is that we \emph{can} account for the diagonal neighbors (and associated $\sqrt 2$s pop up because their distance away is not exactly $h$), and in the limit as $h \rightarrow 0$, the finite differences obtained would still converge to the same Laplacian. 
	\begin{concept}[Laplacian, Intuitively]
	There are many different finite-difference approximations for the Laplacian, all proportional to the idea:
		\begin{equation*}
			\triangle  u ~\propto~ \frac{ \text{sum over } n \text{ neighbors of } u(\text{neighbors}) - n * u(x,y) }{\text{Area}}
		\end{equation*}
		In this way, if the average value of $u$ on a small circle around $x$ is greater than the value of $u$ at $x$, the Laplacian is positive, and vice versa gives that the Laplacian is negative. 
	\end{concept}
	
	It is natural for the average behavior of the neighbors at point to ``drive'' the behavior of the point in the next instant. For waves, the mean value of the neighbors of that point drives that point to \emph{accelerate} in that direction. 
	
	Similarly, for heat flow, a similar derivation using Newton's law of \emph{cooling}, instead of motion, gives that this Laplacian: the imbalance between the point and its environment, drives the point to simply \emph{move} to stabilize that imbalance:
	\todoex{Make a heat equation derivation exercise, from Newton's law of COOLING this time}
	\begin{concept}[Wave and Heat Equations]
		The motion of waves is governed by the equation
		\begin{equation*}
			\frac{1}{c^2} \frac{\partial^2 u}{\partial t^2} = \triangle u
		\end{equation*}
		for some constant speed $c$ depending on the properties of the medium.
		
		Further, the flow of heat is governed by the equation
		\begin{equation*}
			\tau \frac{\partial u}{\partial t} = \triangle u
		\end{equation*}
		for some constant $\tau$ depending on the properties of the medium. 
	\end{concept}
	
	Fourier arrived at the idea of trigonometric series while studying heat flow. We will look at the example of the vibrating string, pinned down at its edges. 
	
	We now begin with a string defined on the interval $[0,1]$. $u$ satisfies the wave equation, as well as the \textbf{boundary conditions}\index{Boundary Conditions} $u(0)= u(1)= 0$. If we have an initial shape of the string $u_0(x)$ as well as an initial velocity $\partial u/\partial t$ at $t=0$, given by $v_0(x)$, then we'd like to see what dynamics emerge. 
	
	

% section trigonometric_series (end)

\section{Spectral Theory on Graphs} % (fold)
\label{sec:spectral_theory_on_graphs}
	\index{Graph|(}

	We begin with the definition of a \textbf{finite graph}
	\begin{defn}
		A finite graph $G$ is a finite set of vertices $V = \{v_1, \dots, v_n\}$ together with a set of edges $E = \{e_1, \dots, e_m\}$ such that each edge is an unordered pair of vertices $(v_i, v_j)$ indicating that they are connected.
	\end{defn}
	\todofig{Draw a figure of a graph}
	
	All of this information can be captured in terms of an $n$ by $n$ matrix called the \textbf{adjacency matrix}\index{Adjacency Matrix}\index{Graph!Adjacency Matrix}. 
	\begin{defn}[Adjacency Matrix]
		The adjacency matrix $A$ for a graph $G$ is a matrix whose $i,j$th entry is $1$ if $(v_i,v_j)$ is an edge, and $0$ otherwise.
	\end{defn}
	The adjacency matrix acting on a vector with $0$s everywhere except for the $i$th slot will give a vector with $0$s everywhere except for the slots of the \textbf{neighbors}\index{Graph!Neighbors} of $v_i$. That is, all vertices $v_j$ such that $(v_i, v_j)$ is an edge. We denote the set of neighbor vertices to $v_i$ by $N(v_i)$.
	
	Note that this matrix is symmetric, and consists of only zeroes and ones. Similarly we define the \textbf{degree matrix}\index{Degree Matrix}\index{Graph!Degree Matrix} as
	\begin{defn}[Degree Matrix]
		The degree matrix $D$ for a graph is a diagonal matrix whose $(i,i)$th entry is the \emph{number of neighbors} of $v_i$, that is $|N(v_i)|$
	\end{defn}
	
	Graphs appear throughout modern mathematics and computer science, to the point that we expect that the reader is familiar with them at least at a rudimentary level. The reason we put a focus on graphs in a section that is about Fourier analysis is the following:
	\begin{concept}[Discrete Approximation]
		In almost all practical matters of engineering and applied math, manifolds are approximated by graphs. 
	\end{concept}
	The reason for why we care about this is that it makes everything finite-dimensional, turning all analysis into algebra and allowing us to not worry about notions of convergence and Hilbert spaces. Because of the immense use of such finite methods in practical engineering, physics, and computer science, we have this topic as a section of its own, not just a mere exercise of example. We will illustrate its applicability to science through a series of examples.
	\begin{example}[Signal Processing]
		A signal $f(t)$ on a time interval $t \in [0,1]$ has the interval approximated as a graph $G_n$ of vertices $v_0, \dots, v_n$. $v_k$ represents the point $k/n \in [0,1]$. 
		
		$v_0$ is connected only to $v_1$, $v_n$ is connected only to $v_{n-1}$ and otherwise $v_{i}$ is connected to its immediate neighbors $v_{i\pm1}$. The signal is then a function on the set of $n+1$ vertices. 
	\end{example}
	Consider the derivation of the wave and heat equations in the preceding chapter. 
	\begin{example}[Image Processing]
		A $2D$ image represented by $f(x,y)$ on the interval $[0,1] \times [0,1] = [0,1]^2$ is approximated as a graph $G_n$ of vertices $v_{i,j}$ for $0\leq i, j \leq n$. The vertex $v_{i,j}$ represents the point $(i/n, j/n) \in [0,1]^2$. 
		
		As before, each vertex $v_ij$ on the interior is connected to its four nearest neighbors $v_{i\pm 1, j\pm1}$, vertices on the corners are only connected to two such neighbors, and otherwise vertices on the edges are connected to the three such neighbors. Images are then functions on this graph of $(n+1)$ vertices. 
	\end{example}
	
	In these examples, we can form a \textbf{discrete derivative} operator\index{Derivative!Discrete}. 
	\begin{example}[Discrete Derivative]
		In the first example, we can form a discrete derivative of a function on these $(n+1)$ vertices as:
		\begin{equation}
			\Delta f(v_i) = \begin{cases}
				\frac{f(v_{i+1}) - f(v_{i})}{(1/n)}, & \text{if } \textit{$i = 0$}\\
				\frac{f(v_{i}) - f(v_{i-1})}{(1/n)}, & \text{if } \textit{$i = n$}\\
				\frac{f(v_{i+1}) - f(v_{i-1})}{2 (1/n)}, & \text{otherwise}
			\end{cases}
		\end{equation}
		This derivative is called a \textbf{central difference}\index{Derivative!Central Difference}. Of course we could have also picked just a \textbf{forward difference}\index{Derivative!Forward/Backward Difference}
		\begin{equation}
			\Delta f(v_i) = \begin{cases}
				\frac{f(v_{i+1}) - f(v_{i})}{(1/n)}, & \text{if } \textit{$i < n$}\\
				\frac{f(v_{i}) - f(v_{i-1})}{(1/n)}, & \text{if } \textit{$i = n$}\\
			\end{cases}
		\end{equation}
		or the other way, as a \textbf{backward difference}. All of these operators are linear and representable as $(n+1)$ by $(n+1)$ matrices acting on the set of functions on the graph, equivalent to\footnote{Note we work over complex vector spaces for the same reason as in the previous section: we can find all the eigenvalues of an operator when working over $\mathbb C$.} $\mathbb C^{n+1}$. Their kernel, just like the regular derivative operator, is one-dimensional: the space of constant functions. 
		
		Similarly, in the second example we can form discrete derivatives corresponding to the ``$x$'' and ``$y$'' directions in the same way as before, operating on either the first or the second index of the $v_{i,j}$, respectively. 
	\end{example}
	
	Much like picking a derivative $\partial/\partial q^i$ is coordinate covariant, so is picking a derivative on a graph not a natural thing. We need to pick a sort of ``flow'' on the graph along which our derivative vector field is pointing. On a graph given at random, its not at all obvious if there is any natural way to define ``derivative''. This is further evidence that the coordinate-dependent notion of ``derivative'' is not of central importance in physics. 
	\todofig{Random scrambled graph ``where's your derivative now, fam?"}
	
	On the other hand, there \emph{is} an operator independent of coordinate system that we \emph{can} form: the \textbf{graph Laplacian}\index{Laplacian!On Graphs}. In the previous section, for $1$-dimension we defined the Laplacian to be the sum of the function values at the two neighboring points $x_{i\pm 1}$ minus $2$ times the function value at the point $x_0$ itself, all divided by $h$, the infinitesimal length associated to the point $x_0$. For a $2$-dimensional plane, we defined the Laplacian to be the sum of the function values at the $4$ neighboring points minus $4$ times the value of the function at $(x_0, y_0)$, all divided by $h^2$, the infinitesimal \emph{area} associated to the point $(x_0, y_0)$.
	
	We now generalize this to a graph. A given point may have $k$ neighbors. We take the sum of the function $f$ at all $k$ of these neighbors and subtract $k$ times the function value at $f$. As a matrix, this can be seen as:
	\begin{align}
		\sum_{v_i \in N(v_0)} [f(v_{i}) -  f(v_0)] &= \sum_{v_i \in N(v_0)} f(v_{i}) - D(v_0) f(v_0)\\ &= (A-D) f
	\end{align} 
	This last equality recasts it in terms of the linear operators of the adjacency and degree matrix acting on the vector $f$. 
	
	This thing should be proportional (up to the factor of that infinitesimal area associated to each vertex, assuming each vertex has equal area) to the Laplacian as we've defined it before. For this reason, in graph theory we don't care about an overall constant that multiplies our Laplacian, and so there are multiple conventions. Note that the operators $A$ and $D$ are \emph{invariants}, associated to the graph, and so the Laplacian is invariant as well. On any graph, no matter how scrambled-looking it is, there is a good definition for a Laplacian. 
	\begin{defn}[Laplacian Matrix of a Graph]
		For a graph $G = (V,E)$ represented by adjacency matrix $A$ giving degree matrix $D$, the Laplacian can be written as 
		\begin{equation}
			\triangle = A - D
		\end{equation}
		Because we care about the matrix itself and not the constant multiplying it, different parts of literature define it differently as:
		\begin{itemize}
			\item (reversed) $\triangle = D-A$
			\item (normalized) $\triangle = 1 - D^{-1} A$
			\item (symmetric normalized) $\triangle = 1 - D^{-1/2} A D^{-1/2}$ 
		\end{itemize}
	\end{defn}
	We will use the symmetric normalized, as it is common in mathematical literature. 
	\todoex{Applications to Molecular Orbital Theory}
	
	Regardless of what Laplacian definition we use, the matrix is symmetric. By the spectral theorem, this guarantees us that the eigenvalues are all real and that the eigenvectors span the space of functions on the graph, and are in fact \emph{orthogonal}. This is as powerful a result as we could have ever hoped for, because now, if we write the heat equation as
	\begin{equation}
		-\triangle f = \frac{\partial f}{\partial t}.
	\end{equation}
	We can expand $f$ in an eigenbasis $h_i$ of eigenvalues $\lambda_i$ determined from the matrix for $\triangle$:
	\begin{equation}
		f = \sum_{i=1}^n c_i h_i
	\end{equation}
	where $c_i$ is obtained from orthogonality of the eigenbasis, $c_i = \left< h_i | f \right>$ so that:
	\begin{equation}
		-\triangle f = \sum_{i=1}^n c_i (-\lambda_i) h_i = \sum_{i=1}^n c_i \lambda_i h_i = \frac{\partial f}{\partial t} = \sum_{i=1}^n c_i \dot h_i.
	\end{equation}
	As a vector equation, this implies that components must agree:
	\begin{equation}
		- \lambda_i h_i = \dot h_i \Rightarrow h_i(t) = h_i(0) e^{-\lambda t}
	\end{equation}
	Thus, we get the full solution to the heat equation:
	\begin{equation}
		f(t) = \sum_{i=0}^n c_i e^{-\lambda t} h_i
	\end{equation}
	In shorthand then, the diffusion of heat operator can be written as the exponential:
	\begin{equation*}
		e^{-t \triangle }
	\end{equation*}
	And for this reason we can call the Laplacian the \emph{infinitesimal generator} of diffusion\index{Generator!Of Diffusion}. 
	
	One important subset of graphs that is worth focusing on is the set of all \textbf{k-regular}\index{Graph!Regular Graph} graphs. These are graphs such that each vertex $v$ has exactly $k$ neighbors: $\forall v ~ |N(v)|=k$.
	
	\begin{obs}
		The vector $(1, \dots, 1)$ is an eigenvector of $\Delta$ with eigenvalue $0$.
	\end{obs}
	The matrix $A$ acting on $(1, 0 \dots, 0)$ gives the vector of $1$s on all neighbors of $v_1$ and $0$s elsewhere. It's easy to see then that $A$ acting on $(1, \dots, 1)$ will give a vector with $|N(v_i)|$ on its $i$th entry. But this is exactly the same as the action of $D$ on $(1, \dots, 1)$, so $A-D$ gives zero on this vector. 
	
	The equivalent on manifolds is that the constant function has vanishing Laplacian. 
	
	\index{Graph|)}

% section spectral_theory_on_graphs (end)


\section{Spectral Theory on $\mathbb R$} % (fold)
\label{sec:spectral_theory_on_mathbf_r}

% section spectral_theory_on_mathbf_r (end)

