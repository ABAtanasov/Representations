\documentclass[../master.tex]{subfiles}
 
 
\begin{document}
	
	\section{The Cartesian Coordinate System} % (fold)
	\label{sec:Cartesian}
	
	
	
	One of the most important revolutions in mathematics and physics was introduced by Descartes in the early seventeenth century. It was the idea that any point $P$ in the plane could be represented by a pair of numbers $(x,y)$. The numbers themselves represented distances along two perpendicular axes that met at a point $(0,0)$, called the origin. By introducing this concept, he had done something amazing. He had related the \emph{geometry} of the plane to the \emph{algebra} of variables and equations. Algebra could be \emph{represented} geometrically, and conversely geometric problems could be solved by going into the realm of algebra. 
	
	\textbf{Insert 2D Plane with Coordinates}
	
	This idea: the coordinate system, would then be heavily used by Newton in the development of his infinitesimal calculus, an invention that has ranked as one of the greatest of human accomplishments. Coordinates would become more than just pairs $(x,y)$, but would extend to 3D space, and arbitrarily high dimensions. They would subsequently be used to lay the foundations for modern physics and mathematics. Linear algebra, multivariable calculus, and all the connections between algebra and geometry begin with the concept of a coordinate.
	
	Coordinate systems have been used constantly by all physicists from Newton, through Maxwell and Einstein, to the physicists and engineers of today. In mathematics, Descarte's idea planted the roots for what would turn into the modern field of algebraic geometry. 
	
	When studying a geometric phenomenon in some $n$-dimensional space, say $\mathbb{R}^n$, we pick an origin and axes to form our coordinate system. For a ball falling, we could set the origin at some point on the ground, and pick one axis parallel to the ground, and one perpendicular. We can decide to measure the axes in meters, or we could decide to do it in feet (nothing stops us from making bad choices). So now the point $P$ where the ball lies is represented by $(x,y)=(0~ \mathrm m,10~ \mathrm m)$. Because this is such a natural choice of coordinate system, the coordinate $y$ is a well-known concept, called the height.
	
	\textbf{Insert Ball Falling}
	
	We can now study $h$, free of geometry, as just a function which we can do arithmetic and calculus on. If we are given an equation of motion, say $\frac{d^2y}{dt^2} = -g, \frac{d^2x}{dt^2} = 0$ with initial conditions $\frac{dy}{dt}=0, \frac{dx}{dt}=0$, then we can do kinetics on the system, and see how it evolves in the \emph{time} direction. A recurrent theme will be that dynamics of a system in $n$-dimensional space can be thought of just a special type of geometry in $n+1$ dimensional space, putting time as an added dimension. 
	
	Because this book will, in large part, be concerned with studying the ways in which geometry, algebra, and physics connect, it is worthwhile to dwell on the \emph{philosophy} behind coordinate systems.
	
	The ball will fall from 10 meters, according to the force of gravity. That is the way the world works. It doesn't matter what coordinate system we set up to do that calculation, we should get the \emph{exact same result}. Plainly: nature doens't \emph{care} what coordinate system we use. This fact, obvious as it may be, is worth thinking about: No matter what coordinate system we use, the equation of motion should give the same dynamics. The laws of physics should be \emph{independent of any coordinate system}.
	
	Newton's law $\mathbf F = m \mathbf a$ relates the force vector to the acceleration vector. The vector representing the force $\mathbf F$ that you apply on a surface is an object independent of coordinate system, so is the resulting acceleration vector. The \emph{components} of these vectors ($F_x, F_y, F_z$) and ($a_x, a_y, a_z$), however, depend on what you have chosen for the $x,y,z$ axes. These components \emph{represent} a real physical vector, but only once we pick a coordinate system. If we were to pick a different coordinate system, they would change. It would therefore be extremely wrong if a physical law ever looked like $F_x dx = dW$, because that puts emphasis on just one of the components over the others. While in some coordinate system this may be nonzero, in another it would be zero, so the work done, $dW$, is not an invariant. That's why the true formula looks like $\mathbf F \cdot  d \mathbf r = F_x dx + F_y dy + F_z dz = d W$. Although its not yet obvious that this is invariant under change of coordinates, at the very least it doesn't put one component above any of the others.
	
	
		% In antiquity, Pythagoras discovered that for a right triangle with side lengths $a,b$ and hypotenuse $c$, the following equation related the lengths:
	% 	\begin{equation*}
	% 		a^2 + b^2 = c^2
	% 	\end{equation*}
	% 	Nowadays to us, this equation is interesting to know, and not too hard to prove. To Pythogoras and his students, however, it was absolutely stunning. The reason was that at that time, mathematics was divided into two fields: geometry and arithmetic. Geometry reasoned with measurements and constructions of figures in the plane, while arithmetic dealt with studying equalities of how numbers combined.
	%
	% 	Although equality played a central role in both fields,
	
	% section Descartes (end)
	
	\section{Linear Algebra \& Coordinates} 
	\label{sec:Linear Algebra & Coordinates}% (fold)
	
	The picture that we have of a coordinate system: a series of perpendicular lines that determine numbers associated to each point in space, is not representative of all coordinate systems. For one, we do not need the requirement that the lines be perpendicular (we'll show later that for general spaces, there's not even a good notion for what perpendicular \emph{means}). Our coordinate system could instead look like this:
	
	\textbf{Graphic of non-perp lines and representing a point like that}
	
	Through the lens of linear algebra, once we pick a point to be our origin, choosing a set of coordinate axes is the same as picking a \textbf{basis}\index{Linear Algebra!Basis} for the space (a coordinate basis). We can relate the new system of coordinates $x_i'$ in terms of the old system $x_i$ by matrix multiplication: $x_i' = \sum_{j=1}^n \mathbf A_{ij} x_j$. This is exactly what's called a change of basis in linear algebra. Transformations between coordinate bases are exactly the invertible \textbf{linear transformations} \index{Linear Algebra!Linear Transformation}.
	
	As in linear algebra, we need our coordinate system to both \textbf{span}\index{Linear Algebra!Span} the space so that we can represent any point, and be \textbf{linearly independent}\index{Linear Algebra!Linear Independence} so that every point has exactly \emph{one} representation in our coordinate system. That's what a basis \emph{is}: it specifies a good coordinate system. 
	
	\begin{defn}
		A set of vectors is said to span a space $\mathbb{R}^n$ if every point $P$ can be represented as $a_1 \mathbf v_1 + \dots a_n \mathbf v_n$
	\end{defn}
	
	\begin{defn}
		A set of vectors $\{\mathbf v_1, \dots , \mathbf v_k \}$ is linearly independent if there is only one way to represent the zero vector $\mathbf 0$ as a combination of them, namely as $\mathbf 0 = 0v_1 + \dots + 0 v_k$.
	\end{defn}
	
	This second definition is the same as saying every point has a unique representation. If there were two ways to represent a point $P$: as $a_1 \mathbf v_1 + \dots + a_n \mathbf v_n$ and $b_1 \mathbf v_1 + \dots + b_n \mathbf v_n$, then subtracting these two different combinations would give a nonzero way to represent zero. Conversely, if there were a nonzero combination of vectors summing to zero, then we could add that combination to a coordinate representation of any point and get a \emph{different} representation of the same point. So coordinate representations for all vectors are unique as long as long as there is only one representation for zero: the trivial one.
	
	Bases that don't span, or are not linearly independent, would lead to coordinate systems like these:
	
	\textbf{Show a 2-D basis in a 3-D space, and a basis of 3 vectors in 2-D space}

	Very often in mathematics, we ask ``does a solution exist?'', and ``if there is a solution, is it unique?''. These two questions are dual to one another. If a set of vectors spans the space, then there \emph{exists} a way to represent any point (at least one way to represent any point). If a set of vectors is linearly independent, then \emph{if} you can represent a point, that representation is \emph{unique} (no more than one way to represent any point).
	
	Now to stress the same idea again: because points in $\mathbb R^n$ and vectors are essentially the same thing, the idea that points in space are invariant of a coordinate system of course applies to vectors. If we choose a basis for our vector space $\mathbf v_1, \dots \mathbf v_n$, then we can express any vector $\mathbf u$ by a unique combination $\mathbf u = a_1 \mathbf v_1 + \dots + a_n \mathbf v_n$. We then say that in this basis, we can represent $\mathbf u$ by a list of numbers:
	\begin{equation*}
		\mathbf u = \begin{pmatrix} a_1 \\ \vdots \\a_n	\end{pmatrix}
	\end{equation*}
	
	In some sense, this is wrong. The vector $\mathbf u$ represents something physical: a velocity, a force, the flow of water. It doesn't depend on the coordinate system. On the other hand, the right hand side is just a list of numbers that depend entirely on the coordinate system chosen. If we change coordinate systems, the right hand side changes. Because $\mathbf u$ exists (say, in the real world) independently of coordinates used, it does not change. 
	
	A vector is \emph{not} a list of numbers. Once we pick a basis, a vector can be \emph{represented by} a list of numbers, but if we change into a different basis, those numbers all have to change as well. This exact same idea will be the reason why a tensor is \emph{not} just a multi-dimensional array (like the ones you see in computer science). It can be \emph{represented by} a multi-dimensional array once we pick a coordinate system, but that representation by numbers will change depending on the system we pick. 
	
	The right way to write $\mathbf u$ would be:
	
	\begin{equation*}
		\mathbf u = \begin{pmatrix}
			\mathbf v_1  \dots \mathbf v_n
		\end{pmatrix}\begin{pmatrix} a_1 \\ \vdots \\a_n	\end{pmatrix} = a_1 \mathbf v_1 + \dots + a_n \mathbf v_n
	\end{equation*}
	
	If we \emph{vary} $\mathbf v_i$ to a different basis: $\mathbf v_i'$, then the coordinates will $a_i'$ vary the \emph{other} way, so that $\mathbf u = a_1 \mathbf v_1 + \dots + a_n \mathbf v_n = a_1' \mathbf v_1' + \dots + a_n' \mathbf v_n'$ is \emph{invariant} regardless of coordinate choice.
	
	That is, if we make the linear transformation $\mathbf v'_i = \sum_{j=1}^n A_{ij} \mathbf v_j$ then $a_i = \sum_{j=1}^n (A^{-1})_{ij} a_j$ so that:
	
	\begin{equation*}
		\sum_{i=1}^n a_i' \mathbf v_i' = \sum_{i=1}^n \sum_{j=1}^n (A^{-1})_{ij} a_j
	\end{equation*}
	
	\textbf{IDK HOW TO FINISH THIS}
	
	We say that the basis vectors $\mathbf v_i$ \textbf{co-vary}\index{covariant} and the coordinates $a_i$ \textbf{contra-vary}\index{contravariant} with the change of basis. The idea, although it sounds simple, is rather hard to get the feel of. It's worth thinking a good bit about how coordinates and bases need to vary in opposite ways so that the physical object represented by the coordinates stays the same regardless of how we look at it. 
	
	\textbf{This will be a caption for a sketch of a 3-D rotation}
	
	 When you rotate your character in a video game (or in real life too...), the world rotates \emph{contrary} to the direction you've rotated in. The coordinates of what you've seen have \emph{contra-varied} while your basis vectors have \emph{co-varied}. The end result is that despite changing your coordinate system, physics stays the same: invariant. This extends beyond just rotations to \emph{all} coordinate transformations around a point. 
	
	% section Linear Algebra & Coordinates (end)
	
	\section{Curvilinear Coordinates: Polar \& Beyond} % (fold)
	\label{sec:curvilinear_coordinates_polar_beyond}
	
	Perhaps you may be wondering why we've spent so much time on changing between coordinate systems represented by basis vectors centered at a fixed origin. You've seen change between cartesian and polar coordinates, and that has little to do with the linear change of basis that we've just discussed, right? 
	
	We could use something like a polar system of $(r,\theta)$ or a spherical system $(r, \phi, \theta)$. These are now not representable in terms of three axes, but instead look like this:
	
	\textbf{Graphic of polar coordinate system/spherical} 
	
	This is an example of a non-linear coordinate transformation. They are more commonly referred to as \textbf{curvilinear}. Whereas linear ones map lines to lines, curvilinear ones more generally map lines to curves. The idea for making sure that the equations of physics still stay true for non-linear coordinate transformations is to note that just like a curve locally looks like a line, a \emph{non-linear} transformation locally looks like a \emph{linear} one. The linear transformation that it locally looks like is called the \textbf{Jacobian} \index{Jacobian} $J$. If the laws of physics are invariant under linear transformations locally at each point, then \emph{globally}, they will be invariant under non-linear ones as well. That is why we cared about studying covariance and contravariance for linear transformations: it is what matters. 
	
	
	\textbf{We must elaborate on this more, draw some m.f. pictures}
	
	% section curvilinear_coordinates_polar_beyond (end)
	
	\section{The Manifold} % (fold)
	\label{sec:the_manifold}
	
	Everything we've done so far has been in $\mathbb{R}^n$. We have used different bases to represent it. Perhaps when we were first born, we expect that the universe looks like $\mathbb{R}^3$, and it goes out infinitely far in every direction.  Perhaps the first humans, too, expected that the surface of the earth looks like $\mathbb R^2$, stretching out infinitely. But we know now that the surface of the earth is \emph{not} the plane $\mathbb R^2$. We don't know so much about the universe, but there is no reason to expect it to be $\mathbb R^3$.
	
	We exist in the universe like 2-D people would exist on a sphere, or on some other object with rich geometry that by no means needs to be flat. Because the object is so large, we have no idea how it \emph{globally} looks, but \emph{locally}, just like 2-D humans would see a flat plane, we see $\mathbb R^3$. These geometric objects we are talking about are called \emph{manifolds}. What is a manifold? It is a set of points that looks like Euclidean space around each point.
	
	A line is a one-dimensional manifold (in fact it \emph{is} a Euclidean space) a circle is a one-dimensional manifold (when you zoom in on a point, it looks like a line), so is an ellipse, parabola, hyperbola, and the graph of any smooth function. A sphere is a two dimensional manifold (note that the sphere is just the 2-D surface of a 3-D ball). It locally looks like the euclidean plane, just as the world looks flat to us. The Mobius strip is also a two-dimensional manifold. 
	
	We need to be able to use 
	\textbf{Talk about the sphere here (not the metric part)}
	
	Just like in $\mathbb{R}^n$ where had different coordinate systems around an origin, on a manifold $M$, we will \emph{locally} at each point have coordinate systems that look exactly like the ones we used for $\mathbb{R}^n$ in section \ref{sec:Linear Algebra & Coordinates}. 
	
	Just because we have a coordinate system to describe the manifold doens't mean we have everything. It may seem strange, but up until now we have missed talking about a vital part of geometry: the notion of \emph{distance}. Even though we've talked in all the way points can be represented by coordinates, none of these numbers representing coordinates have any \emph{inherent} notion of distance to them. 
	
	(What you dealt with before is called affine space)
	
	(Difference between affine space and Euclidean space: the metric)
	
	
	% section the_manifold (end)
	
	\section{The Field} % (fold)
	\label{sec:the_field}
	
	One of the most important aspects of physics is studying the \emph{fields} that live on manifolds. Just like in multivariable calculus, this means the study of scalar fields $\phi$ that associate a number to every point $P$. Examples are voltage, potential energy, mass/charge density, etc. This also includes the study of vector fields $\mathbf v$ associating a vector to each point. These can be wind speeds, electric fields. 
	
	One of the important things to note is that while scalar fields associate a number $\phi(P)$ to each point $P$, which looks the same regardless of the local coordinate system used at $P$, vector fields will associate a specific vector $\mathbf u$ to a point $P$ whose coordinate representation will, of course, change depending on the local coordinate system at $P$.
	
	So let's around that part of the manifold, our coordinate patch has the $n$ coordinates $q_i$. At $P$, each $q_i$ is assigned a value. 
	
	{\emph{[I want to somehow motivate why the vectors should looks like $\partial/\partial q_i$]}}
	
	\textbf{Show a graph of the curves for coordinates $q_i$}
	
	\section{What Follows} % (fold)
	\label{sec:what_follows}
	
	The rest of this book will expand both on the geometry of fields and manifolds, and also on the larger ideas of groups, homogenous spaces, and representations. \\
	
	In chapter 1, we will continue studying the fields that live on manifolds. We'll prove the General Stokes' theorem, an elegant generalization of the divergence, curl, and line integral theorems that have been taught in multivariable calculus. From there, we will study more thoroughly the concept of a metric, and how this relates vector fields to differential forms. The notion of a derivative will be extended to manifolds, and will take the form of a ``Lie Derivative''.\\
	
	In chapter 2, we will introduce Fourier Analysis as a powerful tool for studying functions on the real line an Euclidean space. Then we will shift to looking at the representation theory of \emph{finite} groups and illustrate the parallels. We will then return to the study of continuous group actions on especially symmetric ``homogenous'' spaces, and show how Fourier analysis is related to their representation theory. Towards the end, we will expand on the idea behind a group actions on manifolds and look at the representation theory, giving a small glimpse into harmonic analysis: the Fourier transform on manifolds. Just as in the first chapter, we'll recognize the importance of the underlying differential geometry of the group action. The underlying differential structure is known as the ``Lie Algebra'' of the group, and we will discuss that.\\
	
	In chapter 3, we introduce some background behind Lie Algebras. We put almost all of our focus on one special case: the Lie algebra $\frak{sl}_2 (\mathbb C)$. The relationship between this algebra and the symmetries of the sphere are explored, as well as its applications in quantum physics for studying angular momentum. The representation theory of a variant of this algebra gives rise to the concept of spin. \\
	
	In chapter 4, we move further into physics, going over classical Lagrangian and Hamiltonian Mechanics. We discuss Noether's theorem in both the Lagrangian and Hamiltonian Pictures, and then we move to study Hamiltonian mechanics using the language of differential geometry that we have developed. This will give rise to \emph{symplectic geometry}. In chapter 7, combining this with representation theory gives rise to \emph{quantum mechanics}.\\
	
	In chapter 5, we apply differential geometry first to the study of electromagnetism, and then to gravitation. We shall arrive at Einstein's theory of gravity. Along the way, we study in even greater detail the notion of a metric, a connection, and curvature. \\
	
	In chapter 7, we use the representation theory and differential geometry that we have developed so far to study how quantum mechanics can arise from quantizing a symplectic manifold.\\
	
	Finally, chapter 8 studies Lie algebras in greater detail, working towards the \emph{classification of complex semisimple Lie algebras}. Along the way, we will look at the relationship between representation theory of Lie algebras and modern physics. 
	
	% section what_follows (end)
	
	% section the_field (end)
	
	
\end{document}