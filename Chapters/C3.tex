\documentclass[../master.tex]{subfiles}

\begin{document}

\chapter{Differential Geometry}\label{ch:diffgeo}\thispagestyle{empty}

In calculus class, the fundamental theorem of calculus is introduced: that the total difference of a function's value at the end of an interval from its value at the beginning is the sum of the infinitesimal changes in the function over the points of the interval:

	\begin{equation}\label{eq:FTOC}
		\int_a^b f'(x) dx = f\Big\rvert^b_a
	\end{equation}

And later, in multivariable calculus, more elaborate integral formulae are encountered, such as the divergence theorem of Gauss:

	\begin{equation}\label{eq:Divergence}
		\int_\Omega \nabla \cdot \mathbf{F} ~ dV = \int_S \mathbf{F} \cdot d\mathbf S
	\end{equation}

	where $\Omega$ is the volume of a 3D region we are integrating over, with infinitesimal volume element $dV$ and $S$ is the surface that forms the boundary of $\Omega$. $dS$ then represents an infinitesimal parallelogram through which $\mathbf{F}$ is flowing out, giving the flux integral on the right. Read in English, Gauss' divergence theorem says ``Summing up the infinitesimal flux over every volume element of the region is the same as calculating the total flux coming out of the region''. The total flux coming out of a region is the sum of its parts over the region. You might see that in English, this reads very similar to the description of the fundamental theorem of calculus.
	
	Alongside this, there is Stokes' theorem for a 2D region. In English: summing up the infinitesimal amount of circulation of a vector field $\mathbf F$ over every infinitesimal area is equal to calculating the total circulation of $\mathbf F$ around the boundary of the region. In mathematical language:
	
	\begin{equation}\label{eq:Stokes}
		\int_R \nabla \times \mathbf{F} ~ dA = \int_C \mathbf{F} \cdot d\mathbf r
	\end{equation}
	
	where $R$ is our region and $C$ is its boundary.
	
	Perhaps now, the pattern is more evident. In all the above cases, summing up some \emph{differential} of the function on the interior of some region is the same as summing up the function itself at the \emph{boundary} of the region. All these theorems, that on their own look so strange to a first-year calculus student, are part of a much more general statement, the \textbf{General Stokes' Theorem}\index{Stokes' Theorem!General}:

	\begin{theorem}[General Stokes' Theorem]\label{thm:GeneralStokes}
		\begin{equation} \label{eq:GeneralStokes}
			\int_\Omega \mathrm d \omega = \int_{\partial \Omega} \omega.
		\end{equation}
	\end{theorem}

	

	
	Above, $\omega$ is an object that will generalize both the ``functions" and ``vector fields" that you've seen in multivariable calculus, and $\mathrm d$ will generalize of all the differential operators (gradient, divergence, curl) that you've dealt with. Lastly, when $\Omega$ is the region in question $\partial \Omega$ represents the \emph{boundary} of the region $\Omega$. The fact that it looks like a derivative symbol is no coincidence, as we'll see that the natural way to define the ``derivative'' of a region is as its boundary.
	
	%This is good, I feel like it belongs here, whatchu think Hilldog
	
	Through abstraction, we can reach results like this that not only look elegant and beautiful, but also provide us with insight into the natural way to view the objects that we've been working with for centuries. This gives us not only understanding of what language to use when studying mathematics, but also what is the natural language in which to describe the natural world. The general Stokes' theorem is one of the first examples of this beautiful phenomenon, and this book will work to illustrate many more. 
	
	%Again, this is outdated now, we've already talked about this 
	
	For the first half of this chapter, we will work towards giving the intuition behind  this result. On our way, we will begin to slowly move into a much more general setting, beyond the $3$-dimensional world in which most of multivariable calculus was taught. That doesn't just mean we'll be going into $n$-dimensional space. We'll move outside of euclidean spaces that look like $\mathbb{R}^n$, into non-euclidean geometries. This will put into question what we really mean by the familiar concepts of ``vector'', ``derivative'', and ``distance'' as the bias towards Euclidean geometry no longer remains central in our minds. At its worst, the introduction of new concepts and notation will seem confusing and even unnecessary. At its best, it will open your mind away from the biases you've gained from growing up in a euclidean-looking world, and give you a glimpse of how modern mathematics \emph{actually} looks. 
	
	%This is such a good paragraph, it probably belongs somewhere earlier on!
	
	Modern mathematics is learning that the earth isn't flat. To someone who's never had those thoughts, it is difficult to get used to, tiring, and sometimes even rage inducing, but to someone who has spent months thinking and reflecting on it, it quickly becomes second nature. Far from being the study of numbers or circles, it is a systematic climb towards abstraction.  It is a struggle towards creating one language, free from all-encompassing human bias, in order to try and describe a world that all other human languages, for so many centuries, have failed to grasp. It is humbling, and in the strangest of ways, it is profoundly beautiful.



\section{The Derivative and the Boundary} % (fold)
\label{sec:the_derivative_and_the_boundary}

% section the_derivative_and_the_boundary (end)

	Let's start working towards understanding Equation~\eqref{eq:GeneralStokes}. First, let's work with what we've already seen to try and explore the relation between integrating within a region and integrating on the boundary. 
	
	If we are in one dimension, we have a function $f$ defined on the interval $x \in [a,b]$. Proving Equation~\eqref{eq:FTOC} is much easier than you'd think. Let's take a bunch of steps: $x_i = a + (b-a)i/N$, so that $x_0 = a, x_N = b$. Then all we need is to form the telescoping sum:	
	\begin{align*}
		f\rvert^b_a &= f(x_N) - f(x_0) \\& = \sum_{i=1}^N f(x_{i})-f(x_{i-1}).
	\end{align*}
	If we make the number of steps $N$ large enough, the stepsize shrinks so that in the limit, we get
	\begin{align*}
		\lim_{N \rightarrow \infty} 	\sum_{i=1}^N f(x_{i})-f(x_{i-1}) & = \lim_{N \rightarrow \infty} 	\sum_{i=1}^N \Delta f \\ & = \int_a^b df.
	\end{align*}
	Of course, the way its written more often is:
	\begin{align*}
		\lim_{N \rightarrow \infty} 	\sum_{i=1}^N \frac{\Delta f}{\Delta x} \Delta x  = \int_{a}^{b} \frac{df}{dx} dx.
	\end{align*}
	
	What is the idea of what we've done? At each point we've taken a difference of $f$ at that point with $f$ at the preceding one. Because we're summing over all points, the sum of differences between neighboring points will lead to cancellation everywhere \emph{except} at the boundary, where there will not be further neighbors to cancel out the $f(b)$ and $f(a)$. From this, we get Equation~\eqref{eq:FTOC}. 
	
\noindent \textbf{Note:}
	Now for a distinction which may seem like it isn't important. We haven't integrated from point $a$ to point $b$. We have integrated from where the coordinate $x$ take \emph{value} $a$, to the where coordinate $x$ takes \emph{value} $b$. $a$ and $b$ are \emph{NOT} points. They are numbers, values for our coordinate $x$. As we have said in the preceding chapter, the idea that numbers form a \emph{representation} for points is ingenius, but numbers are \emph{not} points. Although we could write this interval as $[a,b]$ in terms of some variable $x$, it would be a completely different interval should we have chosen a different coordinate $u$. This is why, when doing $u$-substitution, we change the bounds. In coordinate free, language, then:

	
	\begin{theorem}[Fundamental Theorem of Calculus]\label{thm:FTOC}
		For a given interval $I$ with endpoints $p_0, p_1$ and a smooth function $f$, we have
		\begin{equation}
			  \int_{p_0}^{p_1} df = f \Big\rvert_{p_0}^{p_1} 
		\end{equation}
	\end{theorem}
	Notice something: the end result doesn't depend on the partition $x_i$ at all, so long as it becomes infinitesimal as $N \rightarrow \infty$. That is to say: we are summing up the change of $f$ over some interval, but it doesn't matter what coordinate system we use to describe this interval. The integral is \emph{coordinate independent}. We chose to use $x$ as our coordinate, describing the interval as going from $x=a$ to $x=b$, but we didn't \emph{have} to make this specific choice. This makes perfect physical sense. For example, if we had a temperature at each point in space, the temperature difference between two fixed points some shouldn't depend on whether we use meters or feet to measure their distance apart.  
	
	Written mathematically: 
	\begin{align*}
		\int_I df = \int_I \frac{df}{dx} dx = \int_I \frac{df}{du} du 
	\end{align*}
	
	If we chose an $I$ that's very small around some point, essentially an infinitesimal line segment, we get:
	\begin{align*}
		\frac{df}{dx} dx =  \frac{df}{du} du \Rightarrow \frac{df}{dx} = \frac{df}{du} \frac{du}{dx}
	\end{align*}
	this is the $u$-substitution rule from calculus.
	\\

	
	Now what if $f$ was a function defined not on the real line $\mathbb{R}$, but on 2-dimensional space $\mathbb{R}^2$, or more generally $n$-dimensional space $\mathbb{R}^n$. To each point $p = (p_1, \dots, p_n)$ we associate $f(p)$. Now again, consider $f(p_f)-f(p_i)$ for two points in this space.
	
	For any curve $C$ going between $p_i$ and $p_f$, say defined by $\mathbf r(t)$ for $t$ a real number going from $a$ to $b$, we can make the same partition $t_i = a + (b-a)i/N$ and let $N$ get large. Again, it becomes a telescoping sum:
	\begin{align*}
		f(p_f) - f(p_i) = &f(\mathbf r(b)) - f(\mathbf r(a)) \\= & \sum_{i=1}^N f(\mathbf r(t_{i}))-f(\mathbf r(t_{i-1})) \\ = & \sum_{i=1}^N \Delta f_i  \rightarrow \int_C df.
	\end{align*}
	Now if we cared about coordinates, we could ask ``how can we write $df$ in terms of $dt$ or $dx_i$?''. 
	
	We know from the multivariable chain rule that the infinitesimal change of $f$ is the sum of the change in $f$ due to every individual variable, so: 
	\begin{equation}
		df = \sum_i \frac{df}{dx_i} dx_i
	\end{equation}

	We know that $dx_i$ together must lie along $C$. In terms of $t$ since $x_i = r_i (t)$, we have $dx_i = \frac{dr_i}{dt} dt$ giving:
	\begin{theorem}[Fundamental Theorem of Line Integrals]
	For a smooth function $f$ defined on a piecewise-smooth curve $C$ parameterized by $\mathbf r(t)$
		\begin{equation}
			f\rvert^{p_f}_{p_i} = \int_C \sum_i \frac{df}{dx_i} \frac{dr_i}{dt} dt = \int_C \nabla f \cdot \frac{d \mathbf r}{dt} dt =  \int_C \nabla f \cdot d \mathbf r
		\end{equation}
	\end{theorem}
	The proof of this was no different from the 1-D case.\\
	
	Let's go further. Consider a region in three dimensions and a vector field 
	\begin{equation*}
		\mathbf F = F_x \hat{\mathbf{i}} + F_y \hat{\mathbf j} + F_z \hat{\mathbf k}
	\end{equation*} 
	We want to relate the total flux coming out of the region to the infinitesimal flux at each point inside the region. To do this, as before, we will subdivide the region. This time, it will not be into a series of intervals, but instead into a mesh of increasingly small \emph{cubes}, as below.
	
	\todofig{Flux Graphic}
	
	See that the flux out a side of each cube is cancelled out by the corresponding side on its neighboring cube. That means that the only sides that do not cancel are for the cubes at the boundary$^1$\footnote{You may be worried that the cubes do not perfectly fit into the boundary when it is not rectangular. As the mesh gets smaller and smaller, it approximates the region better so this does not pose a problem. This idea can be made rigorous (c.f. \todoref{Stokes' Theorem Rigor})}, giving us the desired flux out.
	
	So if we sum the fluxes over all infinitesimal cubes, we will get the total flux out of the boundary. For a single cube of sides $dx,dy,dz$, drawn below, the total flux will be the sum over each side. 
	\begin{align*}
		\text{Flux} =&~~~  F_z(x,y,z+dz/2) dx dy -  F_z(x,y,z-dz/2) dx dy \\ 
						   & + F_y (x,y+dy/2,z) dx dz - F_y (x,y-dy/2,z) dx dz \\ 
						   & + F_x (x+dx/2,y,z) dy dz - F_x (x-dx/2,y,z) dy dz \\ 
	\end{align*}
	\todofig{Figure of infinitesimal flux on a cube: Stokes' proof}
	
	But we can write this as: 
	\begin{align*}
		\left( \frac{\partial F_x (x,y,z)}{\partial x} + \frac{\partial F_y (x,y,z)}{\partial y} + \frac{\partial F_z(x,y,z)}{\partial z} \right) dx dy dz = \nabla \cdot \mathbf F ~ dV
	\end{align*}
	So the total flux will be the sum over all these cubes of each of their total fluxes. But then this becomes exactly the divergence theorem:
	\begin{theorem}[Divergence Theorem, Gauss]
	For a smooth vector field $\mathbf F$ defined on a piecewise-smooth region $\Omega$, then we can relate
		\begin{equation*}
			\int_\Omega \nabla \cdot \mathbf F ~ dV = \int_{\partial \Omega} \mathbf{F} \cdot d \mathbf S
		\end{equation*}
	\end{theorem}
	
	It is an easy \textbf{exercise} to show that this exact same argument holds for an $n$-cube. 
	
	What did we do? In the fundamental theorem of calculus/line integrals, we had a function $f$ evaluated on the 1-D boundary, and we chopped the curve into little pieces that cancelled on their neighboring boundaries, making a telescoping sum. Then we evaluated the contribution at each individual piece, and found that it was $df = f'(x_i) dx$, meaning that the evaluation on the boundary could be expressed as an integral of this differential quantity over the curve. That is Equation~\eqref{eq:FTOC}.
	
	For the divergence theorem, we had a vector field $\mathbf F$, again \emph{evaluated on the boundary}, this time in the form of a surface integral. We chopped the region into little pieces (cubes now) that cancelled on their neighboring boundaries, making a telescoping sum. Then we evaluated the contribution at each individual piece and found that it was $\nabla \cdot \mathbf F ~ dV$, meaning that the integration on the boundary could  be expressed as an integral of this differential quantity over the region. That is Equation~\eqref{eq:Divergence}.
	
	
	Through abstraction, we see that there is really no difference. Perhaps now Equation~\eqref{eq:GeneralStokes} does not look so mysterious and far-off.\\
	
	For Equation~\eqref{eq:Stokes}, we have a vector field $\mathbf F$ evaluated on the boundary in the form of a contour integral around a region. This is the total circulation of $\mathbf{F}$ around the region. Let us chop the region into little pieces. 
	
	\todofig{Figure of infinitesimal circulation on a sqauare: Stokes' proof}
	
	On an infinitesimal square, we get that the circulation is:
	\begin{align*}
		\text{Circulation} =& ~~~  F_y (x+dx/2,y) dy - F_y (x-dx/2,y) dy \\ &+  F_x (x,y-dy/2) dx - F_x (x,y+dy/2) dx
	\end{align*}
	This can be written as:
	
	\begin{align*}
		\left( \frac{\partial F_y}{\partial x} - \frac{\partial F_x}{\partial y} \right) dx dy = (\nabla \times \mathbf F) ~ dA
	\end{align*}
	so that
	\begin{theorem}[Stokes' Theorem in $2$D] For a smooth vector field on a piecewise smooth region $S$ 
		\begin{equation}
			\int_C \mathbf{F} \cdot d\mathbf r = \int_S \nabla \times \mathbf{F} ~ dA
		\end{equation}
	\end{theorem}
	Exercise \textbf{(MAKE AN EXERCISE)}\todoex{Make an exercise to generalize classical Stokes' to 3D} generalizes this to a surface in 3D, to get the 3D version of Stokes' theorem \index{Stokes' Theorem!For Curl}:
	
	\begin{equation}
		\int_C \mathbf{F} \cdot d\mathbf r = \int_S (\nabla \times \mathbf{F}) \cdot d\mathbf S 
	\end{equation}
	
	The philosophy behind these proofs is always the same. It is the manipulation of the differentials that seems wildly different every time. The curl looks nothing like a divergence, and a divergence is distinct from a gradient. Moreover, its not clear in what way each one generalizes the one dimensional derivative $df = f'(x) dx$. This is the problem that the symbol `$\mathrm d$' in Equation~\eqref{eq:GeneralStokes} was made to solve.\\
	
	We must stop thinking of the $1$D derivative, the gradient, the divergence, and the curl, as unrelated operations. They are in fact, the same operation, applied in different circumstances. Infinitesimal change, flux, and circulation are all the same differential action applied to different types of objects. 
	
	Perhaps part of this was clear from multivariable calculus: the gradient is nothing more than a generalization of the derivative to functions on higher dimensions. Then why are there seemingly two different, unrelated types of ``derivative'' on vector fields? Instead of a regular, gradient-like object, we have two: the divergence and the curl. 
	
	It will turn out that the reason that there are two is this: the vector fields that we take curls of are a different type of object from the vector fields we take divergences of. Looking forward, we'll see that we only take the curl on a vector field that is ``meant to be integrated along curves" (\textbf{1-form}), and the curl gives us another vector field ``meant to be integrated over surfaces" (\textbf{2-form}). On the other hand, the divergence takes a vector field ``meant to be integrated over surfaces'' (\textbf{2-form}) and gives us a scalar field ``meant to be integrated over volumes'' (\textbf{3-form}). Every object that we've encountered when integrating: from functions in 1-D or 3-D, to vector fields in $n$-D, have been examples of these \textbf{forms}. \index{Differential Form} \\
	
	To get to this idea, we first need to stop thinking of functions and vector fields as totally separate objects. A function is an object that is ``meant to be evaluated at a point'' (\textbf{0-form}). The derivative takes us from a function to a 1-form, meant to be integrated along a curve. It is the exact same object as the one in Section~\ref{sec:vectors_reimagined}. The gradient, properly speaking, is not a vector describing the local behavior of a \emph{curve} but is the opposite: a 1-form describing the local behavior of a \emph{function}.
	So the correct way of writing this, is to go from the old $\mathbb{R}^3$ notation
	\begin{equation*}
		\nabla f = \frac{\partial f}{\partial x} \hat{\mathbf i}
					+\frac{\partial f}{\partial y} \hat{\mathbf j}
					+\frac{\partial f}{\partial k} \hat{\mathbf k}
	\end{equation*}
	to the modern language
	\begin{equation}
		\mathrm d f  = \frac{\partial f}{\partial x^i} dx^i ~\Big(= \nabla f \cdot d \mathbf r\Big)
	\end{equation}
	This is a 1-form, as we already know.
	
	What we would like is to have the old multivariable calculus chain
	\begin{equation*}
		\text{functions} ~ \overbrace{\longrightarrow}^{\text{grad}}
		~ \text{vector fields} ~ \overbrace{\longrightarrow}^{\text{curl}}
		~ \text{vector fields} ~ \overbrace{\longrightarrow}^{\text{div}} 
		~ \text{functions} 
	\end{equation*}
	be converted to
	\begin{equation*}
		~~~~~~
		\begin{matrix}
			\text{0-forms} & \overbrace{\longrightarrow}^\mathrm d & 
			\text{1-forms} & \overbrace{\longrightarrow}^\mathrm d & 
			\text{2-forms} & \overbrace{\longrightarrow}^\mathrm d & 
			\text{3-forms}  \\
			\downarrow & & \downarrow & & \downarrow & & \downarrow \\
			^{\text{Evaluated}}_{\text{~~~~~at}} & & 
			^{\text{Integrated}}_{\text{~~~along}} & &
			^{\text{Integrated}}_{\text{~~~along}} & &
			^{\text{Integrated}}_{\text{~~~along}} & &\\
			\downarrow & & \downarrow & & \downarrow & & \downarrow \\
			\text{Points} & \underbrace{\longleftarrow}_\partial & 
			\text{Curves} & \underbrace{\longleftarrow}_\partial & 
			\text{Surfaces} & \underbrace{\longleftarrow}_\partial & \text{Volumes}
			
			% \downarrow ^{\text{Evaluated}}_{\text{~~~~~at}} \downarrow & &
% 			\downarrow ^{\text{Integrated}}_{\text{~~~along}} \downarrow & &
% 			\downarrow ^{\text{Integrated}}_{\text{~~~along}} \downarrow & & 			\downarrow ^{\text{Integrated}}_{\text{~~~along}} \downarrow \\
		\end{matrix}
	\end{equation*}
	where $\partial$ is the \textbf{boundary operator}\index{Boundary Operator} that takes us from an $n$ dimensional manifold to its $n-1$ dimensional boundary. At this point, we won't be afraid to use our new word: manifold\index{Manifold}, when referring collectively to curves, surfaces, volumes, or any of their higher dimensional generalizations (points too, as the degenerate 0 dimensional case). 
	
	As a final note of this section, let us try to give a sketch for why on a region $\Omega$, we denote its boundary with the partial derivative symbol as $\partial \Omega$. Picture in your mind a ball (interior of a sphere) of radius $r$,  $B_r$. If we increase the radius by a tiny amount $h$ then we have a slightly larger radius $B_{r+h}$. If we took the difference $B_{r+h} - B_r$, by which we mean all the points of $B_{r+h}$ that are not in $B_r$, we would be left with a thin shell. In the limit as $h \rightarrow 0$, this becomes a sphere of radius $r$, precisely the boundary of $B_r$ (note that a sphere is always the two-dimensional boundary of the ball). See how similar this is to taking derivatives. This is why $\partial B_r$ is what we use to denote the sphere boundary of the ball. 
	
	You may ask ``but what about dividing by $h$ at the end, like we do for a regular derivative?''. This also has an interpretation. The 3D volume of a sphere is zero, since it is a 2-D boundary. Dividing by $h$ as $h$ goes to zero puts increasing ``weight'' on the shell so that as the shell shrinks to becoming absolutely thin, 3-D integrals on it become 2-D \footnote{For those familiar with the terminology: dividing by $h$ corresponds to multiplying by a dirac delta that spikes exactly on the sphere. This turns integrals over 3-D space into 2-D integrals on the sphere}.
	 % But even here, we can go deeper. We've figured out why the 1-D integral becomes just a difference at two points, but we can actually interpret the \emph{difference} $f(b)-f(a)$ an integral! It is a zero-dimensional integral over the boundary of the region. The boundary of an interval is just two points, and a zero dimensional integral is a sum over points.
 %
	
	
	
\section{The Notion of a Form}

	A differential form $\omega$, in short, is an object that is meant to be integrated. We've seen the example of 1-forms in the preceding chapter. At a point $p$ on a manifold, one-forms $\omega$ are exactly all the first-order behaviors of the functions at $p$. Just as we can have a vector field on $\mathbb{R}^3$, or manifolds in general, we have 1-form fields. You have seen this before: the gradient is in fact a one-form field
	\begin{equation}
		(\nabla f \cdot d\mathbf r) (p) = \frac{\partial f}{\partial q^i}(p)~ dq^i
	\end{equation}
	The components of the gradient are \emph{covariant} with our change of coordinate system, just like those of a 1-form (and unlike those of a vector field). 
	
	A general 1-form associates a first-order function behavior $\omega_i(p)~ dq^i$ to every point $p$ in space. Just because $\omega$ is some differential behavior of a function at a point $p$ doesn't mean that $\omega$ actually \emph{is} the differential of a function. That is, it doesn't mean there exists a function so that $\partial f/\partial q^i = \omega_i$ at every point $p$ so that $\omega = \mathrm df$.
	
	It rarely true that $\omega = \mathrm df$. In fact, this happens \emph{exactly} when $\omega$ actually \emph{does} correspond to a gradient. This is exactly what we called a conservative vector field in introductory physics and in multivariable calculus. In this language, we call $\omega$ an \textbf{exact form}\index{Differential Form!Exact} when it is the differential of a function.
	
	
	
	This is what we care about when integrating. It is more fundamental than $\mathbf{F}$, but what does it mean \emph{physically}? If $\mathbf{F}$ was a force field, then since we know $\mathbf{F} \cdot d \mathbf{r} = dW$, this form $\omega$ represents all possible infinitesimal changes in work $dW$ at a given point, depending on what changes $dx,dy,dz$ we do.
	
	If we were actually \emph{given} the changes in each of the coordinates $dx,dy,dz$, we could plug them in to $\omega$ and get the first-order approximation of the amount of work done over that distance. This is a point that has been said before: $\omega$ does not represent a specific change in work, but rather the \emph{relationship} between the changes in coordinate and the change in work. If you \emph{give it} an infinitesimal displacement, it will tell you the associated work. When integrating along a curve, the displacement is simply the tangent vector to the curve.

	Even simpler than one-forms are the \textbf{zero forms}, with no differentials appearing. A zero-form precisely a scalar function at $f(p)$ each point $p$. Regardless of how we change our coordinate system, the value of the \emph{function} at point $p$ is the same.
	
	We are now in a good place to define $\mathrm d$, at least for going from functions (zero-forms) to one-forms. Given a function $f$, $\mathrm d f$ will produce a form representing the local change in $f$ depending on the displacement. We call $\mathrm d$ the \textbf{exterior derivative}\index{Exterior!Derivative}\index{Derivative!Exterior} operator.
	
	For example, for a potential energy function $\phi$, $\mathrm d \phi$ can be written as 
	
	\begin{equation}
		\mathrm d \phi = \sum_{i=1}^n \frac{\partial \phi}{\partial x^i} dx^i
	\end{equation}
	
	because of $\mathrm d$, we will no longer have to use the gradient at all. This is more important than simply meaning that we'll grow to stop using $\mathbf{\hat i}, \mathbf{\hat j},\mathbf{\hat k}$. It is something much deeper. In in two-dimensional motion, if you have some potential $\phi$ at a point $p$, then of course the value of $\phi$ at $p$ is independent of any coordinate system you use. If you have two cartesian coordinates, say $x,y$, then you can define the $x,y$ components of force by 
	\begin{equation*}
		\mathrm d \phi= \frac{\partial \phi}{\partial x} dx + \frac{\partial \phi}{\partial y} dy= F_x ~ dx + F_y ~dy 
	\end{equation*}
	If our coordinates were $r,\theta$, then the analogous force would be the covariant components of the same \emph{form}, in a different coordinate system:
	\begin{equation*}
		\mathrm d \phi = \frac{\partial \phi}{\partial \theta} d\theta + \frac{\partial \phi}{\partial r} dr = F_\theta ~ d\theta + F_r ~ dr
	\end{equation*}
	Note that the first component has units not of force, but of force times distance. It is precisely the torque that the potential induces. In this sense, quantities like torque are precisely just generalizations of force to non-cartesian coordinate systems (polar, in this case). The second component is just radial force, plain and simple.
	
	To hammer the point across: these two ``forces'' have components that mean completely different things, and cannot easily be compared. On the other hand, since $\mathrm d \phi$ is independent of coordinate system, we get:
	
	\begin{equation}
		\mathrm d \phi = F_x ~ dx + F_y ~ dy = F_\theta ~ d\theta + F_r ~ dr 
	\end{equation}
	
	Because we know how to go from $x,y$ to $r, \theta$ and because this nonlinear change of coordinates is \emph{linear} at every point on the differentials, this would allow us to go between the language of ``x-y force'' and the language of ``torque + radial force about the origin''  at any point.
	
	All forces (including the generalized forces, like torque) are covariant coefficients of the invariant differential form for work. If you're working in a coordinate system $q^i$, whether it be cartesian $x,y,z$ or polar $r, \theta$, then the coefficient corresponding to $dq^i$ is precisely the generalized force associated with that coordinate in your system.\\
	
	
	% \begin{concept}[One-Forms Relate Change to Direction]
	% 	For a function $\phi$, the one-form $\omega = \mathrm d \phi$ gives the first-order change in $\phi$ along a given direction $(dx^1,\dots, dx^n)$. In general, for a one-form $\omega$ that is not exact, $\omega$ along a given direction $(dx^1,\dots, dx^n)$ gives the change in a quantity that cannot be represented by a function of the coordinates. This occurs, for example, with non-conservative forces such as friction or when calculating heat added to a system.
	% \end{concept}
	
	
	\section{The Exterior Algebra and the Wedge} % (fold)
	\label{sec:the_exterior_algebra_and_the_wedge}
	
	If a $1$-form must be fed a vector of some associated change of coordinates $dq^i$, then what about a $2$ form? A 2-form, $\omega$, should associate a ``flux" out of a plane, so we need $\omega$ to be given a plane associated with \emph{two} directions $v^i, u^i$, and then $\omega$ acting on these two directions would give the associated ``flux'' out of the infinitesimal parallelogram gained by varying $q^i$ in both directions. So if $\omega$ were a 1-form, it would act on one vector as $\omega(\mathbf v)=\omega_i v^i$, but now $\omega$ as a 2-form acts on two vectors:
	\begin{equation*}
		\omega(\mathbf u, \mathbf v) \longleftrightarrow \text{Flux Coming out of $\mathbf u$ and $\mathbf v$s Parallelogram}
	\end{equation*}
	This is an intuitive \emph{geometric idea}. This is what we want to be true, and the following observations will be \emph{algebraic properties of $\omega$} based on our geometric notions of flux.\\
	\todofig{1-form giving flux out of a parallelogram.. how is this even drawn?}

	\begin{obs}
		$\omega(\mathbf v, \mathbf v) = 0$
	\end{obs}
	The parallelogram generated by $\mathbf v$ and itself has no second dimension, so it has no area. Therefore there isn't room for any flux to come out of it.
	\begin{obs}
		$\omega(2\mathbf u, \mathbf v) = 2 \omega(\mathbf u, \mathbf v)$ and $\omega(\mathbf u,2 \mathbf v) = 2 \omega(\mathbf u, \mathbf v)$.\\
		Moreover, in general $\omega(a \mathbf u, \mathbf v) = \omega(\mathbf u,a \mathbf v)= a \omega(\mathbf u, \mathbf v)$ for $a>0$.
	\end{obs}
	Of course, if we scaled the parallelogram by some positive amount $a$ along one of the sides, then its total area scales by $a$, so that $\omega$ gives $a$ times as much ``stuff'' coming out of the scaled parallelogram. This observation, together with our linear algebraic ideas, suggest that this should naturally extend beyond positive $a$ so that $\omega(-\mathbf u,\mathbf v) = -\omega(\mathbf u, \mathbf v)$, giving us negative flux through the parallelogram. But what does that mean? This means that if one of the vectors gets scaled negatively, the \emph{orientation} of the new parallelogram reverses. 
	\todofig{Oriented Parallelograms}
	
	So now even though the pair $-\mathbf u, \mathbf v$ are on the same plane, the notion of ``out" through their parallelogram has reversed. This makes physical sense, 
	\begin{concept}
		What matters, when finding the flux associated with $\omega$ along two directions $\mathbf u, \mathbf v$
		\begin{enumerate}
			\item The \emph{plane} that $\mathbf u, \mathbf v$ span, through which the flux is going
			\item That scaling $\mathbf u$ or $\mathbf v$ also scales the flux.
			\item The \emph{orientation} for which direction is in and which is out.
		\end{enumerate}
	\end{concept}
	This is exactly what we've seen before with the \textbf{Right-Hand Rule}\index{Right-Hand Rule}. Because of this, and from what we've seen before with objects like cross products, we would \emph{expect} that $\omega(\mathbf u, \mathbf v)$ represents one orientation, and $\omega(\mathbf v, \mathbf u)$ represents the opposite one so that 
	 \begin{equation*}
	 	\omega(\mathbf u, \mathbf v) = -\omega(\mathbf v, \mathbf u)
	 \end{equation*}
	we can add this as another property \emph{but} we can instead actually prove it if we make just one more geometric observation
	\begin{obs}
		$\omega(\mathbf u+\mathbf v, \mathbf w) = \omega(\mathbf u, \mathbf w) + \omega(\mathbf v, \mathbf w)$
	\end{obs}
	If $\mathbf u = a \mathbf v$ are linearly dependent then this is just the scaling observation. If $\mathbf v$ is linearly dependent with $\mathbf w, \mathbf u = a \mathbf w$, then this is just the observation that the parallelogram of $(\mathbf u, \mathbf w)$ would have the same amount of associated area if you were to add vectors in the direction of $\mathbf w$ to $\mathbf u$ so $\omega(\mathbf u + a \mathbf w, \mathbf w) = \omega(\mathbf u, \mathbf w)$. It's the same idea if $\mathbf u$ is linearly dependent with $\mathbf w$. Now let's assume all vectors are linearly independent. Geometrically, the two planes associated with $(\mathbf u,\mathbf w)$ and $(\mathbf v,\mathbf w)$, and the plane associated with the sum $(\mathbf u + \mathbf v, \mathbf w)$ can look like:
	
	\todofig{Proving additivity for forms, sum of planes. Need to discuss about this argument}

	Now $\omega$ represents a constant flux in space. Enclosing a region by these three planes should mean that the flux that goes through the first two is the flux that comes out of the third.
	
	The same exact argument can be applied to show this holds for the second slot in $\omega(-,-)$.
	\begin{cor}
		The above observations imply that $\omega$ is linear and \emph{\textbf{antisymmetric}} in its arguments so that $\omega(\mathbf u, \mathbf v) = -\omega(\mathbf v, \mathbf u)$. That is, as we expected, reversing the order of $\mathbf u$ and $\mathbf v$ reverses the orientation of the plane.
	\end{cor}
	\begin{proof}
		We have shown that $\omega$ is compatible with scaling and addition in each argument, so it is linear in both.
		
		Now consider $\omega(\mathbf u + \mathbf v, \mathbf u + \mathbf v)$. By our first observation, such a parallelogram has no area, so this is zero. On the other hand, by the linearity of $\omega$ in both arguments:
		\begin{align*}
			0 &= \omega(\mathbf u, \mathbf u) + \omega(\mathbf u, \mathbf v) + \omega(\mathbf v, \mathbf u) + \omega( \mathbf v , \mathbf v) 
			\\
			&= \omega(\mathbf u, \mathbf v) + \omega( \mathbf v, \mathbf u) \\
			&\Rightarrow \omega(\mathbf u, \mathbf v) = - \omega(\mathbf v, \mathbf u)
		\end{align*}
	\end{proof}
	
	2-forms are bilinear operators that act on pairs of vectors that represent coordinate changes $(\mathbf u, \mathbf v)$. They associate a flux to a given plane defined by such coordinate changes. 
	In $\mathbb{R}^3$, the space of 1-forms is spanned by $dx, dy, dz$. We can also know the flux through any plane if we knew the flux on the $xy, yz$, and $zx$ planes, so our basis for our set of 2-forms should also be three dimensional. It is spanned by three elements that we will write as
	\begin{equation*}
		dx \wedge dy, ~ dy \wedge dz, ~ dx \wedge dz
	\end{equation*}
	The first element represents a flux of magnitude $|dx ~ dy|$ through the $xy$ plane an no flux through the other two. On the other hand $dy \wedge dx$ would represent a flux in the OTHER direction so that $dx \wedge dy = - dy \wedge dx$. We have invented something called the \textbf{wedge product}\index{Wedge Product}.
	\begin{align*}
		\text{1-form in $\mathbb R^3$} &= \omega_x dx + \omega_y dy + \omega_z dz\\
		\text{2-form in $\mathbb R^3$} 
		&=
		\omega_{xy} (dx \wedge dy) + \omega_{yz} (dy \wedge dz) + \omega_{xz} (dx \wedge dz) 
	\end{align*}
	
	
	In multivariable calculus, we would define planes, together with orientations, by specifying a normal vector to those planes. In a general dimension, we aren't able to associate a normal vector to a plane, so we should talk about the plane \emph{itself}. For this reason we define the \textbf{wedge product}. 
	
	\begin{defn}[The Wedge Product]\label{def:wedge}
		The product $\wedge$ on a real vector space $V$ defined so as to satisfy the following properties
		\begin{itemize}
			\item $ \forall \mathbf a,\mathbf b, \mathbf c \in V, ~ (a \mathbf a + b \mathbf b) \wedge \mathbf c$ = $ a (\mathbf a \wedge \mathbf c) + b (\mathbf b \wedge \mathbf c)$
			\item $ \forall \mathbf a,\mathbf b, \mathbf c \in V, ~ \mathbf c \wedge (a \mathbf a + b \mathbf b) = a \mathbf c \wedge \mathbf b + b \mathbf c \wedge \mathbf a$
			\item $\forall \mathbf a\in V, ~ \mathbf a \wedge \mathbf a = 0$
		\end{itemize}
	\end{defn}
	As before, this last condition, together with bi-linearity, implies anti-symmetry of $\wedge$.
	
	In our case, when we have a basis $dq^i$ for our cotangent space of 1-forms\index{Manifold!Tangent Space}\index{Tangent Space}, the basis for our space of 2-forms can be written as $dq^i \wedge dq^j$ for $i<j$. The 2-form $dq^3 \wedge dq^4$, for example, represents a flux of magnitude $|dq^3 dq^4|$ out of the $q^3q^4$ plane and no flux out of $q^iq^j$ planes for any other $i,j$. On the other hand $dq^4 \wedge dq^3$ would be the same flux in the opposite direction (and again, no flux out of any of the other $q^i q^j$ planes).
	
	It may be frustrating to see this new product without any previous background. Let's do an example of wedging two 1-forms in 3D: $\alpha$ and $\beta$, and see what happens.
	\begin{align*}
		\alpha \wedge \beta &=(\alpha_x dx + \alpha_y dy + \alpha_z dz) \wedge (\beta_x dx + \beta_y dy + \beta_z dz) \\
		\\
		& =  ~~~ \alpha_x \beta_x (dx \wedge dx) + \alpha_x \beta_y (dx \wedge dy) + \alpha _x \beta_z (dx \wedge dz) \\
		 & ~~~ + \alpha_y \beta_x (dy \wedge dx) + \alpha_y \beta_y (dy \wedge dy) + \alpha_y \beta_z (dy \wedge dz) \\
		 & ~~~ + \alpha_z \beta_x (dy \wedge dx) + \alpha_z \beta_y (dy \wedge dy) + \alpha_z \beta_z (dy \wedge dz)\\
		 \\
		&=(\alpha_x \beta_y - \alpha_y \beta_x)(dx \wedge dy)+ (\alpha_y \beta_z - \alpha_z \beta_y)(dy \wedge dz)\\
		&  ~~ +(\alpha_z \beta_x - \alpha_x \beta_z)(dz \wedge dx) 
	\end{align*}
	
	\todochange{Very bad equation formatting here, Aaron probs knows how to fix}
	but if we were back in multivariable calculus world, not caring about vectors and forms and writing everything in terms of $\hat{\mathbf{i}}, \hat{\mathbf{j}}, \hat{\mathbf{k}}$, and we identified 
	\begin{align*}
		&\hat{\mathbf{i}} \leftrightarrow dx, \hat{\mathbf{j}} \leftrightarrow dy, \hat{\mathbf{k}} \leftrightarrow dz 
	\end{align*}
	as well as
	\begin{align*}
		&\hat{\mathbf{i}} \leftrightarrow dy \wedge dz, \hat{\mathbf{j}} \leftrightarrow dz \wedge dx, \hat{\mathbf{k}} \leftrightarrow dx \wedge dy
	\end{align*}
	then the wedge product becomes \emph{exactly} the cross product. Why wedge then, when we already know the cross product? Because the cross product, going from vectors to vectors, only works in three dimensions. The wedge product taking us from 1-forms to 2-forms, is universally valid.
	
	Moreover, now we can go beyond just 2-forms and form higher wedges, $k$-forms\index{Differential Form!$k$-Form}. The wedge of two forms $\alpha \wedge \beta$ will always be linear in both arguments and antisymmetric. For example, we can make 3-forms. In 3-D, the space of 3-forms is one dimensional spanned by $dx \wedge dy \wedge dz$. Any other wedge of these differentials will either be the same, or a negative of this one. It corresponds to the one true infinitesimal 3D volume. What about the sign? It is the distinction between flow ``into'' the volume.
	
	\todofig{GRAPHIC of 2-form wedge a 1-form giving ``inward'' orientation, and then wedge the negative of that 1-form to give the ``outward'' orientation}
	
	This is more general, in $n$ dimensions the space of $n$-forms is one dimensional, spanned by the form $\bigwedge_{i=1}^n dq^i$. For some terminology:
	\begin{defn}[\textbf{Exterior Power}\index{Exterior!Power}] For a given vector space $V$, the vector space of of $k$ forms spanned by wedging elements of $V$ with themselves $k$ times is called the $k$th exterior power of $V$, and is denoted by $\Lambda^kV$.
	\end{defn}
	Our $V$ in this case is the cotangent space at a point $T^*_p M$: the space of 1-forms and $V = \Lambda^1 V$ always. The space of zero forms $\Lambda^0 (T^*_p M)$ at a point is the set of possible function values so is just $\mathbb R$ in our case, since we are working over the reals. The space of $k$ forms at $p$ is the $k$th exterior power of the cotangent space at $p$: $\Lambda^k T^*_p M$. If we consider all $k$-forms at $p$, then we get the \emph{exterior algebra} of the cotangent space at $p$. 
	\begin{defn}[\textbf{Exterior Algebra}\index{Exterior!Algebra}]
		The vector space of \emph{all} $k$-forms is called the exterior algebra of $V$ and is denoted $\Lambda V$.
	\end{defn}
	What about the tangent space of vectors? What does the exterior algebra mean there? If $dq^1 \wedge dq^2$ is the form that associates an oriented flux to the $q^1 q^2$ plane, then $\partial_1 \wedge \partial_2$ is the oriented plane \emph{itself}.
	
	In this way 
	\begin{equation*}
		(\text{Flux} ~ dq^1 \wedge dq^2)(\text{Coordinate Area} ~ \partial_1 \wedge \partial_2) = (\text{Flux}) (\text{Coordinate Area}).
	\end{equation*} 
	This is the invariant \emph{total flux} coming out from varying $dq^1$ and $dq^2$ together to sweep out a coordinate area. In general, $k$-wedges of vectors $v^i \partial_i$ represent the oriented $k$-volumes \emph{themselves}, on which $k$-forms act. In Einstein's convention, you can show that in for a general 2-forms and 2-vectors whose coordinates are doubly covariant and contravariant, respectively, we get the invariant value:
	\begin{align*}
		(\omega_{ab} dx^a \wedge dx^b) (v^{cd} \partial_c \wedge \partial_d) &= \omega_{ab} v^{cd} (\delta^{a}_c \delta^{b}_d - \delta^{a}_d \delta^{b}_c)\\
		& = \omega_{ij}v^{ij} - \omega_{ij}v^{ji}.
	\end{align*}
	\todoex{Exercise to generalize $n$ forms acting on $n$ vectors, and also to check that it makes sense in 3D when our wedges are cross products and areas are normal vectors}
	
	You may have a question, concerning the fact that for 1-forms, we expect them to act on vectors representing infinitesimal displacements. Therefore, $k$-forms should act on something as well! Do they act on wedges of vectors, representing infinitesimal $k$-volumes? Yes, and in addition to this, we can act on a $k$ form by a vector (or you can say act on a vector by a $k$ form, no difference here) in the form of an \textbf{interior product}\index{Interior Product}. 
	
	\todoadd{Interior Product}
	
	As a last note, philosophically, \emph{where does this antisymmetry come from?} We've already seen it in the cross product, and now we have it in this wedge. Geometrically, what is happening? That the wedge product of a form with itself is zero is easy to understand: you cannot geometrically extend an object to higher dimensions without introducing new directions. Antisymmetry, on the other hand, is less obvious.
	
	 When we extend a geometric $k$-volume to a $k+1$-volume, there is a notion of orientation. Going from the line to the plane, we need to know ``which direction is out for flux?" Similarly, for the plane to the volume, we have basically the same question ``which direction is in/out for flux?'', and the antisymmetry of the wedge reflects that orientation will always exist for higher $k$ volumes.
	
	% section the_exterior_algebra_and_the_wedge (end)
	
	\section{Stokes' Theorem} % (fold)
	\label{sec:stokes_theorem}
	
	% section stokes_theorem (end)
	
	To go from a 0-form $f$ to a 1-form $\omega$, we applied the exterior derivative operator, which could just be written as:
	\begin{equation}
		\mathrm df = dq^i \frac{\partial f}{\partial q^i}.
	\end{equation}
	Going further, perhaps we could write the exterior derivative operator explicitly asa the invariant: 
	\begin{equation}\label{eq:exterior_derivative}
		\mathrm d = dq^i \frac{\partial}{\partial q^i}.
	\end{equation}
	We can view this $\mathrm d$ operator as a 1-form whose coefficients on each $dq^i$, rather than being numbers, are derivative operators $\partial/\partial q^i$ with respect to the corresponding coordinates. 
	
	Now for a 1-form $\omega = \omega_i dq^i$, we want the exterior derivative to take us to a 2-form. If this $\mathrm d$ operator can be thought of as a 1-form, the obvious way to go to a form one step higher is by wedging, meaning that we would define:
	\begin{equation}\label{eq:exterior_derivative2}
		\mathrm d \omega := (dq^i \frac{\partial}{\partial q^i}) \wedge \omega= dq^i \wedge \frac{\partial \omega}{\partial q^i}.
	\end{equation}
	Note that the only reason we did this is because of what the \emph{algebra} seemed to tell us to do, independent of any geometric intuition beforehand. This is powerful, but is this right? Is this the derivative operator that will generalize the gradient, divergence, curl, and \emph{everything else}?
	
	Let us first check that for a 1-form $\omega$ $\mathrm d \omega$ gives us Stokes' theorem, as we want:
	
	\begin{theorem}[Stokes' Theorem for 1-forms]
		If $\omega$ is a 1-form, then with $\partial$ defined as the boundary operator of a manifold and $\mathrm d$ defined as in Equation~\eqref{eq:exterior_derivative2}, we have
		\begin{equation*} 
			\int_\Omega \mathrm d \omega = \int_{\partial \Omega} \omega.
		\end{equation*}
	\end{theorem}
	\begin{proof}
		Take $\Omega$, and as in all of our proofs in the section Section~\ref{sec:the_derivative_and_the_boundary}, let us cut $\Omega$ into a mesh of infinitesimal parallelograms. If we integrate $\omega$ over the boundary $\partial \Omega$, this is the same as integrating $\omega$ over every single individual parallelogram on the interior, as \emph{BECAUSE OF ORIENTATION}, the integrals over the boundaries of these parallelograms will cancel between neighbors, leaving us with only the boundary, as always.
		
		It remains to show that for an arbitrary small parallelogram, Stokes' theorem holds. This parallelogram is obtained by varying $q^i$ along two vectors $u^i \partial_i$ and $v^i \partial_i$. After a suitable linear transformation of coordinates, we can assume WLOG that this parallelogram is obtained by changing $q^1$ by some fixed small amount $dq^1$ and $q^2$ by $dq^2$. Let's integrate $\omega$ on the boundary:
		
		Because the parallelogram is small, we can approximate these integrals as:
		\begin{equation*}
			(\partial_1 \omega_2 - \partial_2 \omega_1) dq^1 dq^2
		\end{equation*}
		On the other hand the exterior derivative is:
		\begin{align*}
			\mathrm d \omega 
			&= dq^i \wedge (\partial_i \omega) \\
			&= (dq^1 \wedge dq^2) \partial_1 \omega_2 + (dq^2 \wedge dq^1)  \partial_2 \omega_1 + \text{other} \\
			&= (\partial_1 \omega_2 - \partial_2 \omega_1) dq^1 \wedge dq^2 + \text{other}
		\end{align*}
		where the other terms involve wedges that aren't of $q^1,q^2$ and will vanish along integration of this specific parallelogram. Since integrating $(\partial_1 \omega_2 - \partial_2 \omega_1) dq^1 \wedge dq^2$ gives exactly $(\partial_1 \omega_2 - \partial_2 \omega_1) dq^1 dq^2$, we have proven it for this parallelogram, and by linearity and coordinate change for \emph{any} paralellogram. Because adding these parallelograms together forms the bulk of $\Omega$ and they cancel when integrated on neighboring boundaries (if one boundary is associated with $dq^i$, the neighboring one is associated with $-dq^i$), this gives the desired result. 
	\end{proof}
	\todochange{Fix Stokes' Theorem Proof}
	\textbf{WE DONT NEED ANY OTHER COORDINATES}
	
	
	Note that if $q^i = (x,y,z)$ then this is exactly Stokes' theorem in 3-D for the curl! More than this, it generalizes Stokes' theorem in $\mathbb{R}^3$: for any 2-D surface, the circulation of $\omega$ over the boundary is exactly the sum total of the curl $\mathrm d \omega$ over the interior.
	
	From this let us prove what we set out to prove in the most general case:
	{
	\renewcommand{\thetheorem}{\ref{thm:GeneralStokes}}
	\begin{theorem}[General Stokes' Theorem]
		With $\partial$ defined as the boundary operator of a manifold and $\mathrm d$ defined as in Equation~\eqref{eq:exterior_derivative2}, we have for a general differential $k$-form $\omega$ that
		\begin{equation*} 
			\int_\Omega \mathrm d \omega = \int_{\partial \Omega} \omega.
		\end{equation*}
	\end{theorem}
	\addtocounter{theorem}{-1}
	}
	First, a lemma:
	
	\begin{lemma}[Restriction of a Form]
		If $\omega$ is a $k$ form of $n$ variables that is being integrated over some $k$-dimensional manifold associated with changing only the first $k$ variables, then for the integration, we can work with the restricted $\omega_{\text{res}}$ associated with setting the last $n-k$ $dq^i$ equal to zero, and eliminating any wedge terms holding those $dq^i$.
	\end{lemma}
	\begin{proof}
		This follows from
	\end{proof}
	\textbf{IM NOT SURE WE NEED THIS NOW}
	
	Now to prove the General Stokes' Theorem:
	\begin{proof}
		For a given manifold $\Omega$, divide it's bulk into $k$-volumes that are generalizations of parallelograms to $k$-dimensions. Just like a line's boundary has two points, a parallelogram's has 4 lines, a parallelepiped's has $6$ parallelograms, a $k$-volume's boundary is $2k$ $k-1$ volumes. Again, we can set up local coordinates so that this $k$ volume has each $k-1$ obtained by holding some $q^i$ constant and letting the others vary. For each $q^i$ there are exactly two opposing $k-1$ volumes obtained by holding that coordinate constant, and they have opposite orientation.
		
		Now let us perform the integration of $\omega$ over the $k-1$ boundaries. In terms of these coordinates, $\omega$ can be written as a linear combination of wedges of $k-1$ of the $dq^i$, meaning that each such wedge misses exactly one $dq^i$.
	\end{proof}
	\textbf{THE PROBLEM WITH THE ABOVE IS THERE COULD BE $n$ $q^i$ and }
	\begin{prop}
		$\mathrm d^2 = 0$
	\end{prop}
	\begin{proof}
		For a general form $\omega$, consider
		\begin{align*}
			\mathrm d (\mathrm d\omega) &= (dq^j \partial_j)\wedge (dq^i \partial_i \omega) \\
					& = dq^j \wedge dq^i (\partial_j \partial_i \omega)
		\end{align*}
		This is a summation over $i$ and $j$ running from $1$ to $n$. Now pick any specific term in the sum with \emph{specific} indices $a, b$. This corresponds to a term:
		\begin{equation*}
			dq^a \wedge dq^b (\partial_{a} \partial_{b} \omega)
		\end{equation*}
		in the sum. We can assume $a \neq b$ as otherwise that'd mean $dq^a \wedge dq^b = 0$.
		But that means we will have another distinct term with those indices reversed $(b,a)$ equal to
		\begin{equation*}
			dq^b \wedge dq^a (\partial_{b} \partial_{a} \omega)
		\end{equation*}
		Since partials commute but the wedge products anti-commute, this $(b,a)$ term is equal to the \emph{negative} of that first $(a,b)$ term, meaning they will cancel. The whole double-sum will then become a sum of cancelling terms, giving zero.
	\end{proof}
	\begin{cor}
		$\partial^2 = 0$: The boundary of a boundary is nothing. 
	\end{cor}
	\begin{proof}
		For any $k$-form $\omega$, assume we are integrating a form $\mathrm d \omega$ on a $(k+1)$-boundary. By Stokes' Theorem (twice):
		\begin{equation*}
			\int_{\partial^2 \Omega} \omega = \int_{\partial \Omega} \mathrm d \omega = \int_{\Omega} \mathrm d^2 \omega = \int_\Omega 0 = 0.
		\end{equation*}
		Since $\omega$ was any arbitrary form, we must have $\partial^2 \Omega = 0$.
	\end{proof}
	This is an amazing geometric fact that we have gotten, via Stokes' theorem, from the \emph{purely algebraically derived} fact that $\mathrm d^2 = 0$. The duality between forms and the manifolds we integrate them over is a gorgeous duality between algebra and geometry that extends very deeply and profoundly (c.f. Hodge Theory \textbf{INSERT TEXTS HERE}).
	
	We have already seen that differential forms that are the exterior derivatives $\mathrm d \omega$ of some other form $\omega$ are called exact. We know exact 1-forms correspond to conservative vector fields. \textbf{It will be an exercise to show} exact 2-forms in $\mathbb R^3$ correspond to solenoidal vector fields.\todoex{Exact 2-forms on $\mathbb R^3$ are correspond to solenoidal vector fields} On the other hand, forms $\omega$ that have $\mathrm d \omega = 0$ are called \textbf{closed}\index{Differential Form!Closed}. Why this language? It is taken for the corresponding geometric language for the boundary operator. If a region has no boundary, it is called closed, so if a form has zero exterior derivative, it will also be called closed. Clearly exact forms are closed, since $\mathrm d^2 = 0$, but are \emph{all} the closed forms exact? In $\mathbf{R}^3$, the answer is yes, but consider this:
	
	\begin{example}
		$d\theta$ is a closed form defined on the punctured plane that is not exact. 
	\end{example}
	
	We know $\mathrm d (d\theta) = 0$, so it is indeed closed. Although locally, a function $\theta$ (that this form represents the change of) can be defined just by calculating the angle from the $x$ axis, if you go around counterclockwise in a circle containing the origin, then $\theta$ continuously increases. At the end of the revolution, even though you are at the same point, $\theta$ has increased by $2\pi$. So although $d\theta$ makes sense locally as a differential form everywhere in the plane minus the origin, we cannot define a global smooth function representing $\theta$ without a discontinuity. The existence of a closed form that is not exact happens because the manifold on which $d\theta$ is defined is \emph{not} $\mathbb R^2$, but is instead defined on $\mathbb{R}^2$ without the origin (where $d\theta$ would not be well-defined). This change in global geometric structure gives rise to these interesting closed, inexact forms. 
	
	The study of the closed forms that are not exact on a manifold is called the \textbf{De-Rham cohomology}\index{De-Rham Cohomology} of a manifold. \\
	
	\todofig{Punctured Plane for De-Rham Cohomology}
	
	
	\section{Distance, a Metric}	
	
	\index{Metric Tensor|(}\index{Tensor!Metric|(}\index{Manifold!Metric|(}
	
	\emph{Why have we been making such a difference between vectors and forms?! In multivariable calculus I could have done all the 3-D calculation working only with my regular vector fields using $\hat{\mathbf{i}}$ and the rest. Why can't I just turn these forms into vector fields?}.\\
	
	These are the types of questions you might ask, were this book written in dialogue. It's a fair point. For some reason, in multivariable calculus, there seemed to be no problem just using the language of vector fields for gradients, curls, and integration. What are all these forms doing here? The reason, as we've said before, is because we've no longer assumed that we were working in an ``orthogonal reference frame''. In fact, up until now we have been working in vector spaces of tangent vectors and forms that had \emph{no notion of distance whatsoever}. Accordingly, we made no assumptions that our manifolds themselves had any notion of distance and length either. Even though we talked about wedge products as associated with area, it was not true area of euclidean space but merely a (contravariant) number obtained by multiplying the changes of coordinates $dq^i$ together. 
	
	
	But we know that in the physical world there \emph{is} a notion of length. It is not just affine space. There is in fact a notion of \emph{being perpendicular}. If we had this notion, something powerful happens. Every 1-form, as we know from the previous chapter, can be viewed of as local function behavior, and therefore visualized in terms of level curves of that function. If we had a notion of being perpendicular, then we have a unique direction \emph{perpendicular to the level curves of the first order behavior} along which $\omega$ increases. We can then associate a specific, invariant vector $\mathbf v = v^i \partial_i$ that is perpendicular to the level curves of $\omega$, and along which omega increases by exactly $|\mathbf v|^2$. 
	
	Conversely, if we have a notion of being perpendicular, then to a specific vector, we can pick a unique 1-form $\omega = \omega_i dx^i$ whose level curves are perpendicular to the direction of $\mathbf v$ and such that the increase of $\omega$ along $\mathbf v$ is $|\mathbf v|^2$.
	
	What we want is to associate an invariant \emph{physical length element} to a small local change of coordinates $v^i$ around any point. In 3-dimensional Euclidean space $\mathbf E^3$, with an orthogonal coordinate system $x,y,z$ we know how to obtain a physical change in length from the changes of the coordinates
	\begin{equation}
		ds^2 = dx^2 + dy^2 + dz^2
	\end{equation}
		%
	% Since such a change is a vector, to get something invariant the first guess might be to have a 1-form $\omega^*$ that takes in a direction $\mathbf v$ and associates the invariant magnitude  $\omega^* \mathbf v$ to it. But then if $\omega^* \mathbf v$ were positive then $\omega^* (-\mathbf v)$ would be negative, and we want distance to be a positive quantity.
	
	
	
	We are let by Pythogoras: In euclidean space $\mathbb E^n$, for an orthogonal frame, the length associated with a change of coordinates is 
	\begin{equation}
		|x| = \sqrt{\sum x_i^2} \Rightarrow |x|^2 = x_i x_i
	\end{equation}
	where we've used lower indices for coordinates because in an orthogonal frame there's no big deal between covariance and contravariance. More generally, in an orthogonal frame, $\mathbf E^n$ has a dot product producing a scalar $\mathbf x \cdot \mathbf y = x_i y_i$. 
	In general coordinates, though, we've already seen $v^i v^i$ is not invariant just by noting the two upper indices, and also by analyzing how it trasforms directly in Section~\ref{sec:the_notion_of_length_on_vector_spaces}. 
	
	We want to define an \emph{invariant} \textbf{inner product}\index{Inner Product} $\left< \mathbf v | \mathbf w \right>$ on our tangent space, bilinear in both arguments, just like the one in $\mathbf E^n$. Then we will write:
	\begin{equation}
		\begin{aligned}
			\left< \mathbf v | \mathbf w \right> &= \left< v^i \partial_i | w^j \partial_j \right> \\
			&= v^i w^i \left< \partial_i | \partial_j \right>
		\end{aligned}
	\end{equation}
	This object, $\left< \partial_i | \partial_j \right>$, is the inner product between just the basis vectors themselves. Its double co-variance cancels out the double contra-variance of the coordinates themselves to give an invariant quantity to this inner product. In an orthonormal frame, since the vectors are orthonormal we would have $\left< \partial_i | \partial_j \right> = \delta_{ij}$ so that the inner product $\delta_{ij} v^i w^j$ is exactly the same as the dot product we are used to. 
	
	Now we can get the length of a vector:
	\begin{equation}
		|\mathbf v|^2 = \left< \mathbf v | \mathbf v \right> = v^i v^j \left< \partial_i | \partial_j \right> 
	\end{equation}
	in general, for an \emph{arbitrary} change of coordinates $dq^i$, we can finally associate a \emph{physical length} in space by:
	\begin{equation}
		ds^2 = dq^i dq^j  \left< \partial_i | \partial_j \right> 
	\end{equation}
	Because of the significance of this object $\left< \partial_i | \partial_j \right>$, we will them by $g_{ij}$. All together, this invariant inner product that takes in two coordinate changes:
	\begin{equation}
		ds^2 = g_{ij} {(dq^1)}^i {(dq^2)}^j 
	\end{equation}
	will be called the \textbf{metric} on our tangent space. A metric turns the tangent space into an \textbf{inner product space}\index{Inner Product Space}
	
	At a very far glance, you might ask if this thing is some sort of 2-form, but it is not an anti-symmetric two-form associated with integrating over areas. There are no wedges between the differentials, and in fact since the dot product (which is the inner product in orthonormal basis) is symmetric, $\mathbf u \cdot \mathbf v = \mathbf v \cdot \mathbf u$, this holds true here, so in fact $g_{ij} = g_{ji}$ is symmetric as a matrix. This is a different type of product of 1-forms, that will be discussed in the next section. For now, it is the tool by which we can associate a length with a coordinate change $dq^i$, and how we can take scalar products of two vectors together. 
	
	To avoid abstraction without geometric concreteness, let's do a simple but nontrivial example. In cartesian coordinates on Euclidean space, $g_{ij} = \delta_{ij}$, but in polar coordinates, we now have $\partial_r, \partial_\theta$. At each point, these vectors are orthogonal, and $\partial_r$ corresponds to a unit increase in $r$, meaning that it has magnitude $1$. On the other hand, $\partial_\theta$ corresponds to a unit increase in theta. Increasing $\theta$ by $d\theta$ on a circle of radius $r$ gives an infinitesimal length of $r d\theta$. The total line element then is
	\begin{equation}
		ds^2 = dr^2 + r^2 d\theta^2 \Rightarrow g_{ij} = 
		\begin{pmatrix}
			1 & 0 \\
			0 & r^2 
		\end{pmatrix}.
	\end{equation}
	At every point, this gives the \emph{physical} length associated with any local infinitesimal change of coordinates $(dr, d\theta) = v^i$ that form the components of a vector.
	
	Immediately, being able to associate an invariant length to a tangent vector on our space allows us to be able to define the length of any curve on our manifold. For a curve $\gamma(t) \in M$ parameterized by $t$. For a coordinate patch, we get $q^i = \varphi^i(\gamma)$. On each tangent space we have the tangent vector to $\gamma$:
	\begin{equation}
		\mathbf v_{\gamma,t} = \frac{d}{dt} \left[ \varphi^i(\gamma(t)) \right] \frac{\partial}{\partial q^i}
	\end{equation}
	for which we can now calculate an associated length: 
	\begin{equation}
		|\mathbf v_{\gamma,t}|^2 = g_{ij} ~ \dot \varphi^i(\gamma) \dot \varphi^j(\gamma).
	\end{equation}
	\begin{prop}[The Metric Defines the Length of Curves]
		The length of a curve $\gamma$ parameterized by $t$ on a manifold $M$ with metric $g$ is
		\begin{equation}
			\int_{t_i}^{t_f} \sqrt{g_{ij} ~ \dot \varphi^i(\gamma(t)) \dot \varphi^j(\gamma(t))} ~ dt
		\end{equation}
		and can be more clearly seen as
		\begin{equation}
			\int_{\gamma} \sqrt{g_{ij} \frac{dq^i}{dt} \frac{dq^j}{dt}} dt = \int_{\gamma} \sqrt{g_{ij} dq^i dq^j}
		\end{equation}
		where $dq^i$ is the infinitesimal coordinate change along the curve at that point.
	\end{prop}
	We said that the integrals over curves should be one-forms, but the integrand doesn't look like it is. Note, however, that $\sqrt{g_{ij} dq^i dq^j}$ is exactly the ``square root of a 1-form product'', which could be viewed as the ``norm'' of a 1-form in some sense. This object is not a 1-form, it is a \textbf{measure}\index{Measure}. The difference between the two is that while a 1-form will give a negative of its original value if integrated in the opposite direction of the curve, a measure will \emph{always} associate a positive value to any region it is integrated over. These are centrally important to calculating positive geometric invariants like length, volume, etc. 
	
	In the polar example, this automatically gives us the equation for arc-length:
	\begin{equation}
		\int_{\gamma} \sqrt{dr^2 + r^2 d\theta^2} 
		= \int_{t_i}^{t_f} \sqrt{\left( \frac{dr}{dt}\right)^2+ r^2 \left( \frac{d\theta}{dt} \right)^2 } dt
	\end{equation}
	and indeed, once $g$ is known, we can find the equation for arc length in \emph{any} coordinate system. \todoex{Arc Length in worthwhile coordinate systems}
	
	
	Now note, the metric tensor was not something that we had before on a general manifold $M$. It is added information. A manifold doesn't need a metric or any notion of distance to be well-defined. After all, its definition is just topological. When $M$ is endowed with with a metric $g$ at every tangent space $T_p M$, we say it is a \textbf{Riemannian manifold}\index{Manifold!Riemannian}. 
	\begin{defn}[Riemannian Manifold]
		A Riemannian manifold is a smooth manifold $M$ with a positive definite bilinear form $g(\mathbf u, \mathbf v)$ on each tangent space. This form is called a \emph{\textbf{metric}} or a \emph{\textbf{Riemannian metric}}.
		
		$g$ is positive definite in the sense that $g(\mathbf u, \mathbf u) \geq 0$ always, and equals zero iff $\mathbf u = 0$. This defines our positive-definite inner product. 
	\end{defn}
	This condition ensures that every nonzero vector has positive length. Now, endowed with a metric, we can measure lengths of curves, and much more. 
	\begin{prop}[The Metric Defines Angles]
		If two curves $\gamma_1, \gamma_2$ intersect at a point $p$ on a Riemannian manifold, the angle between them can be calculated in terms of their respective tangent vectors, $\mathbf u, \mathbf v$ by
		\begin{equation}
			\cos(\theta)=\frac{g_{ij} u^i v^i}{|u| |v|}
		\end{equation}
	\end{prop}
	This is the obvious analogue of the Euclidean definition, now we've just extended the dot product of vectors to general coordinates. This is how we will \emph{define} the notion of angle in a manifold. Note under general linear transformations, angles between two vectors can appear sheared and stretched. It is the \emph{metric} that holds us steady and tells us ``this frame is orthogonal, this one is not, it's distorted and so the dot product here should be $g_{ij}$ instead to recover the right notion of angle''.
	
	It is also worth noting a simple fact
	\begin{lemma}
		The metric tensor $g_{ij}$ is invertible as a matrix.
	\end{lemma}
	\begin{proof}
		Assume $g$ isn't invertible. Then there is a nontrivial null space for $g$. Then there exists a nonzero vector $\mathbf u$ with $g \mathbf u = 0$, but that means $g(\mathbf u, \mathbf u) = 0$ without $u=0$, contradicting the positive definiteness of the metric.
	\end{proof}
	Now since a metric gives us an inner product, to each vector $\mathbf v$ is associated a corresponding $\left< \mathbf v | - \right>$ operation that wants to take in a vector. But this is \emph{exactly} what a 1-form is. We then have:
	\begin{prop}[Metric Defines an Isomorphism Between Vectors and Covectors]
		A given metric $g$ associates to each vector $\mathbf u$ a corresponding 1-form $\omega_\mathbf{u}$ in the sense that for all vectors, $\mathbf v$, $\omega_\mathbf u$ acts according to inner product
		\begin{equation}
			\omega_\mathbf{u} \mathbf v =  \left< \mathbf u | \mathbf v \right>, \omega = g \mathbf u
		\end{equation}
		In coordinate notation, we have that for $\mathbf u = u^i \partial_i$ give rise to 
		\begin{equation}
			(\omega_\mathbf u)_i dx^i = (g_{ij} dx^i dx^j)(u^j \partial_j) = (g_{ij} u^j) dx^i
		\end{equation}
	\end{prop}
	so the tuple of coefficients for the form corresponding is the matrix $g_{ij}$ multiplying the tuple $u^j$ for the vector (note we could have had it multiplying $u^i$ since $g_{ij}$ is symmetric so we can switch $i$ and $j$ above). 
	
	Of course, in an orthonormal frame, where $g_{ij}=\delta_{ij}$ is just the identity, this means that the form corresponding to $\partial_i$ is just $dx^i$ and things are simple. It is for this reason that forms and vectors have been interchangeable in our orthonormal frames with trivial metric. In any interesting coordinate system, however, this breaks immediately. 
	
	Note, then, that if $g \mathbf v = \omega$ gives a form associated to a vector, and this is just a matrix-vector equation then we would expect that 
	\begin{equation}
		g^{-1} \omega = \mathbf v
	\end{equation}
	gives a \emph{vector} associated to every 1-form. In index notation
	\begin{equation}
		(g^{-1})^{ij} \omega_j = v^i
	\end{equation}
	For this reason, $g$ and its inverse\footnote{Note that often $(g^{-1})^{ij}$ is often just written as $g^{ij}$ in the literature simply because upper indices automatically mean that we have inverted the covariant $g_{ij}$, and so no confusion ensues.} are said to ``raise and lower indices''. This inverse $g$ then gives us an inner product on the cotangent space:
	\begin{equation}
		\left< \alpha | \beta \right> = g^{ij} \alpha_i \beta_j
	\end{equation} 
	its easy (almost immediate) to check that the inner product of 1-forms gives the same number as the inner product of their corresponding vectors.
	
	Mathematicians, who often write vectors and forms just as $\mathbf v$ and $\omega$, without ever resorting to index notation have invented a very creative notation for the way that $g$ associates vectors to their 1-forms.
	\begin{prop}[Musical Isomorphisms]
		At each point $p$ on a Riemannian manifold $M$, the metric $g$ induces an isomorphism between the tangent space $T_pM$ and the cotangent space $T^*_pM$:
		\begin{equation}
			\flat: T_pM \rightarrow T^*_pM
		\end{equation}
		so that for each vector $\mathbf v \in T_pM$, $\omega = \mathbf v^\flat$ is the associated 1-form. The inverse of the metric gives the opposite direction
		\begin{equation}
			\sharp: T^*_pM \rightarrow T_pM
		\end{equation}
		so that $\mathbf v = \omega^\sharp$ is the associated vector to the 1-form $\omega$.
	\end{prop}
	\todofig{Diagram of Musical Isomorphisms}

	As mentioned in the beginning of this section, since the metric gives the notion of ``orthogonality'' to the tangent space, geometrically the form corresponding to a vector $\mathbf v$ is the local linear behavior of a function with level curves perpendicular to $\mathbf v$, and increasing in the direction of $\mathbf v$ so that $\omega_{\mathbf v}(\mathbf v) = |\mathbf v|^2$. $\mathbf v$ would then be exactly the \emph{gradient vector} to this local behavior, pointing in the direction of greatest ascent. 
	
		This means we can define the gradient\index{Gradient Operator} vector to a function, finally.
		\begin{defn}[The Gradient Vector]
			For a function $f$, we define $\nabla f$ as the vector field 
			\begin{equation}
				\nabla f = (\mathrm df)^\sharp.
			\end{equation}
			In a coordinate system $q^i$ this is:
			\begin{equation}
				(\nabla f)^k \partial_k = \left(g^{ij} \frac{\partial f}{\partial q^j} \right) \partial_i
			\end{equation}
		\end{defn}
		That is, we take the form corresponding to the first-order behavior of $f$ and because of the metric we now have the vector field corresponding to the \emph{direction of greatest ascent}.
		Comparing components, we have that 
		\begin{equation}
			\nabla^i f = g^{ij} (\partial_j f) \Rightarrow \nabla^i = g^{ij} \partial_j
		\end{equation}
		So the gradient operator is the ``contravariant" version of the partial derivative. Often it is just written as the ``raised partial" $\partial^i$ to avoid confusion with what $\nabla$ will denote in later sections. The powerful thing about knowing the metric is that it allows us to define the gradient operator in \emph{any} coordinate system. In the polar example, this is
		\begin{align*}
			g^{ij} = 
			\begin{pmatrix}
				1 & 0 \\
				0 & 1/r^2 
			\end{pmatrix}
			\Rightarrow \nabla f = \frac{\partial f}{\partial r} \partial_r + \frac{1}{r^2} \frac{\partial f}{\partial \theta} \partial_\theta.
		\end{align*}
		Often, engineers want to define this in terms of the ``normalized basis vectors'' of length 1: $\hat {\mathbf r} = \partial_r$, $\hat{\boldsymbol \theta}= \partial_\theta/r$ so that 
		\begin{equation*}
			\nabla f = \frac{\partial f}{\partial r} \hat{\mathbf r} + \frac{1}{r} \frac{\partial f}{\partial \theta} \hat{\boldsymbol \theta}.
		\end{equation*}
		
	
	\index{Metric Tensor|)}\index{Tensor!Metric|)}\index{Manifold!Metric|)}

	\section[Multilinear Algebra: Views]{Multilinear Algebra: $\oplus, \otimes$ and\\ Tensors}
	\label{sec:views}
	\index{Tensor|(}
	
	\todoadd{INSERT SOME QUOTE ABOUT HOW LIKE 97\% OF MATH IS JUST TERMINOLOGY}
	
	Before moving on to completing the study of differential geometry, it is worth reflecting on the mathematical machinery that we have developed. 
	
	We began differential geometry seriously in Section~\ref{sec:vectors_reimagined} by noting that there were two first order behaviors that were dual: vectors, which represented the first order behavior of curves and gave rise to direction, and forms, which represented the first order behavior of functions, and represented an operator that took a direction and gave us an associated change.
	
	Both sets of first order behavior had the property of being vector spaces: their elements could be added together and scaled. From this we defined the tangent and cotangent spaces. If we changed our coordinate system $q^i$, on both spaces this induced a linear transformation (since nonlinear coordinate transforms are locally linear, c.f. Section~\ref{sec:nonlinear_coordinate_systems_are_locally_linear}) on the  \emph{components} of the vectors. The tangent space would have its components transform one way and the cotangent space would have the components transform the other way. As a result, these two spaces were not naturally isomorphic.
	
	With the introduction of the metric $g_{ij}$, whose components are ``doubly'' covariant, we get a linear map that lets us go from the contra-variant components $v^i$ of a vector to the covariant $\omega_i$ of a form. This also gives us an inner product. 
	
	While $v_i$ and $\omega_i$ could both be represented by a tuple, a list of numbers that changed either with or against the basis $\partial_i$ when we changed coordinates, $g_{ij}$ is represented by a \emph{matrix} of numbers. In fact this matrix is symmetric. The metric, $g$, is and object that's neither a vector nor a covector but something ``higher''.
	% OH MASTERRRRRR 
	% SHOW ME SOMETHING HIGHGHGHGHGHGHEERRRR
	
	We will work towards making intuitive and rigorous the concept of a \textbf{tensor}, which generalizes that of a vector and covector from before:
	\begin{concept}[Tensor]
		A tensor is a physical quantity that can be represented by a multidimensional array of $k$ indices. Because the numbers within the tensor depend on the coordinate system, they will change (either co-variantly or contra-variantly). Components associated with some indices can be covariant, coordinates associated with other indices can be contravariant. 
	\end{concept}
	We've already seen before: a $k$-form, the wedge product of $k$ $1$-forms is a tensor with $k$ components. For a set of indices $i_k$ (which we will call a \textbf{multi-index}\index{Multi-Index}), we associate the flux that is going through the $k$-volume $dq^{i_1} \wedge dq^{i_2} \wedge \dots \wedge dq^{i_k}$. This tensor has the property that if we flip any two neighboring $i_k$, say $i_1$ with $i_2$, then the value at that new multi-index is the negative of the value of the old, because $\wedge$ is anti-symmetric. This tensor is thus called anti-symmetric: flipping neighboring indices flips the sign.
	
	In order to appreciate this all better, however, we need to go back and better our understanding of the language of vector spaces. \\
	
	Consider an $n$-dimensional vector space, $V$ with a basis $\{ \mathbf v_i \}$ and an $m$-dimensional vector space $W$ with basis $\{ \mathbf w_i \}$. 
	\begin{defn}
		The \emph{\textbf{direct sum}}\index{Direct Sum} $V \oplus W$ is the $n+m$-dimensional vector space of ordered pairs $(\mathbf v, \mathbf w)$ with $\mathbf v \in V$, $\mathbf w \in W$, with addition defined component-wise.
	\end{defn}
	
	\todofig{GRAPHIC: DIRECT SUM, R with R, and R with $\mathbf R^2$}
	
	The direct sum is one of the easiest concepts to grasp. It is exactly what it seems, at first glance, a addition of new directions to $V$. The only possible uncertainty is ``what if the two bases overlap?''. But when taking the direct sum, $V$ and $W$ are assumed to be entirely different vector spaces from the start, and we assume they share no elements. For this reason, we can take $\mathbb{R} \oplus \mathbb{R} = \mathbb{R}^2$ even though the two vector spaces are the same.
	
	The direct sum is simply the addition of new possibilities, new directions to $V$ by $W$. It is clearly commutative $V \oplus W = W \oplus V$. We can write a direct sum of the individual elements as:
	
	\begin{equation}
		\begin{pmatrix}
			v_1 \\ \vdots \\v_n 
		\end{pmatrix}
		\oplus
		\begin{pmatrix}
			w_1 \\ \vdots \\w_m
		\end{pmatrix}
		=
		\begin{pmatrix}
			v_1 \\ \vdots \\v_n \\
			w_1 \\ \vdots \\w_m
		\end{pmatrix}
	\end{equation}
	
	An element in the direct sum $V \oplus W$ corresponds uniquely to a vector in $V$ paired with a vector in $W$. If a given vector space can be written as a direct sum of two nontrivial sub-spaces, we say it can be \textbf{decomposed}\index{Direct Sum!Decomposition}. 
	\begin{example}
		A plane in $\mathbb R^3$ can be \textbf{decomposed} as a direct sum of any two distinct lines going through it. On the other hand $\mathbb R^3$ clearly cannot be decomposed into a direct sum of that plane and a line going through that same plane. Even though the plane and the line are isomorphic to $\mathbb R^2$ and $\mathbb R$, in context, since their embedding in this 3-space is linearly dependent, their combined span is not $\mathbb{R}^3$.
		
		On the other hand, there \emph{is} a way to pick a plane and a line so that their combined span is $\mathbb R^3$ so $\mathbb R^3$ \emph{is} decomposable into $\mathbb R^2$ and $\mathbb R$. It depends on \emph{which} $\mathbb R^2$ and \emph{which} $\mathbb R$ we choose inside $\mathbb R^3$.
	\end{example}
	
	The notion of a direct sum is so broad and general that it will be seen for the remainder of this book. It is the idea that $V$ and $W$ are two entirely different worlds, and $V \oplus W$ is their combination. 
	
	A very intuitive way to understand the direct product further comes, oddly enough, from quantum mechanical language. In quantum mechanics, the various states that something can have are represented by vectors. Say something can be one of three shapes
	\begin{equation*}
		\{ \square, ~ \triangle,  ~ \bigcirc \}
	\end{equation*}
	Now in quantum mechanics we can mix states so something could look like $\frac{1}{\sqrt 2} \square + \frac{1}{\sqrt 2} \triangle$, so these form a vector space $V$ with vectors $a \square + b \triangle + c \bigcirc$. If we had another two states, say $\smiley$ and $\frownie$ that formed a space of vectors $a \smiley + b \frownie$ labelled $W$ then the direct sum $V \oplus W$ would be spanned by the basis:
	\begin{equation*}
		\{ \square, ~ \triangle,  ~ \bigcirc, ~ \smiley,  \frownie \}
	\end{equation*}
	The idea of the direct sum should resemble exactly how you learned addition as a toddler.
	
	Within \emph{this} scheme, it is easy to explain the \textbf{direct product}\index{Tensor!Tensor Product}\index{Direct Product} also known more commonly as the \textbf{tensor product}. 
	\begin{view}[3-Year-Old's Perspective]
		The tensor product of a space spanned by $\{ \square, ~ \triangle,  ~ \bigcirc \}$ with a space spanned by $\{ \smiley, \frownie \}$ gives rise to a new space, spanned by the \emph{composite pairs}
		\begin{equation*}
			\begin{Bmatrix}
			 \square \otimes \smiley, & \triangle \otimes \smiley,  & \bigcirc  \otimes  \smiley, \\
			\square \otimes \frownie, & \triangle \otimes \frownie,  & \bigcirc  \otimes \frownie ~	
			\end{Bmatrix}
		\end{equation*}
		This is a space of dimension $\dim V \times \dim W$.
	\end{view}
	The tensor product $V \otimes W$ represents all the states of a \emph{composite system} of one object that has a state in $V$ and another object that has a state in $W$. It's especially important to stress is that $V \otimes W$ is a \emph{completely different space} from either $V$ or $W$, and unlike the direct sum, neither of the original vector spaces are a part of this new space.
	
	
	So in general, for a space $V$ with basis $\mathbf v_i$ and a space $W$ with basis $\mathbf w_i$, the tensor product space is simply a ``product'' of $V$ and $W$ in the sense that it is spanned by the abstract new vectors $\mathbf v_i \otimes \mathbf w_j$. There is no more structure to the tensor product other than requiring that $\otimes$ is distributive, just like multiplication is: $\mathbf a \otimes (\mathbf b + \mathbf c) = \mathbf a \otimes \mathbf b + \mathbf a \otimes \mathbf c$, and vice versa for $(\mathbf a + \mathbf b) \otimes \mathbf c$. Also, we say $\mathbf a \otimes (c \mathbf b) = c (\mathbf a \otimes \mathbf b)$. So tensoring with $c$ times a vector gives $c$ times the tensor with that vector. This is the exact same distributivity that ordinary multiplication has $x(cy) = c (xy)$. The tensor product is no more than a new space of ``formal multiplication of vectors, satisfying distributivity''.
	
	
	\begin{defn}[Tensor Product]
		For two vector spaces $V,W$, we define $V \otimes W$ as the vector space of all products $\mathbf v \otimes \mathbf w$ satisfying the following bi-linearity conditions:
		\begin{enumerate}
			\item $(a \mathbf v) \otimes \mathbf w = a (\mathbf v \otimes \mathbf w)$ and $\mathbf v \otimes (a \mathbf w) = a (\mathbf v \otimes \mathbf w)$
			\item $(\mathbf v_1 + \mathbf v_2) \otimes \mathbf w = \mathbf v_1 \otimes \mathbf w + \mathbf v_2 \otimes \mathbf w$ and\\ $\mathbf v\otimes (\mathbf w_1 + \mathbf w_2) = \mathbf v \otimes \mathbf w_1 + \mathbf v \otimes \mathbf w_2$
		\end{enumerate}
	\end{defn}
	
	 Now we can relate pairs of vectors $\mathbf v \in V$ and $\mathbf w \in W$ to an element in the tensor product basis by writing $\mathbf v, \mathbf w$ in terms of the $\mathbf v_i$ and $\mathbf w_i$ bases and using distributivity of $\otimes$:
	\begin{equation*}
		\mathbf v \otimes \mathbf w = (v^i \mathbf v_i) \otimes (w^i \mathbf w_i) = v^i w^i \mathbf v_i \otimes \mathbf w_i
	\end{equation*}

	The second view is the easy answer given when a student asks ``what is a tensor?''. A professor may reply ``you can just think of it as a matrix''. This is also exactly the right way for computer scientist to view tensors, but their language refers to vectors as ``arrays'' and tensors, consequently, as ``double arrays''.
	\begin{view}[Computer Scientist's Perspective]
		For elements in $V$ represented as lists of $n$ numbers, and elements in $W$ represented as lists of $m$ numbers, the elements of $V \otimes W$ are represented as $n \times m$ tables $A_{ij}$ so that the entry of $A_{ij}$ is the coefficient of basis element $\mathbf v_i \otimes \mathbf w_j$.
	\end{view}
	By distributivity, for individual lists $v^i$ and $w^j$ their associated tensor product $\mathbf v \otimes \mathbf w$ has entries $v^i w^j$.
	
	The third view is similar. If we start with a vector space $V$, $V \oplus W$ is similar to saying that ``we now have more vectors'' that we can add into $V$. That means the operation `$+$' an act not just between vectors in $V$ but also between vectors in $V$ and vectors in $W$. We have ``extended the set of vectors we can add together'' to include $W$, giving the resulting larger space of $V \oplus W$.
	
	In this vein, the tensor product can be viewed as an extension of \emph{scalar multiplication}
	
	\begin{view}[Extension of Scalars]
		We view $V \otimes W$ as extending the vectors in $V$ from having \emph{scalars} as their components to having \emph{vectors} in $W$ as their components. That is, we ``extend scalars'' from the one-dimensional $\mathbb R$ to the higher space $W$.
	\end{view}
	Then a general vector $\mathbf a$ in $V \otimes W$ can be viewed as a vector in $\mathbf V$ whose components are not scalars, but are themselves vectors in $\mathbf W$. This is really the same perspective as the computer scientist's.
	
	The last view is probably the least intuitive, but is the one that every mathematician will call the ``correct'' way to define the tensor product. What it lacks in intuition, it makes up for in motivating why this tensor product operation is worth studying \emph{at all}.
	
	
	The tensor product can first be motivated by the action of a bilinear forms. We've seen the metric as an example of a (symmetric) bilinear form on the tangent space. Moreover two any two vectors we can also associate an area $A(\mathbf u, \mathbf v)$ to be the signed area of the parallelogram that they span. This is an antisymmetric bilinear form.
	
	Consider now a general bilinear form $B(\mathbf v, \mathbf w)$ acting on a vectors $\mathbf v,\mathbf w$ in a space $V$. In fact, to go to greater generality, consider $B(\mathbf v, \mathbf w)$ acting on vectors $\mathbf v \in V$ for its first argument, and vectors $\mathbf w$ in another space $W$ for the second argument. $B$ takes a vector from $V$ and a vector to $W$ and sends them to a scalar. $B$ need not be positive definite, symmetric, define a dot product, and can have a null space. It can be anything.
	
	If $\{\mathbf v_i \}$ is a basis for $V$ and $\{\mathbf w_i \}$ for $W$ then we will know how $B$ acts on any pair of vectors in $(V, W)$ iff we know $B(\mathbf v_i, \mathbf w_j)$ for every pair $(\mathbf v_i, \mathbf w_j)$. We need $\dim V \times \dim W$ dimensions of information know the action of $B$.
	In full notation:
	\begin{equation}
		B(\mathbf a, \mathbf b) = B(a^i \mathbf v_i, b^j \mathbf w_j) = a^i b^j B(\mathbf v_i, \mathbf w_j)
	\end{equation}
	Just as in differential geometry, a 1-form's coefficient $\omega_i$ was defined by its action on \emph{each} $\partial_i = \mathbf v_i$, this \emph{bilinear} form's coefficient $B(\mathbf v_i, \mathbf w_j)$ is defined by its action on each \emph{pair} of basis vectors $\mathbf v_i, \mathbf w_j$. Let us form a vector space whose basis is not $\mathbf v_i$ or $\mathbf w_j$ but instead all possible \emph{pairs} of the form $\mathbf v_i \otimes \mathbf w_j$. $B$ is acting exactly on these pairs. 
	
	So vectors in this space look like linear combinations of the basis vectors $c^{ij} \mathbf v_i \otimes \mathbf w_j$ and $B$ acts as a \emph{linear operator} on this space: $B(c^{ij} \mathbf v_i \otimes \mathbf w_j) = c^{ij} B(\mathbf v_i \otimes \mathbf w_j)$. But besides looking at the pairs $\mathbf v_i \otimes \mathbf w_j$, can we take such a product between any two vectors, $\mathbf a \otimes \mathbf b$? We'll define $\otimes$ exactly so that it gives us what we want:
	\begin{align*}
		B(a^i \mathbf v_i, b^j \mathbf w_j) = a^i b^j B(\mathbf v_i, \mathbf w_j)\\ \Rightarrow (a^i \mathbf v_i) \otimes (b^j \mathbf w_j) = a^i b^j \mathbf v_i \otimes \mathbf w_j
	\end{align*}
	That is, the tensor product is \emph{defined} to be bilinear so that $B(\mathbf u, \mathbf v)$ corresponds exactly to $B(\mathbf u \otimes \mathbf v)$ on the tensor product space.
	
	
	
	Bilinear operators are important, however they seem less compatible with the tools of linear algebra. We don't want to generalize ``linear'' algebra to ``quadratic'' or ``cubic'' algebra because that'd be wildly more complicated and less elegant. Therefore, we view all bilinear operators as \emph{linear} operators acting on \emph{tensor spaces}, and the only bilinear operator we have to worry about from now on is $\otimes$ itself. 

	
	\begin{view}[Category Theorist's Perspective]
		The tensor product is defined so that the following definition holds.
	\end{view}
	
	\begin{defn}[Universal Property of the Tensor Product]
		For two vector spaces $V,W$, we define a bilinear map $\mathbf \varphi$ from\footnote{In general, for a function with $2$ arguments: the first in a set $A$, the second in a set $B$, we say the function's domain is $A\times B$. This is just notation.} $V \times W$ into their tensor product space $V \otimes W$. The tensor product space satisfies the \emph{universal property} that for any bilinear map $B(-,-): V \times W \rightarrow Z$ into a space $Z$, there is a \emph{unique} linear map $\tilde B(-)$ induced on $V \otimes W$ so that the following diagram commutes:
		
		\[ 
		\begin{tikzcd}
		V \times W \arrow{r}{\varphi} \arrow[dashrightarrow]{rd}{B} & V \otimes W \arrow{d}{\tilde B} \\
		  & Z \\
		\end{tikzcd}
		\]
		
	\end{defn}
	\begin{nb}
		Although $\mathbf a \otimes \mathbf b$ is an element in the tensor product space, not \emph{all} elements in this space are tensor products of two vectors. The ones that are are called \emph{\textbf{simple tensors}}\index{Tensor!Simple}. It's easy to see that not all tensors are simple, as a tensor of the form $\mathbf a \otimes \mathbf b$ really holds only $\dim V + \dim W$ pieces of information, less than the dimension of the whole space.
	\end{nb}
	
	So we can view the metric $g$ as acting on the tensor product space $T_p M \otimes T_p M$ spanned by tensors of tangent vectors $\partial_i \otimes \partial_j$. For each such basis element we have a corresponding element $dq^i \otimes dq^j$ in our dual basis. This means that the proper way to write the metric in this tensor language is 
	\begin{equation}
		g = g_{ij} ~ dq^i \otimes dq^j
	\end{equation}
	For this reason, $g$ is referred to as not just a metric but a \textbf{metric tensor}\index{Metric Tensor}.
	\begin{obs}
		A metric $g$ is a linear functional on $T_p M \otimes T_p M$, making it an element of the dual space $(T_p M \otimes T_p M)^*$.
	\end{obs}
	We see how given a tensor product basis $\partial_i \otimes \partial_j$, the dual basis is just tensors of the dual vectors of the original space $dq^i \otimes dq^j$. This means that: 
	\begin{obs}
		For finite dimensional vector spaces, $(V\otimes W)^* = V^* \otimes W^*$.
	\end{obs}
	An element in a tensor space will have components that are described by two indices $i$ and $j$, each of which can be either co/contravariant. 
	\begin{example}
		In $T_p M \otimes T_p M$ a general element will be written as $c^{ij} ~\partial_i \otimes \partial_j$. On the other hand in $T_p^* M \otimes T_p^* M$, an element is written as $c_{ij}~dq^i \otimes dq^j$ (and a metric tensor is such and element). We could abstractly define $T_p M \otimes T_p* M$ as a vector space, and there each element would be written as $c^i_j~\partial_i \otimes dq^j$, with one index co-variant, one index contra-variant.
	\end{example}
	
	\begin{nb}
		Tensor spaces are still vector spaces, as we can add and scale their elements. This is a weird thing to get used to, as it doesn't feel right to say ``tensors are vectors'', but this is due to the multiple meanings associated with the word ``vector'', some of which are not compatible with the notion of a tensor.
	\end{nb}

	In this vein of covariant and contravariant indices, let us go back to transformation laws. If we change from $q^i$ to $q'^i$, then the basis vectors/covectors at a point $p$ change according to Equations~\eqref{eq:covariant_einstein},\eqref{eq:contravariant_einstein}. Their components change in the respectively opposite ways depending on the Jacobian matrix $J = \frac{\partial q^j}{\partial q^i}$ and its inverse, $J^{-1} = \frac{\partial q^i}{\partial q^j}$ at $p$:
	\begin{equation}
		v'^i = \frac{\partial q'^i}{\partial q^j} v^j, ~~ \omega'_i = \frac{\partial q^j}{\partial q'^i} \omega_j.
	\end{equation}
	On the other hand, the metric tensor would change like:
	\begin{equation}
		g'_{ij} = \frac{\partial q^k}{\partial q'^i} \frac{\partial q^l}{\partial q'^j} g_{kl}
	\end{equation}
	That is, because the invariant length of a vector $g_{ij} v^i v^j$ involves the contra-variant components twice, $g$ must co-variantly change twice (once for each index) to keep that inner product invariant. We'd say that $g$ is a tensor with two covariant indices, or a $(0,2)$-tensor. 
	
	Of course, we can take tensor products of tensor products and form higher rank tensors. For a tensor living in $V_1 \otimes \dots \otimes V_r$ we call its \textbf{rank}\index{Tensor!Rank} $r$. Vectors and covectors are rank 1, scalars are rank 0, and the metric tensor is rank 2. We use the notation $V^{\otimes r}$ to denote $V \otimes \dots \otimes V$ $r$ times, and save space.
	
	Higher tensor spaces also satisfy a similar universal property on \textbf{multilinear maps}.
	\begin{defn}[Multilinear Map]
		A multilinear map $M$ into some vector space $Z$
		\begin{equation}
			M: V_1 \times \dots \times V_r \rightarrow Z 
		\end{equation}
		is a map with $r$ arguments, with the $i$th argument taking a vector from $V_i$ that is \emph{linear in each argument}. 
	\end{defn}
	
	
	\begin{example}
		The function giving the signed $k$-volume enclosed by a set of $k$ vectors is a multilinear map. 
	\end{example}
	
	\begin{prop}[Universal Property of Higher Tensor Spaces]
		For a multilinear map $M: V_1 \times \dots \times V_r \rightarrow Z$, there is a multilinear map into their tensor product space
		\begin{equation}
			\mathbf \varphi: V_1 \times \dots \times V_r \rightarrow V_1 \otimes \dots \otimes V_r
		\end{equation}
		so that any multilinear map $M$ induces a \emph{unique} linear map $\tilde M$ on the tensor space so that the following diagram commutes. 
		\[ 
		\begin{tikzcd}
		V_1 \times \dots \times V_r \arrow{r}{\varphi} \arrow[dashrightarrow]{rd}{M} & V_1 \otimes \dots \otimes V_r \arrow{d}{\tilde M} \\
		  & Z \\
		\end{tikzcd}
		\]
	\end{prop}
	
	
	
	\begin{defn}[An $(a,b)$-Tensor]
		A tensor of $a$ contra-variant indices and $b$ covariant indices is called an $(a,b)$-tensor, and can be written in local coordinates at a point $p$ on $M$ as:
		\begin{equation*}
			T_{j_1 \dots j_b}^{i_1 \dots i_a}  \partial_{i_1} \otimes \dots \otimes \partial_{i_a} \otimes dx^{j_1} \otimes \dots \otimes dx^{j_b}  \in (T_p M)^{\otimes a} \otimes (T_p^* M)^{\otimes b} 
		\end{equation*}
	\end{defn}
	and we know how $T_{i_1, \dots, i_a}^{j_1, \dots, j_b}$ linearly transforms on each index. Moments like this are when we should be grateful to Einstein's summation convention for saving us $a+b$ summation signs out front. 
	
	If we have a vector space $V$ and we want to consider ALL the tensors we can get from applying $V \otimes V$ to itself any number of times, then we get the \textbf{tensor algebra}\index{Tensor!Algebra}\index{Algebra!Of Tensors} $T^\cdot V$ of $V$, defined as
	\begin{equation}
		\bigoplus_{r=1}^\infty V^{\otimes r}
	\end{equation}
	In this algebra, we have our addition operation $+$ as before, and $\otimes$ becomes a multiplication operation.\\
	\todoex{EXERCISE ON A GRADED ALGEBRA}
	\todoex{Some examples of tensors}

	We gave the example of $k$-forms are antisymmetric tensors of rank $k$, and the wedge product $\wedge$ does have all of the properties of $\otimes$, but it includes the extra property that $\omega \wedge \omega = 0$, which gives it its anti-symmetry. If we exchange any two indices in the component $\omega_{j_1, \dots, j_k}$ of a tensor representing a $k$-form, the new component is the opposite sign. The tensors that satisfy this property are called \textbf{anti-symmetric}\index{Tensor!Anti-symmetric}. 
	
	Alternatively, we can define the space of anti-symmetric tensors of rank $r$ to be the space of tensors of rank $r$ with the additional constraint that $T \otimes T = 0$ for any tensor $T$ in that space. This gives us an antisymmetric space. 
	
	Performing this anti-symmetrization on $V^{\otimes k}$ gives the $k$th exterior power\index{Exterior!Power} $\Lambda^k V$. Anti-symmetrizing the whole tensor algebra gives the whole \emph{exterior algebra}\index{Exterior!Algebra}\index{Algebra!Alternating} $\Lambda V$ from before. 
	
	On the other hand, the metric tensor is \emph{symmetric}, in that exchanging the indices of a component $g_{ij}$ gives a new component $g{ij}$ with the same value. Such tensors whose components \emph{don't} change if you permute the indices are called \textbf{symmetric}\index{Tensor!Symmetric}. Alternatively for the space of rank $r$ tensors over $V$, $V^{\otimes r}$, we can define the subspace space of symmetric tensors of rank $r$ by taking $V^{\otimes r}$ and identifying $\mathbf v_1 \otimes \mathbf v_2 = \mathbf v_2 \otimes \mathbf v_1$ for any vectors $\mathbf v_1, \mathbf v_2 \in V$. Doing this on the whole tensor algebra gives the \textbf{Symmetric Algebra}\index{Symmetric Algebra}\index{Algebra!Symmetric}.
	
	\index{Tensor|)}

	\section{The Hodge Star and the Laplacian (\emph{Optional})} % (fold)
	\label{sec:the_hodge_star_and_the_laplacian}
	
	We've seen how we can get length of curve from the metric associating an infinitesimal length to a coordinate change $dq^i$. We should expect, then, that we can get volumes in our space as well.  
	
	For a point $p$ on a manifold, let us choose coordinates $x^i$ so that we have an \emph{orthonormal basis} $\hat \partial_i$ at $p$, so that $g_{ij} = \delta_{ij}$. That means that the differential form for this orthonormal basis:
	\begin{equation}
		\omega = dx^1 \wedge \dots \wedge dx^n
	\end{equation}
	gives exactly the area of a generalized parallelogram (in fact generalized rectangle) associated with $n$ changes in the $x^i$. Of course.. in an orthonormal basis we didn't need to use the language of forms. The magnitude of the volume is just $|dx^1 \dots dx^n|$.
	
	Now say we \emph{change} our $\hat \partial_i$ to a new arbitrary coordinate system $q'^i$ with local basis $\partial_i'$ by $\partial_i'=A^j_i \hat \partial_j$. Now, from linear algebra, we know that for a transformation $\mathbf A$ acting on an orthonormal basis $\mathbf e_i$, the determinant of $\mathbf A$ is the volume enclosed by the new vectors $\mathbf e'_i = \mathbf A \mathbf e_i$ the absolute value of the volume enclosed by our original choice of unit vectors must have been scaled by $|\det \mathbf A|$.
	
	For an arbitrary basis, we don't keep track of ``how to go back to an orthonormal basis'' but we \emph{do} always keep track of the metric. An if originally $g_{ij} = \left< \hat \partial_i| \hat \partial_j \right> = \delta_{ij}$ then now $g'_{ij} = A^{k}_i A^{l}_j g_{ij}$. The determinant of these matrices, on the other hand is:
	\begin{align*}
		\det g_{ij} &= \det \delta_ij =1\\
		 \Rightarrow \det g'_{ij} &= \det(A^{k}_i A^{l}_j g_{ij})\\
		 & = \det A^j_i \det A^l_j \det g_{ij} \\
		 & = (\det A)^2
	\end{align*}
	This means that the absolute value of the volume enclosed by the new vectors $\partial_i'$ is \emph{in general} equal to $\sqrt{\det g_{ij}}$. Then we have a corresponding \textbf{volume form}\index{Volume Form}\index{Differential Form!Volume Form}.
	\begin{defn}[Volume Form]
		For an $n$-dimensional Riemannian manifold $M$ with metric $g$, the invariant volume form $\mathrm{Vol}_n$ is given by
		\begin{equation}
			\mathrm{Vol}_n = \sqrt{\det g_{ij}} ~ dq^1 \wedge \dots \wedge dq^n
		\end{equation}
		This associates to each infinitesimal $n$ volume given by $n$ changes in the coordinates $dq^i$, an invariant physical volume.
	\end{defn}
	
	With a metric, we can take an inner product not only on the tangent space at a point $p$ but also on the cotangent space as well. Can we extend our inner product to act on exterior powers of the cotangent space? That is, can we take an inner product of $k$-forms if we have an inner product on the 1-forms?
	
	Answering this question is a good example of when actually going to \emph{higher} abstraction helps guide us along and give us the right answer. It's what we've been doing so far, to be able to formulate differential geometry universally and independent of an orthogonal frame. Let us then ask the more general question: given an inner-product space $V$ and an inner product space $W$ is there a natural inner product structure on $V \otimes W$?
	
	Say our basis for $V \otimes W$, as before is $\mathbf v_i \otimes \mathbf w_j$. We want to imagine a way to take:
	\begin{equation*}
		\left< \mathbf v_i \otimes \mathbf w_j | \mathbf v_k \otimes \mathbf w_l \right>
	\end{equation*}
	The naive guess would be to just let this become
	\begin{equation}\label{eq:tensor_inner_product_def}
		\left< \mathbf v_i \otimes \mathbf w_j | \mathbf v_k \otimes \mathbf w_l \right> = \left< \mathbf v_i | \mathbf v_k \right> \left< \mathbf w_j | \mathbf w_l \right> 
	\end{equation}
	so that for general tensors $\mathbf{T}, \mathbf{S} \in V \otimes W$ we would want to extend it bilinearly so that:
	\begin{equation}\label{eq:bilinear_tensor_inner_product}
		\left< \mathbf{T} | \mathbf{S} \right> 
		= \left< T^{ij} \mathbf v_i \otimes \mathbf w_j | S^{kl} \mathbf v_k \otimes \mathbf w_l \right> =  
		T^{ij} S^{kl} \left< \mathbf v_i | \mathbf v_k \right> \left< \mathbf w_j | \mathbf w_l \right> 
	\end{equation}
	
	Is this bilinear and positive definite? If so, it's a valid inner product on the tensor space, uniquely defined.
	\begin{prop}[Inner Product on $V \otimes W$]
		The above construction defines an inner product on the tensor space.
	\end{prop} 
	\begin{proof}
		We see that the inner product must be bilinear for Equation~\eqref{eq:bilinear_tensor_inner_product} to hold. In fact, bilinearity follows just by definition because
		\begin{align*}
			\left< \mathbf{T} | \mathbf{S} \right> 
			&= \left< T^{ij} \mathbf v_i \otimes \mathbf w_j | S^{kl} \mathbf v_k \otimes \mathbf w_l \right> & \\
			&= \left< (T^{ij} \mathbf v_i) \otimes \mathbf w_j | (S^{kl} \mathbf v_k) \otimes \mathbf w_l \right>  & \text{(Property of $\otimes$)}\\
			& =\left< T^{ij} \mathbf v_i | S^{kl} \mathbf v_k \right> \left< \mathbf w_j | \mathbf w_l \right> & \text{Construction, Eq.~\eqref{eq:tensor_inner_product_def}}\\
			& = T^{ij} S^{kl} \left< \mathbf v_i | \mathbf v_k \right> \left< \mathbf w_j | \mathbf w_l \right> & \text{By Inner Product on $V$}
		\end{align*}
		note that on something like the tangent space this would become:
		\begin{equation}
			\left< \mathbf{T} | \mathbf{S} \right> = g_{ik} g_{ij} T^{ij} S^{kl}
		\end{equation}
		Positive definiteness is simple to show as well. Pick the bases $\mathbf v_i, \mathbf w_i$ on each space to be orthonormal. 
		\begin{align*}
			\left< \mathbf{T} | \mathbf{T} \right> 
			&= T^{ij} T^{kl} \left< \mathbf v_i | \mathbf v_k \right> \left< \mathbf w_j | \mathbf w_l \right> \\
			&= T^{ij} T^{kl} \delta_{ik} \delta_{jl} \\
			&= \sum_{i, j} (T^{ij})^2 \geq 0
		\end{align*}
		and is only equal to zero when $T^{ij}=0$ always, i.e. $\mathbf T = 0$. Since this metric assigns positive value to any vector, it is positive definite. The fact that we used an orthonormal basis doesn't change this invariant property, it just makes the proof easier. 
		\todochange{Do this proof without going into an orthonormal basis maybe?}
	\end{proof}
	An example of an inner product on $V$ lifting to a tensor space is when we define an inner product on the set of matrices:
	\begin{example}[Inner Product of Matrices]
		On a tensors space $V \otimes V$ we can always interpret the tensor coefficients themselves as matrices and considering $\mathbf e_i \otimes \mathbf e_j = \mathbf e_{ij}$ corresponding to the $i,j$ th entry. If we have an orthogonal basis $\mathbf e_i$ for both, then an inner product on $V$ gives rise to an inner product on these matrices of $V \otimes V$ by
		\begin{equation}\label{eq:tensor_basis_produt}
			\left< \mathbf e_{ij} | \mathbf e_{kl} \right> = \delta_{ik} \delta_{jl}
		\end{equation}
		this means each of the $n^2$ entries $\mathbf e_{ij}$ is orthogonal from the others and we have an orthogonal basis for our matrix space.
	\end{example}
	Moreover this gives rise to the \emph{matrix norm} in an orthonormal basis
	\begin{equation}
		|A|^2 = \sum_{ij} (A^{ij})^2
	\end{equation}
	or in a general basis, with a metric $g$ this is 
	\begin{equation}
		|A|^2 = g_{i k} g_{j l} A^{i j} A^{k l}
	\end{equation}
	which collapses to what we would get using Equation~\eqref{eq:tensor_basis_produt} when $g_{ij} = \delta_{ij}$. 
	\begin{prop}[Inner Product on $V^{\otimes r}$]
		$V$ has an inner product structure, then so does $V^{\otimes r}$.
	\end{prop}
	\begin{proof}
		By induction, the base case $r=1$ our assumption and for the inductive step, we have just shown that if $V,W$ have inner product structure, so does $V \otimes W$. Let $W = V^{\otimes(r-1)}$ and we are done.
	\end{proof}
	
	But then if we can take inner products of arbitrary rank tensors, then we can \emph{restrict} our inner product to just the antisymmetric tensors and get

	\begin{defn}[Inner Product on $k$-forms]
		The inner product on the $k$th exterior power is the restriction of the inner product defined above on $\mathbf V^{\otimes k}$ to the subspace of antisymmetric tensors $\Lambda^k V$.
		
		In particular, since the cotangent space $T^*_pM$ has inner product given by $g^{ij}$, then the space of $k$ forms $\Lambda^k T^*_pM$ inherits an inner product:
		\begin{equation}
			\left< \alpha | \beta \right> = g^{i_1 j_1} \dots g^{i_k j_k} \alpha_{i_1\dots i_k} \beta_{j_1 \dots j_k}
		\end{equation}
	\end{defn}
	
	So now we can find norms of differential forms, which should correspond in some way to areas and $k$-volumes. There is one differential form in particular that is worth seeing the norm of: the top one, the $n$-form of our manifold:
	\begin{equation}
		\left< dq^1 \wedge \dots \wedge dq^n | dq^1 \wedge \dots \wedge dq^n \right>
	\end{equation}
	Now this is $n$ times contravariant. As a specific anti-symmetric tensor in the space, it should be written
	\begin{equation}
		\omega_{i_1\dots i_n} dq^{i_1} \wedge \dots \wedge dq^{i_n}
	\end{equation}
	where $\omega$ is antisymmetric so that $\omega_{12\dots n} = +1/n!$ and any permutation that flips the indices an \emph{even} number of times is $+1/n!$ while any permutation that flips an \emph{odd} number of times (associated with the volume oriented the other way) is $-1/n!$. Because there are $n!$ permutations, this will add up to produce a coefficient of $1$ in front of the oriented volume $dq^1 \wedge \dots \wedge dq^n$, as desired. Then the inner product of this form with itself is:
	\begin{equation}
		g^{i_1 j_1} \dots g^{i_n j_n} \omega_{i_1 \dots i_n} \omega_{j_1 \dots j_n}
	\end{equation}
	Since $i_1 \dots i_n$ need to be some permutation of $1 \dots n$ ,this can be written as a sum over \emph{all} permutations of the $n$ numbers:
	\begin{equation}
		\sum_{\sigma, \pi} g^{\sigma(1) \pi(1)} \dots g^{\sigma(n) \pi(n)} \omega_{\sigma(1) \dots \sigma(n)}  \omega_{\pi(1) \dots \pi(n)}
	\end{equation}
	where $\sigma, \pi$ denote permutations on $1 \dots n$ numbers. If we're careful with the combinatorics, though, and write $i_1=\sigma(1) \dots i_n = \sigma(n)$ this is the same sum as:
	\begin{equation}
		\sum_{i_1, \dots i_n, \rho} g^{i_1 \rho(i_1)} \dots g^{i_n \rho(i_n)} \omega_{i_1 \dots i_n} \omega_{\rho(i_1) \dots \rho(i_n)}
	\end{equation}
	where $\rho = \sigma^{-1} \pi$ is the relative permutation from the first one. Since $\omega_{i_1 \dots i_n}$ is $\pm 1/n!$ depending on the order, $\omega_{i_1 \dots i_n}$ \emph{times} $\omega_{\rho(i_1) \dots \rho(i_n)}$ will be \emph{+} $1/n!$ if $\rho$ is an \emph{even} number of interchanges and $-1/n!$ if $\rho$ is an odd number of changes so this becomes:
	\begin{equation}
		\frac{1}{n!^2} \sum_{i_1 \dots i_n, \rho}  \text{sgn}(\rho) ~ g^{i_1 \rho(i_1)} \dots g^{i_n \rho(i_n)}
	\end{equation}
	Recognize that this is exactly 
	\begin{equation}
		\frac{1}{n!^2} \det(g^{ij})
	\end{equation}
	So that the norm of this 1-form is
	\begin{equation}
		|dq^1 \wedge \dots \wedge dq^n| = \frac{\sqrt{\det g^{ij}}}{k!} = \frac{1}{k!} \frac{1}{\sqrt{\det{g_{ij}}}}
	\end{equation}
	Again, the determinant has appeared. This is another way to get that our volume form $\sqrt{\det g_{ij}} ~ dq^1 \wedge \dots \wedge dq^n$ is an invariant form with norm $1/n!$. In general inner products between $k$-forms involve these $k$ determinants that make the calculation difficult. The important thing is to know that such inner products do exist.\\
	
	Consider a $k$ form and an $(n-k)$-form (note these spaces have the same dimension, as shown in \textbf{an exercise}). Necessarily, their wedge is an $n$ form. All 0-forms (functions) are invariant, and they are a 1-dimensional space. On the other hand, now that we have a notion of metric, we have an invariant object ``on the other side'' of the tower of forms: at $\Lambda^n (T^*_p M)$, namely the volume form $\mathrm{Vol}_n$. 
	
	On one hand, for something like a 1-form we would say vectors are functionals bringing us to 1-dimensional space of invariant real numbers representing function values at $p$: $\Lambda^0 T^*_p M$. On the \emph{other} hand we could say that $(n-1)$-forms bring us to $\Lambda^n T^*_p M$ which is \emph{another} one-dimensional space. And for a given basis $dq^i$ we have a dual basis in the $(n-1)$ forms of all $(n-1)$ wedges missing a $dq^i$, appropriately oriented, ready to send us to $\Lambda^n T^*_p M$. The same happens for $k$-forms and their sister $(n-k)$-forms.
	
	Just like our metric lets us go between the 1-forms and their dual space of vectors because we have a notion of associating ``vector $\mathbf v_\omega$ perpendicular to the first order behavior of the form'' in such a way so that $\mathbf v_\omega(\omega) = \left< \omega | \omega \right>$ or more generally, using the notation $\omega^\sharp = \mathbf v_\omega$ then the dual to $\beta$, $\beta^\sharp$ is such that: 
	\begin{equation}
		\alpha(\beta^\sharp) = \left< \alpha, \beta^\sharp \right>.
	\end{equation}
	for all 1-forms $\alpha$.
		
	Similarly, now for a given $k$-form $\alpha$ we define its \textbf{Hodge dual} or \textbf{Hodge star}\index{Hodge Star} to be an $n-k$ form $\star \alpha$ such that:
	\begin{defn}[Hodge Star]
		For a given $k$-form $\beta$, we define the operator $\star: \Lambda^k T_p^*M \rightarrow \Lambda^{n-k} T_p^*M$ so that for any other $k$-form $\alpha$
		\begin{equation}
			\alpha \wedge (\star \beta) = \left< \alpha | \beta \right> \mathrm{Vol}_n
		\end{equation}
	\end{defn}
	
	The hodge star fundamentally stems from the same idea that says ``If we have a notion of being orthogonal from a metric, then as soon as we have an area element for an oriented plane in $\mathbb{R^3}$, we can associate a normal vector of that area'' which allowed us to integrate vector fields. That idea is exactly how a 2-form $\omega$ representing flux, in Euclidean space can just be represented by a vector field $(\star \omega)^\sharp$. This is more general, though: to any $k$-volume there is (upon choosing orientation) and $n-k$ that is orthogonal to it. Correspondingly, to a $k$ form there is an associated $n-k$ form. 
	
	It is worth doing a calculation:
	\begin{obs}[Hodge Star of $dq^i$]
		We have
		\begin{equation}
			\star dq^i = \omega~\mathrm{ s.t. }~ (dq^j \wedge \omega = \left<dq^j |  dq^i \right> \mathrm{Vol}_n )
		\end{equation}
		and since
		\begin{equation}
			\left<dq^j |  dq^i \right> \mathrm{Vol}_n  = g^{ij} \sqrt{\det g_{ij}} ~ dq^1 \wedge \dots \wedge dq^n
		\end{equation}
		and
		\begin{equation}
			dq^1 \wedge \dots \wedge dq^n = (-1)^{j+1} dq^j \wedge dq^1 \wedge \dots \wedge dq^{j-1} \wedge dq^{j+1} \wedge \dots \wedge dq^n
		\end{equation}
		then
		\begin{equation}
			\star dq^i = (-1)^{j+1} g^{ij} \sqrt{\det g_{ij}} dq^1 \wedge \dots \wedge dq^{j-1} \wedge dq^{j+1} \wedge \dots \wedge dq^n
		\end{equation}
	\end{obs}
	 
	 It is not too difficult to see that the hodge star applied twice, since it brings us back to $k$ forms, gives the same form \emph{up to a sign} that depends on $n$ and $k$. 
	 
	Now the divergence operator should tell us how much is going into some infinitesimal volume. It is associated with applying the exterior derivative to a $n-1$ form representing the oriented ``flux'' through each side of this volume. However, with the metric, we now have a way of taking a 1-form $\omega$ and getting an $n-1$ form by using our Hodge star to get $\star \omega$. 
	 
	More intuitively, why this works is that for an $n-1$ plane in $n$ dimensions, we can associate to it just a \emph{unit normal} (because we can define the notion of ``perpendicular'' using the metric). The flux going through this plane as an $n-1$ form can just as well be described by a $1$ form representing the component of flow along that normal vector. With all this, for any vector field, we can form an associated 1-form using the metric, and from \emph{that} another associated $n-1$ form. Then, we can take an exterior derivative to get:
	 
	\begin{defn}[The Divergence]
		The \textbf{divergence}\index{Divergence} of a vector field $\mathbf v$ is defined as 
		\begin{equation}
			\mathrm{div} ~ \mathbf v = \star \mathrm d \star (\mathbf v^\flat)
		\end{equation}
	\end{defn} 
	
	This answers a question from before: in two dimensions, the exterior derivative gave us a chain like:
	
		\begin{equation*}
			~~~~~~
			\begin{matrix}
				\text{0-forms} & \overbrace{\longrightarrow}^\mathrm d & 
				\text{1-forms} & \overbrace{\longrightarrow}^\mathrm d & 
				\text{2-forms}  \\
				\downarrow & & \downarrow & & \downarrow \\
				^{\text{Evaluated}}_{\text{~~~~~at}} & & 
				^{\text{Integrated}}_{\text{~~~along}} & &
				^{\text{Integrated}}_{\text{~~~along}} & &\\
				\downarrow & & \downarrow & & \downarrow \\
				\text{Points} & \underbrace{\longleftarrow}_\partial & 
				\text{Curves} & \underbrace{\longleftarrow}_\partial & 
				\text{Surfaces} 
			
				% \downarrow ^{\text{Evaluated}}_{\text{~~~~~at}} \downarrow & &
	% 			\downarrow ^{\text{Integrated}}_{\text{~~~along}} \downarrow & &
	% 			\downarrow ^{\text{Integrated}}_{\text{~~~along}} \downarrow & & 			\downarrow ^{\text{Integrated}}_{\text{~~~along}} \downarrow \\
			\end{matrix}
		\end{equation*}
		The first $\mathrm d$ stood for the gradient's analogue on forms, and the second for the curl's, but we know there is also a divergence in 2 dimensions. Where is it's corresponding analogue with forms? We needed a metric to define it. That gives an operation $\star \mathrm d \star$ between 1-forms and 0-forms. For the interest of the reader, in general the operator $\star \mathrm d \star$ is called the \textbf{codifferential}\index{Codifferential}\footnote{This is up to an annoying factor of a sign, depending on $n$ and $k$.}, $\delta$. It is dual to $\mathrm d$ in the sense that it is a differential operator taking us from $k$-forms to $k-1$-forms now. 
	
	Aside from this, having a definition for the divergence operator is incredibly useful because it gives rise to the central differential operator in all of physics:
	
	\begin{defn}[The Laplacian]
		The \textbf{Laplacian}\index{Laplacian} operator on a function (0-form) $f$ is defined as 
		\begin{equation}
			\triangle f = \mathrm{div} ~ \mathrm{grad} ~ f 
			= \star \mathrm d \star \mathrm d f
			= (\star \mathrm d)^2 f
		\end{equation}
	\end{defn}
	
	Again, purely for the interest of the reader: the Laplacian on $0$-forms can be written as $\delta \mathrm d$, and on general $k$-forms the natural definition can be shown to be
		\begin{equation}
			\triangle = \delta \mathrm d + \mathrm d \delta
		\end{equation}
	this allows for the definition of the wave equation (and others) on $k$-forms.
	
	For calculations, it is important to have the Laplacian of a function, $f$, explicitly:
	\begin{align*}
		\triangle f &=   \star \mathrm d ((\star{dq^i}) ~ \partial_i f ) 
				\\&= \star \mathrm d \left[ (-1)^{i+1} g^{ij} \sqrt{\det g} ~ \partial_i f ~ dq^1 \wedge \dots \wedge dq^{j-1} \wedge  dq^{j+1} \wedge \dots \wedge dq^n \right]
				\\&= \star \left[ \partial_j (g^{ij} \sqrt{\det g} ~ \partial_i f ) ~ dq^1 \wedge \dots \wedge dq^n \right]
				\\&= \partial_j (g^{ij} \sqrt{\det g} ~ \partial_i f ) ~ \star\left[ dq^1 \wedge \dots \wedge dq^n\right]
				\\&= \frac{1}{\sqrt{\det g}} \partial_j (g^{ij} \sqrt{\det g} ~ \partial_i f )
				\\&= \frac{1}{\sqrt{\det g}} \partial_j \left(\sqrt{\det g}~ \partial^j f\right)
	\end{align*}
	Of course with a simple metric $\delta_{ij}$ in an orthonormal frame this becomes just $\partial_j \partial^j f$, the sum of all second derivatives.
	
	We can even understand what this is saying intuitively. There is a function $f$. There is an associated vector field $\partial^i f$ along the direction of increase of this function. Say something (like energy) moves according to this field, wanting to go in the direction of maximum increase. For a given point $p$, we calculate the associated $n-1$ form of the flux of this thing and turn that back into a scalar that tells us the ``influx'' per unit volume of this thing. If there is just as much coming in as coming out, then $\triangle f = 0$ (Laplace's equation) and we are in equilibrium. Otherwise $\triangle f$ tells us there is a source/sink $\rho$ within this volume (Poisson's equation). 
	
	% section the_hodge_star_and_the_laplacian (end)

	\section{Movement, Lie's Ideas}
	\index{Exponential Map|(}

		To end this chapter, lets begin with something beautiful. 
		\begin{equation*}
			e^{i \pi} + 1 = 0
		\end{equation*}
		Hopefully this equation is familiar to the reader. More generally, though manipulating Taylor series in calculus class, it is known that
		\begin{equation}
			e^{i \theta} = \cos(\theta) + i \sin(\theta).
		\end{equation}
		The exponential function gives uniquely fascinating results on imaginary numbers.
		
		But now, we will consider something even stranger than exponentiating an imaginary number. Imagine we are working on the simplest manifold that we have been used to: the 1-dimensional real line $\mathbb R$. Consider
		\begin{equation*}
			e^{\frac{\partial}{\partial x}}
		\end{equation*}
		What could this nonsensical looking thing possibly mean? As before, by working through using the known Taylor series of $e^x$ we get:
		\begin{align}
			e^{\partial_x} &= 1 + \frac{\partial}{\partial x} + \frac{1}{2} \left( \frac{\partial}{\partial x} \right)^2 + \frac{1}{3!} \left( \frac{\partial}{\partial x} \right)^3 + \dots\\
			&= \sum_{k=1}^\infty \frac{1}{k!} \frac{\partial^k}{\partial x^k}
		\end{align}
		This is a formal sum of powers of the derivative operator. This object, when acting on a function gives:
		\begin{equation}
			e^{\partial_x} f = \sum_{k=1}^\infty \frac{1}{k!} \frac{\partial^k f}{\partial x^k}
		\end{equation} 
		This takes $f$ and operates on the function giving a \emph{new} one. But if we \emph{recognize this as a Taylor series} then evaluating this at a point $x_0$ gives:
		\begin{equation}
			(e^{\partial_x} f)(x_0) = \sum_{k=1}^\infty \frac{1}{k!} \frac{\partial^k f (x_0)}{\partial x^k} = f(x_0 + 1)
		\end{equation}
		This thing, $e^{\partial_x}$ \emph{evaluates $f$ to the right of $x_0$ by $1$}. It is the \textbf{translation} operator\index{Lie Group!Translations on $\mathbf R$} for functions on $\mathbb R$. But then more generally: 
		\begin{equation}
			(e^{a \partial_x} f) (x_0) = f(x_0 + a)
		\end{equation}
		so that $e^{a \partial_x}$ translates the argument of $f$ over by $a$. This is a remarkably simple and beautiful (not to mention unexpected) result about the exponential function and its action on derivatives.
		
		By the exact same logic, in much fuller generality, say we are on a manifold $M$ and we have set up coordinates $q^i$ with associated tangent vector fields $\partial_i$ on each point $p$ on the coordinate patch. Then the \emph{exact} same argument, using Taylor series says that for a multivariable function $f$ on $M$
		\begin{equation}
			(e^{X^i \partial_i} f)(q^i) = f(q^i + X^i)
		\end{equation}
		It evaluates $f$ over by a coordinate change $X^i$ from the starting point $q^i$.
		
		The exponential map is then fundamental! It takes us from a vector field $X$ defined locally on the tangent spaces of $M$ and \emph{moves us} along the flow of the vector field on $M$. The exponential map allows vector fields to becomes actual \emph{flows} along the manifold. 
		\begin{defn}
			For a manifold $M$, a flow $\varphi$ is a mapping
			\begin{equation}
				\varphi: M \times \mathbb R \rightarrow M
			\end{equation}
			such that for all points $p$ on the manifold and all real numbers $s,t$:
			\begin{itemize}
				\item $\varphi(p,0) = p$
				\item $\varphi(p,s+t) = \varphi(\varphi(p,s),t)$
			\end{itemize}
			and such that the mapping is \emph{continuous} (with respect to the topology of $M$).
		\end{defn}
		This means that starting at a point $p$, a flow gives rise to a curve $\gamma = \varphi(p,t)$. The vector field of the tangent vectors to $\gamma$ at the points that it passes through can be \textbf{exponentiated} to give the flow along $\gamma$.
		\todofig{Flow of exponentiated vector field}
		
		Starting at $p$ and given a vector field $X$ on $M$, getting a curve $\gamma$ associated with flowing along $X$ means solving the differential equation:
		\begin{equation}\label{eq:ODE_curve}
			\frac{d}{dt} q^i = X^i.
		\end{equation}
		The solutions of many dynamical systems in physics and engineering can be seen as curves found by solving differential equations that end up amounting to  Equation~\eqref{eq:ODE_curve}. Much more on this will be discussed in Chapter~\ref{ch:Symplectic}. 
		
		\begin{nb}
			Certainly there are pathological vector fields that don't generate flows, and not all vector fields are guaranteed to generate global flows on the manifold. The study of when vector fields generate global flows, and the subtleties arising therein will be glossed over in this text.
		\end{nb}
		
		Now since vector fields on a manifold form a vector space, what does it mean to exponentiate $c X$? It gives rise to the same curve, but parameterized by a parameter $t$ that traverses it $c$ times as fast. For negative $c$ this would mean reversing direction. 
		
		What about a sum of two vector fields? Naively we'd say that moving along the flow of $X$ and then moving along the flow of $Y$ should be the same as moving along the flow of $X+Y$, which should in turn be the same as moving along the flow of $Y$ and \emph{then} along the flow of $X$. For translations $\partial_x, \partial_y$ in $\mathbb R^2$, this turns out to be true. Consider however the sphere, with $X$ and $Y$ defined so as to generate circles like in the graphic below. Then moving along the flows of $X$ or $Y$ corresponds to rotating the sphere around the $z$ or $x$ axis, respectively. 
		
		\todofig{MAKE A GRAPHIC: Two vectors at different points on a sphere.. what's their difference? Two paths getting two different results.}
		
		For these vector fields on the sphere, $e^{X}e^{Y}\neq e^{X+Y} \neq e^Y e^X$. 
		
		What happens algebraically? Just exponentiating two \textbf{coordinate vector fields}\index{Coordinate Vector Field} $\partial_i, \partial_j$ associated with coordinates $q^i, q^j$
		\begin{align*}
			e^{\partial_i + \partial_j} &= 1 + (\partial_i + \partial_j) + \frac{1}{2}(\partial_i + \partial_j)^2 + \dots\\
			&= 1 + (\partial_i + \partial_j) + \frac{1}{2}(\partial_i^2 + \partial^2_j + \partial_i \partial_j + \partial_j \partial_i) + \dots\\
			&= (1+\partial_i + \frac{1}{2}\partial_i^2+\dots)(1+\partial_j + \frac{1}{2} \partial_j^2+\dots)\\
			&= e^{\partial_i} e^{\partial_j}
		\end{align*}
		Since in this case partials commute and behave just like real numbers, the same algebra giving $e^{a+b} = e^a e^b$ still holds. This was for \emph{coordinate vector fields}. This would still hold, of course, if we put \emph{constants} in front of the vectors $X = a \partial_i, Y=\partial_j$, then $e^{X+Y}=e^X e^Y = e^Y e^X$ because constants commute as well. 
		
		On the other hand, if we took an exponential of a sum of \emph{two arbitrary vector fields} $X=X^i \partial_i, Y = Y^i \partial_i$ whose components \emph{themselves} were functions of $p \in M$ so the $\partial_i X^j \neq X^j \partial_i$, then we would see something else. Just to second order: 
		
		\begin{align*}
			e^{X^i \partial_i} e^{Y^i \partial_i} &= (1 + X^i \partial_i + \frac{1}{2} {(X^i \partial_i)^2})(1 + Y^i \partial_i + \frac{1}{2} {Y^i \partial_i^2})\\
			& = 1 + (X^i + Y^i) \partial_i + \frac{1}{2} (X^i \partial_i)^2 + \frac{1}{2}(Y^i \partial_i)^2 + X^i \partial_i (Y^j \partial_j)
		\end{align*}
		on the other hand 
		\begin{align*}
			e^{Y^i \partial_i} e^{X^i \partial_i} &= (1 + Y^i \partial_i + \frac{1}{2} {(Y^i \partial_i)^2})(1 + X^i \partial_i + \frac{1}{2} {X^i \partial_i^2})\\
			& = 1 + (Y^i + X^i) \partial_i + \frac{1}{2} (Y^i \partial_i)^2 + \frac{1}{2}(X^i \partial_i)^2 + Y^i \partial_i (X^j \partial_j)
		\end{align*}
		These share all the same terms in common except their last:
		\begin{equation*}
			X^i \partial_i (Y^j \partial_j) ~\text{ vs. }~ Y^i \partial_i (X^j \partial_j)
		\end{equation*}
		These terms correspond exactly to the products $XY$ and $YX$ of the derivatives. The lowest order difference between $e^X e^Y$ and $e^Y e^X$ lies in the difference $XY - YX$. 
		
		The product rule can be applied to expand both, giving
		\begin{equation*}
			\Rightarrow (X^i \partial_i Y^j) \partial_j + X^i Y^j \partial_i \partial_j  ~\text{ vs. }~ (Y^i \partial_i X^j) \partial_j + Y^i X^j \partial_i \partial_j
		\end{equation*}
		The second terms on both expressions are equal because $\partial_i \partial_j$ is symmetric as a tensor in $T_p M \otimes T_p M$, since partials commute. This means that the lowest order difference between $e^X e^Y$ and $e^Y e^X$ lies just in:
		\begin{equation}\label{eq:first_commutator}
			XY - YX = \left[  (X^i \partial_i Y^j) - (Y^i \partial_i X^j) \right] \partial_j 
		\end{equation}
		
		From the fact that the second order terms in the taylor series expansions of these two flows differ, you can see that there is a difference that depends on the \emph{order} that we flow in, just like the example of the sphere illustrated. For general vector fields, we shouldn't expect that the exponential of the sum is the product of the exponentials either. % This difference isn't even second order (something small that can be ignored to first order), the squared terms gave rise to differing coefficients in the \emph{first order derivative} $\partial_j$.
		This failure to commute further means that general vector fields can't be represented as coordinate fields  $\frac{\partial}{\partial q_i}$ in some coordinate system $q^i$. 
		
		Something even more interesting has been revealed: we took two vector fields $X$ and $Y$ on $M$, and were able to get a \emph{third} vector field: $XY-YX$. Indeed, note that Equation~\eqref{eq:first_commutator} is a vector field. It is called the \textbf{commutator}\index{Commutator!Of Vector Fields} vector field of $X$ and $Y$.
		\begin{defn}[Commutator]
			The commutator (also called the \textbf{\emph{Lie bracket}}\index{Lie Bracket}) of two vector fields $X$ and $Y$, denoted by $[X,Y]$, is a bilinear anti-symmetric operation defined as
			\begin{equation}
				[X,Y] := X Y - Y X 
			\end{equation}
		\end{defn}
		This seems to be telling us to what extent two vector fields are \emph{incompatible}, but how can we see that this more intuitively and geometrically? \\
		
		To do this we need to develop two concepts associated with flow: the \textbf{pullback}\index{Pullback|(} and the \textbf{pushforward}\index{Pushforward|(}. Consider two manifolds, $M$ and $N$ and a smooth map $\varphi: M \rightarrow N$. In general $\varphi$ will not have an inverse.
		\begin{defn}[Pullback of a Function]
			If we have a function $f$ defined on $N$, we can \emph{pull it back} to $M$ using $\phi$ by defining the pullback of $f$ as
			\begin{equation*}
				\varphi^* f = f \circ \varphi
			\end{equation*}
		\end{defn}
		Because $\varphi$ doesn't have an inverse in general, we can't in general make a construction to do the opposite: taking a function on $M$ to a function on $N$. The pullback illustrates a \textbf{duality} that was first hinted at by vectors and 1-forms. There, the duality was between the first order behavior of curves on the manifold against the first order behavior of functions. Here, $\varphi$ allows us to take \emph{curves} on $M$ forward into curves on $N$ and its pullback allows us to take \emph{functions} on $N$ back to functions on $M$. 
		
		A special case of all this is when $U \subseteq M$ and $\varphi: U_\alpha \rightarrow \mathbb R^n$ is a coordinate patch on $M$. Coordinate patches allow us to take functions on $\mathbb R^n$ (in particular the coordinates $x^i$) and associate them to functions on $U$ (in particular coordinates $q^i$) but in general function on $U$ will be pushed forward to functions on the range of $\varphi \in \mathbb R^n$, that don't in general extend to functions on all of $\mathbb R^n$.
		
		So we cant push forward functions from $M$ to $N$ in general (although if $\varphi^{-1}$ existed, we could). On the other hand, for a vector field $X$ on $M$ it \emph{is} in general possible to associate a vector field on $N$. The reason is that because vector fields are determined by their actions $Xf = X^i \partial_i f$ on functions, then we can act by $X$ on a function $f \in C^{\infty} (N)$ as follows:
		\begin{defn}[Pushforward of a Vector Field]
			For a vector field $X \in \text{Vec}(M)$ we define its pushforward, denoted by $\varphi_*(X) \in TN$ or $d\varphi X$ by its action on $f \in C^\infty(N)$
			\begin{equation*}
				\varphi_*(X)(f) = X (\varphi^*(f))
			\end{equation*}
			Or in general $\varphi_* X= X \varphi^*$
		\end{defn}
		To understand this better let's do a small general example.
		Say we have a manifold $M$ a map $\varphi$ into $N$. If we have coordinates $q^i$ on $M$ and $q'^i$ on $N$, then $\varphi$ maps a point $p \in M$ specified by $q^i$ to a $p'\in N$ specified by $q'^i = \varphi^i (p) = \varphi^i(q_1 \dots q_n)$
		 If we have a field written in local coordinates on $M$ as $X=X^i \frac{\partial}{\partial q^i}$, how do we push this forward? For a function $f$ on $N$, we get
		\begin{align*}
			\varphi_* \Big(X^i \frac{\partial}{\partial q^i}\Big)(f) &= X^i \frac{\partial}{\partial q^i} [f\circ \varphi] &\\
			&= X^i \frac{\partial \varphi^j}{\partial q^i} \frac{\partial f}{\partial q'^j} & \text{(Chain Rule)}\\
			&= \Big(X^i \frac{\partial q'^j}{\partial q^i} \frac{\partial}{\partial q'^j}\Big)(f)
		\end{align*}
		So we get our coordinate expression of the pushforward in terms of $\partial/\partial q'^i$. The Jacobian $\partial q'^i/\partial q^j$ is a pushforward if we view a coordinate change $\varphi:M \rightarrow M$ as an active transformation on the  manifold itself. 
			
		Being able to push forward vector fields along $\varphi$ will be very useful. Now we will specialize to when $\varphi = \varphi_X(t): M \rightarrow M$ is a flow associated with a vector field $X$. Sowing together the language of pushforwards and pullbacks with the language of flows gives:
		\begin{obs}
			A flow along a field $X$ gives rise to a family of invertible maps, one for each $\varphi_X(t): M \rightarrow M$ over $t \in \mathbb R$. At a point $p$ we can pull back $f(\varphi_X(t)(p))$ and push forward another field $Y(p)$ to act at $Y(\varphi_X(t)(p))$ by
			\begin{align}
				[\varphi_X (t)]^* (f) &= e^{t X} f  \\
				[\varphi_X (t)]_* (Y) &= Y [\varphi_X (t)]^*  = Y e^{tX}
			\end{align}
		\end{obs}
		The pullback of a function along a flow is obvious. It answers ``what is the value $f$ takes on later along the flow?''. The pushforward of a field $Y$ along $X$'s flow is more interesting. The pushed forward $Y$ in the new tangent space $T_{p'} M$ will act on functions $f$ at $p'$ just like $Y$ would act on them if you were to pull them back to $p$. Now with a good language of flows, we can try to generalize the notion of a derivative along a field by taking a limit of pushforwards/pullbacks. \\
		
		
		
		% \begin{equation}
		% 	e^{X^i \partial_i + Y^i \partial_i} = 1 + (X^i + Y^i) \partial_i + \frac{1}{2} (X^i \partial_i + Y^i \partial_i)^2 + \dots
		% \end{equation}
		% the second order squared term can be expanded, and using the product rule $\partial_i (X^j \partial_j) = (\partial_i X^j) \partial_j + \partial_i \partial_j$
		% \begin{align*}
		% 	 \frac{1}{2} \Big[ X^i \partial_i &(X^j \partial_j) + Y^i \partial_i (Y^j \partial_j) + X^i \partial_i (Y^j \partial_j) + Y^i \partial_i (X^j \partial_j) \Big]
		% 	  \\
		% 	 &= \frac{1}{2} \left[ X^i (\partial_i X^j)+ Y^i (\partial_i Y^j) + X^i (\partial_i Y^j) + Y^i (\partial_i X^j) \right] \partial_j \\
		% 	 & ~~~ + \frac{1}{2} (X^i X^j + Y^i Y^j + X^i Y^j + Y^i X^j) \partial_i \partial_j
		% \end{align*}
		
		
		% On the other hand $e^{X} e^{Y}$ gives:
		% \begin{align*}
		% 	e^{X^i \partial_i} e^{Y^i \partial_i} &= (1 + X^i \partial_i + \frac{1}{2} {(X^i \partial_i)^2})(1 + Y^i \partial_i + \frac{1}{2} {Y^i \partial_i^2})\\
		% 	& = 1 + (X^i + Y^i) \partial_i + \frac{1}{2} (X^i \partial_i)^2 + \frac{1}{2}(Y^i \partial_i)^2 + X^i \partial_i (Y^j \partial_j)
		% \end{align*}
		% This shares the terms $1+(X^i + Y^i) \partial_i$ with the expansion for $e^{X+Y}$, however the last three terms of the above expression expand to:
		% \begin{align*}
		% 	&\frac{1}{2} (X^i (\partial_i X^j) + Y^i (\partial_i Y^j)+ X^i \partial_i Y^j) \partial_j  \\
		% 	& ~~~+ \frac{1}{2}(X^i X^j + Y^i Y^j + X^i Y^j)\partial_i \partial_j
		% \end{align*}


		
		
		 % Another result that follows from the fact that derivatives $\partial_i$ and functions $X^i$ don't commute is that $e^{X} e^{Y}$ is \emph{not} in general equal to $e^{Y} e^{X}$, and neither of these is in general equal to $e^{X + Y}$. This is easy to check. $e^Y e^X$ has second order terms:
 % 		\begin{align*}
 % 			&\frac{1}{2} (X^i (\partial_i X^j) + Y^i (\partial_i Y^j)+ Y^i (\partial_i X^j)) \partial_j  \\
 % 			& ~~~+ \frac{1}{2}(X^i X^j + Y^i Y^j + Y^i X^j)\partial_i \partial_j.
 % 		\end{align*}
 % 		This is almost the same as $e^{X} e^{Y}$ except for in the first lines of both, while one has $(X^i \partial_i Y^j) \partial_j$ the other has $(Y^i \partial_i X^j) \partial_j$.
 %
 % 		Ignoring higher order derivatives, the difference
 % 		\begin{equation}
 % 			e^{X} e^{Y} - e^{Y} e^{X} = (X^i \partial_i Y^j - Y^i \partial_i X^j) \partial_j + \text{higher order $\partial_i$s}
 % 		\end{equation}
 % 		This first derivative operator arose exactly from the $XY-YX$ differences in the expansion. Two flows along vector fields in general don't commute because in some sense, the vector fields \emph{themselves} don't commute. \\
		
	We know that if we have a function $f$ we can find its change along a vector field $X = X^i \partial_i$ immediately: $Xf$. We'll denote the ``change of an object along $X$" by the symbol $\mathcal L_X$. So the change of $f$ along $X$ is denoted $\mathcal L_X f := Xf$. Using flows, we could write this to get a limit definition of the derivative:
		\begin{align*}
			\mathcal L_X f(p) = \lim_{t \rightarrow 0} \frac{f(\varphi_X(p,t)) - f(p)}{t}\\\
			\Rightarrow \mathcal L_X f = \lim_{t \rightarrow 0} \frac{[\varphi_X(t)]^* f - f}{t}
		\end{align*}
		The pullback here means that we are \emph{pulling back} the value of $f$ from a point later along the flow and comparing it to the value of $f$ at the current point.
		
		
		But now, it is natural to go more general and ask \emph{how does a vector field $Y$ change along $X$?}. That is, what is $\mathcal L_X Y$? We expect it to be a vector that point in the direction that $Y$ will change in the next instant if we flow along $X$. 
		
		Consider the radial vector field $\partial_r$ and the circulatory field $\partial_\theta$. If we are flowing along $\partial_\theta$, we do \emph{not} see $\partial_r$ change at all! If $\partial_r$ was the only vector field that we cared about, then flowing along $\partial_\theta$ would be like changing \emph{nothing}. How can we make idea this more mathematical? The flow curves associated with $\partial_r$ do not change as we flow along $\partial_\theta$
		
		
		The naive guess of $\mathcal L_X Y = XY$ doesn't produce a vector. It would produce something like 
		\begin{equation}
			XY = (X^i \partial_i) (Y^j \partial_j) = X^i (\partial_i Y^j) \partial_j + (X^i Y^j) \partial_i \partial_j
		\end{equation}
		which is some strange mix of a vector and a higher derivative. What did we do wrong? When we act on an object by the derivative operator $X$ at a point $p$, it corresponds to a limiting process of changing the coordinates $q^i$ in the direction $X^i dt$ and taking the difference between the value at that new point $p'$ and the old one $p$, then dividing that by $dt$ as $dt$ goes to zero. In terms of flows it compares $[\varphi_X(t)]^* f$, which pulls back the value of $f$ from the point $t$ along the flow of $X$ against the value of $f$ now: 
		\begin{equation}
			Xf(p) = \lim_{dt \rightarrow 0} \frac{f(\varphi_X (p,dt)) - f(p)}{dt}
		\end{equation}
		For functions, there's no problem in doing this. On the other hand, for a vector field $Y^j$ we would get:
		\begin{equation}
			XY(p) = \lim_{h \rightarrow 0} \frac{Y(\varphi_X (p,dt)) - Y(p)}{dt}
		\end{equation}
		In the numerator, there is a problem: $Y(\varphi_X (p,dt))$ is an element of the tangent space at $\varphi_X (p,dt)$ while $Y(p)$ is an element of the tangent space at $p$. These are two entirely different vector spaces, so we can't just pretend the vectors are in the same space. We can't \emph{a-priori} compare vectors on different tangent spaces. Intuitively, though, in the real world we locally \emph{do} compare vectors at different places on our manifold $M$. We will see in chapter \ref{ch:GR} how the metric allows us to do this by using a \emph{connection}.  
		
		Although we can't compare vectors at different points, we \emph{can} flow the other way by $\varphi_{X}(-t)$ and \emph{push forward} $Y(\varphi_X (p,t))$ along the \emph{backwards} flow to get a vector back at $T_p$:
\\		\textbf{GRAPHIC HERE}
		\begin{equation}\label{eq:pushforward_backwards}
			[\varphi_X(-t)]_* \big(Y(\varphi_X(p, t))\big) \in T_pM
		\end{equation}
		and as $t \rightarrow 0$ this is just $Y$, so it is worth looking at
		\begin{equation*}
			\lim_{t \rightarrow 0} \frac{[\varphi_X(-t)]_* \big(Y(\varphi_X(p, t))\big)-Y(p)}{t}
		\end{equation*}
		This gives us a notion of derivative, which we will call the \textbf{Lie derivative}\index{Lie Derivative}\index{Derivative!Lie} of $Y$ along the flow of $X$. Note that at no point did we ever need to use a metric, or any notion of distance/orthogonality in defining this derivative.
		 
		If we wrote \label{eq:pushforward_backwards} in terms of exponentials of the vector fields, then it would be:
		\begin{equation*}
			 [\varphi_X(-t)]_* (e^{tX} Y(p))
		\end{equation*}
		by the definition of pullback, we must have 
		\begin{equation*}
			\Big([\varphi_X(-t)]_* (e^{tX} Y(p)) \Big) (f) = \big(e^{tX} Y(p) \big) (f(\varphi_X(-t))) = e^{tX} Y e^{-tX} f
		\end{equation*}
		Note how interesting that this initially complex thing became just a ``conjugation" of $Y$ by $e^{tX}$. The limit definition becomes:
		\begin{equation}
			\mathcal L_X Y = \lim_{t \rightarrow 0} 
			\frac{e^{tX} Y e^{-tX} - Y}{t}
		\end{equation}
		But expanding the exponentials to first order in $t$ gives something remarkable:
		\begin{equation}
			\mathcal L_X Y = \lim_{t \rightarrow 0} \frac{(1+tX)Y(1-tX) - Y}{t} = XY-YX
		\end{equation}
		The Lie derivative along $X$ of $Y$ is the commutator $[X,Y]$. 
		
		Intuitively why is it a difference of each vector field acting on the other, in both orders? There were two steps that we took when taking the Lie derivative:
		\begin{enumerate}
			\item We moved from $p$ along the flow of $X$, $e^{tX} Y$, getting $Y$ at a neighboring point $p'$.
			\item We pushed that vector $Y(p')$ \emph{back} along the flow of $X$, by defining it acting on translated functions $[\varphi_X(-t)]_* Y(p') f = Y(p') e^{-tX} f$ flowed back along the field of $X$
		\end{enumerate}
		
		Note the first step resulted in a change in $Y$: the $Y(p')$ at $p'$ is a change from our original $Y(p)$ at $p$ because of how the coordinates $Y^i$ are functions that change in space. We cared about how $Y^i$ changed as we moved along that flow, giving us $XY$, but we didn't care about taking changes in $X$. 
		
		In the second step, we now \emph{had} our changed $Y(p')$ at the neighboring point, and we wanted to bring it back, using $X$'s flow, and compare it to our starting $Y(p)$. We did that by defining its action on functions that have been ``flown'' backwards long $X$. These functions distorted by the flow-lines of $X$ so we wanted to see how the flow back along $X$ varied in the $Y$ direction, giving the $-YX$. 
		
		\textbf{CAN DO BETTER ON THIS LAST ONE}
		
		Another identity that can be derived from the notion of this Lie derivative is that since $e^{X} e^{Y} - e^{Y} e^{X} = [X,Y]$ to first order, the \emph{infinitesimal} flow along $Y$ followed by the flow along $X$ of a vector $Z$ minus the infinitesimal flow along $X$ followed by the flow along $Y$ of that same vector should equal the flow along $[X,Y]$ of that vector. In terms of commutators:
		
		\begin{align}
			\mathcal L_X (\mathcal L_Y Z) - \mathcal L_Y (\mathcal L_X Z) &= \mathcal L_{[X,Y]} Z 
			\\ \Rightarrow [X,[Y,Z]] - [Y,[X,Z]] &= [[X,Y],Z]
		\end{align}
		This is called the \textbf{Jacobi identity}\index{Jacobi Identity}. It can easily be rewritten, using the anti-symmetry of $[-,-]$ in the elegant form:
		\begin{equation*}
			[X, [Y,Z]] + [Y, [X,Z]] + [Z, [X,Y]] = 0.
		\end{equation*}
		
		The fact that $X$ and $Y$ can be joined together through a bilinear operation to produce another vector means that the space of vector field on a manifold $\text{Vect}(M)$ is endowed with a structure that is bilinear and antisymmetric. This Lie bracket turns the vector space of vector fields on $M$ into a \textbf{Lie Algebra}\index{Lie Algebra}.
		\begin{defn}[Lie Algebra]
			A Lie Algebra is a vector space $V$ together with a bilinear operation called the \textbf{commutator}\index{Commutator!Of a Lie Algebra} of the Lie algebra
			\begin{equation*}
				[-,-]: V \times V \rightarrow V
			\end{equation*}
			That satisfies $[X, X] = 0$ for all $X \in V$ (so is anti-symmetric), as well as the Jacobi identity. 
		\end{defn} 
		The Jacobi identity needs to be added for the definition of a Lie algebra because it gives all the information the failure of the commutator to have associativity $[X,[Y,Z]] \neq [[X,Y],Z]$, and Jacobi's identity shows that the difference between these two can be expressed as another commutator ($[Y,[X,Z]]$).
		
		Lie algebras are of central importance to studying manifolds and their geometry as it relates to symmetry, and we will return to them in later chapters. \\
		
		To end this entire chapter, it is worth looking back at the start of the section, to Euler's identity. Can this all be interpreted within the ideas of flows? 
		
		
		\todoadd{Relate this all to the complex exponential from the beginning of the section}
		
		
		\index{Exponential Map|)}
		\index{Pullback|)}\index{Pushforward|)}
	
	\section{Exercises}

	\textbf{WILL FORMAT THIS LATER}

	\todoex{Alternative definition for commutator}

Nothing stops us from going from our starting point $p$, moving along $X$ by $dt$ and at least seeing how the flow of $Y$ looks there, namely:
	\begin{equation}
		\varphi_{Y} (\varphi_{X}(p, dt), s)
	\end{equation}
	As $s$ varies, this defines a curve $\gamma_{Y,p'}$ passing through the point $p'$ associated with $q^i + X^i dt$. Similarly, nothing stops us from then \emph{pulling BACK} $\gamma_{Y,p'}$ along $X^i$ by acting \emph{inversely} by $\varphi_X$ at each point of $\gamma_{Y,p'}$.
	\begin{equation}
		\varphi^{-1}_X (\varphi_Y (\varphi_X(p, dt), s), dt)
		 = \varphi_X (\varphi_Y(\varphi_X(p, dt), s), -dt)
	\end{equation}
	\todofig{Alternative intuition for commutator}
	Denote the pulled back curve by $\gamma_{Y,p}'$. Note this at this curve also passes through $p$ at $s=0$. The corresponding tangent vector to $\gamma_Y'$ at $p$ which we can call $Y'(p)$ will approach $Y(p)$ as $dt \rightarrow 0$. We can define this difference:
	\begin{equation}
		\mathcal L_X Y = \lim_{dt \rightarrow 0} \frac{Y'(p) - Y(p)}{dt}
	\end{equation}

	But there's also a symmetry to the drawing above. At a specific $dt$, if we start at $p$ and flow along the $Y'$ for that $dt$ by $\varphi_{Y'}(p, t)$ to $p''$, then the curve $\gamma_{X, p''}$, when pulled back along $Y$ becomes \emph{exactly} the flow $\gamma_{X,p}$ of $X$ at $p$.
	\todofig{And dual diagram to that}
	\begin{equation}
		\varphi_Y (\varphi_X (\varphi_{Y'} (p, dt), s), -dt) =  \varphi_X(s)
	\end{equation}
	Intuitively, it seems that if we flowed for $dt$ along $Y$ instead of $Y'$, then pulling that back would produce a curve who's tangent vector is off, to first order, by $Y-Y' = - \mathcal L_X Y dt$. But flowing along $Y$ and comparing the tangent vector to the pulled back level curve for $X$ is exactly $\mathcal L_Y X$. This gives some intuition for us to expect a striking symmetry:
	\begin{equation}
		\mathcal L_X Y = - \mathcal L_Y X
	\end{equation}

	
	% We denote the 2-form representing the infinitesimal area formed by one-forms $dx^i$ and $dx^j$ by $dx^i \wedge dx^j$. This is called the wedge product between $dx^i$ and $dx^j$.
%
%
% 	Note that it is not as easy as just defining the area to be $dx dy$, like a simple scalar. This two-form is a vector-like object. Indeed, the set of all two forms in some dimension form a vector space: we can add them, we can scale them by functions, and we have $0$ to be a trivial two form of no area.
%
% 	What properties does this wedge product have?
%
% 	\begin{prop}[Properties of $\wedge$]
% 		The wedge product\index{Wedge Product} satisfies:
% 		\begin{enumerate}
% 			\item $dx^i \wedge dx^i = 0$
% 			\item $(\alpha dx^i) \wedge dx^k = \alpha (dx^i \wedge dx^j)$
% 			\item $(dx^i + dx^j) \wedge dx^k = dx^i \wedge dx^k + dx^j \wedge dx^k$
% 		\end{enumerate}
% 	\end{prop}
%
% 	Three forms? Infinitesimal parallelepipeds. Past that, it gets difficult to visualize, but you get the idea. Moreover, the formalism does not change.
%
% 	\textbf{Talk about coordinate independence of the FTOC and now how we get it for the proof in the divergence theorem}
%
% 	\textbf{Example in 1-D, 2-D, and 3-D}
%
%
%
%
%
% 	If we have the vectors $\mathbf u, \mathbf v$ then there is an associated area between them. In multivariable calculus, to \emph{represent} this area, we would have taken the \textbf{cross product}\index{Cross Product}
% 	\begin{equation}
% 		\mathbf u \times \mathbf v = A \hat {\mathbf n}
% 	\end{equation}
% 	where $A$ is the area of the parallelogram they generate, and $\mathbf{n}$ defines the normal to that plane. This only worked in three dimensions, where we could take about ``normal vectors to planes'' instead of planes, because every 2-D plane defined just two unique 1D normal vectors (because 3 = 2 + 1).
% 	\begin{equation}
% 		\mathbf u \wedge \mathbf v
% 	\end{equation}
% 	to be not the normal vector to the plane spanned by $\mathbf u, \mathbf v$ but an area on the plane \emph{itself}. For this reason, just like with the cross product $\mathbf u \wedge \mathbf u = 0$, because we haven't introduced a new direction aside from $u$ to form a plane. Similarly just like the cross product, we want it to be linear in both arguments.
%
%
% 	\begin{cor}[Antisymmetry of $\wedge$]
% 		From the above properties, it follows that $\mathbf u \wedge \mathbf v = - \mathbf v \wedge \mathbf u$.
% 	\end{cor}
% 	\begin{proof}
% 		Consider $(\mathbf u + \mathbf v) \wedge (\mathbf u + \mathbf v)$. This is a vector wedged with itself, so is zero, on the other hand it expands out to:
% 		\begin{align*}
% 			0 &= \mathbf u \wedge \mathbf u + \mathbf u \wedge \mathbf v + \mathbf v \wedge \mathbf u + \mathbf v \wedge \mathbf v \\&= \mathbf u \wedge \mathbf v + \mathbf v \wedge \mathbf u \\
% 			&\Rightarrow \mathbf u \wedge \mathbf v = - \mathbf v \wedge \mathbf u
% 		\end{align*}
% 	\end{proof}
% 	What does this antisymmetry mean? We had it with the cross product as well, $\mathbf u \times \mathbf v = - \mathbf v \times \mathbf u$. This corresponded to the direction in which the normal vector pointed. It means that not only do we associate an area to the plane which $\mathbf u$ and $\mathbf v$ span, but we associate a \emph{signed area} to it, corresponding to an \textbf{orientation}\index{Orientation}. What does this mean? In multivariable calculus we asked ``How much of our vector field $\mathbf F$ is flowing through an infinitesimal parallelogram?'', but we need to know \emph{which side means ``out''}.
%
% 	So if we can take wedge products of elements in a vector space $V$, the resulting space of wedge products is denoted $\Lambda^2 V$ and is called the second \textbf{Exterior Power}\index{Exterior Algebra!Exterior Power} of $V$. The first exterior power is just $\Lambda^1 V = V$, and the zeroth is just the underlying $\Lambda^0 V = \mathbb{R}$.
%
% 	 If these coordinates are in the same direction, there
% 	\begin{equation}
% 		\omega = \omega_{ij} dx^i \wedge dx^j
% 	\end{equation}
%
% 	So now what is a 2-form? It is an object that is meant to be integrated over a 2D surface $\Omega$, but in multivariable calculus, the formula for $2$-dimensional integration was
% 	\begin{equation}
% 		\int_\Omega \mathbf F \cdot d\mathbf S
% 	\end{equation}
% 	this calculates the flux of $\mathbf F$ through the surface. The 2-form should then be $\omega = \mathbf F \cdot d\mathbf S$. Now $d\mathbf S$ represents an infinitesimal parallelogram obtained by varying, say, $dq^1$ and $dq^2$ independently.
%
% 	Say we are integrating a 1-form $\omega$ along a small line segment where (without loss of generality) $q^1$ is changing by some very small length $dq^1$. All other coordinates are held fixed, so $dq^i = 0 ~ \forall i \neq 1$. This infinitesimal line segment could be turned into a 2-D parallelogram by adding another \emph{different} $q^i$ into the mix, say (again without loss of generality) $q^2$. Then in multivariable calculus this would be an infinitesimal area element of size $dA = dq^1 dq^2$. This is what we want to do: we want to be able to multiply two one-forms to get a two-form that is meant to be integrated along areas.
%
% 	This multiplication is not just as straightforward as multiplying the differentials as if they were numbers $dx^i dx^j$ like we did when integrating. The differentials are elements of the vector space of one forms at each point. We want to multiply two vectors (that give values when given a specific direction or infinitesimal line segment $dx^i$) and obtain a new type of object that gives a value when given a specific \emph{plane element}, or infinitesimal parallelogram $dx^i, dx^j$. Extending a length in its own direction would not give rise to an area. We need to define a product $\wedge$ of 1-forms so that $dq^1 \wedge dq^1 = 0$
%
%
% 	When we visualize any vector space, whether physical or abstract algebraic, we can view each component as being an axis, and the vector as being a magnitude and direction along this space.
%
% 	%% Need to work on this
% 	A physical vector takes specific direction (a line passing through the origin) and associates to it a magnitude. The vector's information is the direction of the line, together with ``how far we go along this line". Algebraic vectors are anything that can be added and scaled. Physical vectors $v^i \partial/\partial q^i$ are examples of algebraic vectors, as are 1-forms. 1-forms can also be seen
% 	%%
%
\end{document}
