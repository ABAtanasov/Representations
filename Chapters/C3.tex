
\chapter{Differential Geometry}

In calculus class, the fundamental theorem of calculus is introduced: that the total difference of a function's value at the end of an interval from its value at the beginning is the sum of the infinitesimal changes in the function over the points of the interval:

	\begin{equation}\label{eq:FTOC}
		\int_a^b f'(x) dx = f\Big\rvert^b_a
	\end{equation}

And later, in multivariable calculus, more elaborate integral formulae are encountered, such as the divergence theorem of Gauss:

	\begin{equation}\label{eq:Divergence}
		\int_\Omega \nabla \cdot \mathbf{F} ~ dV = \int_S \mathbf{F} \cdot d\mathbf S
	\end{equation}

	where $\Omega$ is the volume of a 3D region we are integrating over, with infinitesimal volume element $dV$ and $S$ is the surface that forms the boundary of $\Omega$. $dS$ then represents an infinitesimal parallelogram through which $\mathbf{F}$ is flowing out, giving the flux integral on the right. Read in English, Gauss' divergence theorem says ``Summing up the infinitesimal flux over every volume element of the region is the same as calculating the total flux coming out of the region''. The total flux coming out of a region is the sum of its parts over the region. You might see that in English, this reads very similar to the description of the fundamental theorem of calculus.
	
	Alongside this, there is Stokes' theorem for a 2D region. In English: summing up the infinitesimal amount of circulation of a vector field $\mathbf F$ over every infinitesimal area is equal to calculating the total circulation of $\mathbf F$ around the boundary of the region. In mathematical language:
	
	\begin{equation}\label{eq:Stokes}
		\int_R \nabla \times \mathbf{F} ~ dA = \int_C \mathbf{F} \cdot d\mathbf r
	\end{equation}
	
	where $R$ is our region and $C$ is its boundary.
	
	Perhaps now, the pattern is more evident. In all the above cases, summing up some \emph{differential} of the function on the interior of some region is the same as summing up the function itself at the \emph{boundary} of the region. All these theorems, that on their own look so strange to a first-year calculus student, are part of a much more general statement, the \textbf{General Stokes' Theorem}\index{Stokes' Theorem!General}:

	\begin{theorem}[General Stokes' Theorem]\label{thm:GeneralStokes}
		\begin{equation} \label{eq:GeneralStokes}
			\int_\Omega \mathrm d \omega = \int_{\partial \Omega} \omega.
		\end{equation}
	\end{theorem}

	

	
	Above, $\omega$ is an object that will generalize both the ``functions" and ``vector fields" that you've seen in multivariable calculus, and $\mathrm d$ will generalize of all the differential operators (gradient, divergence, curl) that you've dealt with. Lastly, when $\Omega$ is the region in question $\partial \Omega$ represents the \emph{boundary} of the region $\Omega$. The fact that it looks like a derivative symbol is no coincidence, as we'll see that the natural way to define the ``derivative'' of a region is as its boundary.
	
	%This is good, I feel like it belongs here, whatchu think Hilldog
	
	Through abstraction, we can reach results like this that not only look elegant and beautiful, but also provide us with insight into the natural way to view the objects that we've been working with for centuries. This gives us not only understanding of what language to use when studying mathematics, but also what is the natural language in which to describe the natural world. The general Stokes' theorem is one of the first examples of this beautiful phenomenon, and this book will work to illustrate many more. 
	
	%Again, this is outdated now, we've already talked about this 
	
	For the first half of this chapter, we will work towards giving the intuition behind  this result. On our way, we will begin to slowly move into a much more general setting, beyond the $3$-dimensional world in which most of multivariable calculus was taught. That doesn't just mean we'll be going into $n$-dimensional space. We'll move outside of euclidean spaces that look like $\mathbb{R}^n$, into non-euclidean geometries. This will put into question what we really mean by the familiar concepts of ``vector'', ``derivative'', and ``distance'' as the bias towards Euclidean geometry no longer remains central in our minds. At its worst, the introduction of new concepts and notation will seem confusing and even unnecessary. At its best, it will open your mind away from the biases you've gained from growing up in a euclidean-looking world, and give you a glimpse of how modern mathematics \emph{actually} looks. 
	
	%This is such a good paragraph, it probably belongs somewhere earlier on!
	
	Modern mathematics is learning that the earth isn't flat. To someone who's never had those thoughts, it is difficult to get used to, tiring, and sometimes even rage inducing, but to someone who has spent months thinking and reflecting on it, it quickly becomes second nature. Far from being the study of numbers or circles, it is a systematic climb towards abstraction.  It is a struggle towards creating one language, free from all-encompassing human bias, in order to try and describe a world that all other human languages, for so many centuries, have failed to grasp. It is humbling, and in the strangest of ways, it is profoundly beautiful.



\section{The Derivative and the Boundary} % (fold)
\label{sec:the_derivative_and_the_boundary}

% section the_derivative_and_the_boundary (end)

	Let's start working towards understanding Equation~\eqref{eq:GeneralStokes}. First, let's work with what we've already seen to try and explore the relation between integrating within a region and integrating on the boundary. 
	
	If we are in one dimension, we have a function $f$ defined on the interval $x \in [a,b]$. Proving Equation~\eqref{eq:FTOC} is much easier than you'd think. Let's take a bunch of steps: $x_i = a + (b-a)i/N$, so that $x_0 = a, x_N = b$. Then all we need is to form the telescoping sum:	
	\begin{align*}
		f\rvert^b_a &= f(x_N) - f(x_0) \\& = \sum_{i=1}^N f(x_{i})-f(x_{i-1}).
	\end{align*}
	If we make the number of steps $N$ large enough, the stepsize shrinks so that in the limit, we get
	\begin{align*}
		\lim_{N \rightarrow \infty} 	\sum_{i=1}^N f(x_{i})-f(x_{i-1}) & = \lim_{N \rightarrow \infty} 	\sum_{i=1}^N \Delta f \\ & = \int_a^b df.
	\end{align*}
	Of course, the way its written more often is:
	\begin{align*}
		\lim_{N \rightarrow \infty} 	\sum_{i=1}^N \frac{\Delta f}{\Delta x} \Delta x  = \int_{a}^{b} \frac{df}{dx} dx.
	\end{align*}
	
	What is the idea of what we've done? At each point we've taken a difference of $f$ at that point with $f$ at the preceding one. Because we're summing over all points, the sum of differences between neighboring points will lead to cancellation everywhere \emph{except} at the boundary, where there will not be further neighbors to cancel out the $f(b)$ and $f(a)$. From this, we get Equation~\eqref{eq:FTOC}. 
	
\noindent \textbf{Note:}
	Now for a distinction which may seem like it isn't important. We haven't integrated from point $a$ to point $b$. We have integrated from where the coordinate $x$ take \emph{value} $a$, to the where coordinate $x$ takes \emph{value} $b$. $a$ and $b$ are \emph{NOT} points. They are numbers, values for our coordinate $x$. As we have said in the preceding chapter, the idea that numbers form a \emph{representation} for points is ingenius, but numbers are \emph{not} points. Although we could write this interval as $[a,b]$ in terms of some variable $x$, it would be a completely different interval should we have chosen a different coordinate $u$. This is why, when doing $u$-substitution, we change the bounds. In coordinate free, language, then:

	
	\begin{theorem}[Fundamental Theorem of Calculus]\label{thm:FTOC}
		For a given interval $I$ with endpoints $p_0, p_1$ and a smooth function $f$, we have
		\begin{equation}
			  \int_{p_0}^{p_1} df = f \Big\rvert_{p_0}^{p_1} 
		\end{equation}
	\end{theorem}
	Notice something: the end result doesn't depend on the partition $x_i$ at all, so long as it becomes infinitesimal as $N \rightarrow \infty$. That is to say: we are summing up the change of $f$ over some interval, but it doesn't matter what coordinate system we use to describe this interval. The integral is \emph{coordinate independent}. We chose to use $x$ as our coordinate, describing the interval as going from $x=a$ to $x=b$, but we didn't \emph{have} to make this specific choice. This makes perfect physical sense. For example, if we had a temperature at each point in space, the temperature difference between two fixed points some shouldn't depend on whether we use meters or feet to measure their distance apart.  
	
	Written mathematically: 
	\begin{align*}
		\int_I df = \int_I \frac{df}{dx} dx = \int_I \frac{df}{du} du 
	\end{align*}
	
	If we chose an $I$ that's very small around some point, essentially an infinitesimal line segment, we get:
	\begin{align*}
		\frac{df}{dx} dx =  \frac{df}{du} du \Rightarrow \frac{df}{dx} = \frac{df}{du} \frac{du}{dx}
	\end{align*}
	this is the $u$-substitution rule from calculus.
	\\

	
	Now what if $f$ was a function defined not on the real line $\mathbb{R}$, but on 2-dimensional space $\mathbb{R}^2$, or more generally $n$-dimensional space $\mathbb{R}^n$. To each point $p = (p_1, \dots, p_n)$ we associate $f(p)$. Now again, consider $f(p_f)-f(p_i)$ for two points in this space.
	
	For any curve $C$ going between $p_i$ and $p_f$, say defined by $\mathbf r(t)$ for $t$ a real number going from $a$ to $b$, we can make the same partition $t_i = a + (b-a)i/N$ and let $N$ get large. Again, it becomes a telescoping sum:
	\begin{align*}
		f(p_f) - f(p_i) = &f(\mathbf r(b)) - f(\mathbf r(a)) \\= & \sum_{i=1}^N f(\mathbf r(t_{i}))-f(\mathbf r(t_{i-1})) \\ = & \sum_{i=1}^N \Delta f_i  \rightarrow \int_C df.
	\end{align*}
	Now if we cared about coordinates, we could ask ``how can we write $df$ in terms of $dt$ or $dx_i$?''. 
	
	We know from the multivariable chain rule that the infinitesimal change of $f$ is the sum of the change in $f$ due to every individual variable, so: 
	\begin{equation}
		df = \sum_i \frac{df}{dx_i} dx_i
	\end{equation}

	We know that $dx_i$ together must lie along $C$. In terms of $t$ since $x_i = r_i (t)$, we have $dx_i = \frac{dr_i}{dt} dt$ giving:
	\begin{theorem}[Fundamental Theorem of Line Integrals]
	For a smooth function $f$ defined on a piecewise-smooth curve $C$ parameterized by $\mathbf r(t)$
		\begin{equation}
			f\rvert^{p_f}_{p_i} = \int_C \sum_i \frac{df}{dx_i} \frac{dr_i}{dt} dt = \int_C \nabla f \cdot \frac{d \mathbf r}{dt} dt =  \int_C \nabla f \cdot d \mathbf r
		\end{equation}
	\end{theorem}
	The proof of this was no different from the 1-D case.\\
	
	Let's go further. Consider a region in three dimensions and a vector field 
	\begin{equation*}
		\mathbf F = F_x \hat{\mathbf{i}} + F_y \hat{\mathbf j} + F_z \hat{\mathbf k}
	\end{equation*} 
	We want to relate the total flux coming out of the region to the infinitesimal flux at each point inside the region. To do this, as before, we will subdivide the region. This time, it will not be into a series of intervals, but instead into a mesh of increasingly small \emph{cubes}, as below.
	
	\textbf{PUT A GRAPHIC HERE}
	
	See that the flux out a side of each cube is cancelled out by the corresponding side on its neighboring cube. That means that the only sides that do not cancel are for the cubes at the boundary$^1$\footnote{You may be worried that the cubes do not perfectly fit into the boundary when it is not rectangular. As the mesh gets smaller and smaller, it approximates the region better so this does not pose a problem. This idea can be made rigorous (c.f. \textbf{GIVE A REFERNCE HERE})}, giving us the desired flux out.
	
	So if we sum the fluxes over all infinitesimal cubes, we will get the total flux out of the boundary. For a single cube of sides $dx,dy,dz$, drawn below, the total flux will be the sum over each side. 
	\begin{align*}
		\text{Flux} =&~~~  F_z(x,y,z+dz/2) dx dy -  F_z(x,y,z-dz/2) dx dy \\ 
						   & + F_y (x,y+dy/2,z) dx dz - F_y (x,y-dy/2,z) dx dz \\ 
						   & + F_x (x+dx/2,y,z) dy dz - F_x (x-dx/2,y,z) dy dz \\ 
	\end{align*}
	\textbf{SHOW GRAPHIC HERE}
	
	But we can write this as: 
	\begin{align*}
		\left( \frac{\partial F_x (x,y,z)}{\partial x} + \frac{\partial F_y (x,y,z)}{\partial y} + \frac{\partial F_z(x,y,z)}{\partial z} \right) dx dy dz = \nabla \cdot \mathbf F ~ dV
	\end{align*}
	So the total flux will be the sum over all these cubes of each of their total fluxes. But then this becomes exactly the divergence theorem:
	\begin{theorem}[Divergence Theorem, Gauss]
	For a smooth vector field $\mathbf F$ defined on a piecewise-smooth region $\Omega$, then we can relate
		\begin{equation*}
			\int_\Omega \nabla \cdot \mathbf F ~ dV = \int_{\partial \Omega} \mathbf{F} \cdot d \mathbf S
		\end{equation*}
	\end{theorem}
	
	It is an easy \textbf{exercise} to show that this exact same argument holds for an $n$-cube. 
	
	What did we do? In the fundamental theorem of calculus/line integrals, we had a function $f$ evaluated on the 1-D boundary, and we chopped the curve into little pieces that cancelled on their neighboring boundaries, making a telescoping sum. Then we evaluated the contribution at each individual piece, and found that it was $df = f'(x_i) dx$, meaning that the evaluation on the boundary could be expressed as an integral of this differential quantity over the curve. That is Equation~\eqref{eq:FTOC}.
	
	For the divergence theorem, we had a vector field $\mathbf F$, again \emph{evaluated on the boundary}, this time in the form of a surface integral. We chopped the region into little pieces (cubes now) that cancelled on their neighboring boundaries, making a telescoping sum. Then we evaluated the contribution at each individual piece and found that it was $\nabla \cdot \mathbf F ~ dV$, meaning that the integration on the boundary could  be expressed as an integral of this differential quantity over the region. That is Equation~\eqref{eq:Divergence}.
	
	
	Through abstraction, we see that there is really no difference. Perhaps now Equation~\eqref{eq:GeneralStokes} does not look so mysterious and far-off.\\
	
	For Equation~\eqref{eq:Stokes}, we have a vector field $\mathbf F$ evaluated on the boundary in the form of a contour integral around a region. This is the total circulation of $\mathbf{F}$ around the region. Let us chop the region into little pieces. 
	
	\textbf{INSERT GRAPHIC HERE}
	
	On an infinitesimal square, we get that the circulation is:
	\begin{align*}
		\text{Circulation} =& ~~~  F_y (x+dx/2,y) dy - F_y (x-dx/2,y) dy \\ &+  F_x (x,y-dy/2) dx - F_x (x,y+dy/2) dx
	\end{align*}
	This can be written as:
	
	\begin{align*}
		\left( \frac{\partial F_y}{\partial x} - \frac{\partial F_x}{\partial y} \right) dx dy = (\nabla \times \mathbf F) ~ dA
	\end{align*}
	so that
	\begin{theorem}[Stokes' Theorem in $2$D] For a smooth vector field on a piecewise smooth region $S$ 
		\begin{equation}
			\int_C \mathbf{F} \cdot d\mathbf r = \int_S \nabla \times \mathbf{F} ~ dA
		\end{equation}
	\end{theorem}
	Exercise \textbf{(MAKE AN EXERCISE)} generalizes this to a surface in 3D, to get the 3D version of Stokes' theorem \index{Stokes' Theorem!For Curl}:
	
	\begin{equation}
		\int_C \mathbf{F} \cdot d\mathbf r = \int_S (\nabla \times \mathbf{F}) \cdot d\mathbf S 
	\end{equation}
	
	The philosophy behind these proofs is always the same. It is the manipulation of the differentials that seems wildly different every time. The curl looks nothing like a divergence, and a divergence is distinct from a gradient. Moreover, its not clear in what way each one generalizes the one dimensional derivative $df = f'(x) dx$. This is the problem that the symbol `$\mathrm d$' in Equation~\eqref{eq:GeneralStokes} was made to solve.\\
	
	We must stop thinking of the $1$D derivative, the gradient, the divergence, and the curl, as unrelated operations. They are in fact, the same operation, applied in different circumstances. Infinitesimal change, flux, and circulation are all the same differential action applied to different types of objects. 
	
	Perhaps part of this was clear from multivariable calculus: the gradient is nothing more than a generalization of the derivative to functions on higher dimensions. Then why are there seemingly two different, unrelated types of ``derivative'' on vector fields? Instead of a regular, gradient-like object, we have two: the divergence and the curl. 
	
	It will turn out that the reason that there are two is this: the vector fields that we take curls of are a different type of object from the vector fields we take divergences of. Looking forward, we'll see that we only take the curl on a vector field that is ``meant to be integrated along curves" (\textbf{1-form}), and the curl gives us another vector field ``meant to be integrated over surfaces" (\textbf{2-form}). On the other hand, the divergence takes a vector field ``meant to be integrated over surfaces'' (\textbf{2-form}) and gives us a scalar field ``meant to be integrated over volumes'' (\textbf{3-form}). Every object that we've encountered when integrating: from functions in 1-D or 3-D, to vector fields in $n$-D, have been examples of these \textbf{forms}. \index{Differential Form} \\
	
	To get to this idea, we first need to stop thinking of functions and vector fields as totally separate objects. A function is an object that is ``meant to be evaluated at a point'' (\textbf{0-form}). The derivative takes us from a function to a 1-form, meant to be integrated along a curve. It is the exact same object as the one in Section~\ref{sec:vectors_reimagined}. The gradient, properly speaking, is not a vector describing the local behavior of a \emph{curve} but is the opposite: a 1-form describing the local behavior of a \emph{function}.
	So the correct way of writing this, is to go from the old $\mathbb{R}^3$ notation
	\begin{equation*}
		\nabla f = \frac{\partial f}{\partial x} \hat{\mathbf i}
					+\frac{\partial f}{\partial y} \hat{\mathbf j}
					+\frac{\partial f}{\partial k} \hat{\mathbf k}
	\end{equation*}
	to the modern language
	\begin{equation}
		\mathrm d f  = \frac{\partial f}{\partial x^i} dx^i ~\Big(= \nabla f \cdot d \mathbf r\Big)
	\end{equation}
	This is a 1-form, as we already know.
	
	What we would like is to have the old multivariable calculus chain
	\begin{equation*}
		\text{functions} ~ \overbrace{\longrightarrow}^{\text{grad}}
		~ \text{vector fields} ~ \overbrace{\longrightarrow}^{\text{curl}}
		~ \text{vector fields} ~ \overbrace{\longrightarrow}^{\text{div}} 
		~ \text{functions} 
	\end{equation*}
	be converted to
	\begin{equation*}
		~~~~~~
		\begin{matrix}
			\text{0-forms} & \overbrace{\longrightarrow}^\mathrm d & 
			\text{1-forms} & \overbrace{\longrightarrow}^\mathrm d & 
			\text{2-forms} & \overbrace{\longrightarrow}^\mathrm d & 
			\text{3-forms}  \\
			\downarrow & & \downarrow & & \downarrow & & \downarrow \\
			^{\text{Evaluated}}_{\text{~~~~~at}} & & 
			^{\text{Integrated}}_{\text{~~~along}} & &
			^{\text{Integrated}}_{\text{~~~along}} & &
			^{\text{Integrated}}_{\text{~~~along}} & &\\
			\downarrow & & \downarrow & & \downarrow & & \downarrow \\
			\text{Points} & \underbrace{\longleftarrow}_\partial & 
			\text{Curves} & \underbrace{\longleftarrow}_\partial & 
			\text{Surfaces} & \underbrace{\longleftarrow}_\partial & \text{Volumes}
			
			% \downarrow ^{\text{Evaluated}}_{\text{~~~~~at}} \downarrow & &
% 			\downarrow ^{\text{Integrated}}_{\text{~~~along}} \downarrow & &
% 			\downarrow ^{\text{Integrated}}_{\text{~~~along}} \downarrow & & 			\downarrow ^{\text{Integrated}}_{\text{~~~along}} \downarrow \\
		\end{matrix}
	\end{equation*}
	where $\partial$ is the \textbf{boundary operator}\index{Boundary Operator} that takes us from an $n$ dimensional manifold to its $n-1$ dimensional boundary. At this point, we won't be afraid to use our new word: manifold\index{Manifold}, when referring collectively to curves, surfaces, volumes, or any of their higher dimensional generalizations (points too, as the degenerate 0 dimensional case). 
	
	As a final note of this section, let us try to give a sketch for why on a region $\Omega$, we denote its boundary with the partial derivative symbol as $\partial \Omega$. Picture in your mind a ball (interior of a sphere) of radius $r$,  $B_r$. If we increase the radius by a tiny amount $h$ then we have a slightly larger radius $B_{r+h}$. If we took the difference $B_{r+h} - B_r$, by which we mean all the points of $B_{r+h}$ that are not in $B_r$, we would be left with a thin shell. In the limit as $h \rightarrow 0$, this becomes a sphere of radius $r$, precisely the boundary of $B_r$ (note that a sphere is always the two-dimensional boundary of the ball). See how similar this is to taking derivatives. This is why $\partial B_r$ is what we use to denote the sphere boundary of the ball. 
	
	You may ask ``but what about dividing by $h$ at the end, like we do for a regular derivative?''. This also has an interpretation. The 3D volume of a sphere is zero, since it is a 2-D boundary. Dividing by $h$ as $h$ goes to zero puts increasing ``weight'' on the shell so that as the shell shrinks to becoming absolutely thin, 3-D integrals on it become 2-D \footnote{For those familiar with the terminology: dividing by $h$ corresponds to multiplying by a dirac delta that spikes exactly on the sphere. This turns integrals over 3-D space into 2-D integrals on the sphere}.
	 % But even here, we can go deeper. We've figured out why the 1-D integral becomes just a difference at two points, but we can actually interpret the \emph{difference} $f(b)-f(a)$ an integral! It is a zero-dimensional integral over the boundary of the region. The boundary of an interval is just two points, and a zero dimensional integral is a sum over points.
 %
	
	
	
\section{The Notion of a Form}

	A differential form $\omega$, in short, is an object that is meant to be integrated. We've seen the example of 1-forms in the preceding chapter. At a point $p$ on a manifold, one-forms $\omega$ are exactly all the first-order behaviors of the functions at $p$. Just as we can have a vector field on $\mathbb{R}^3$, or manifolds in general, we have 1-form fields. You have seen this before: the gradient is in fact a one-form field
	\begin{equation}
		(\nabla f \cdot d\mathbf r) (p) = \frac{\partial f}{\partial q^i}(p)~ dq^i
	\end{equation}
	The components of the gradient are \emph{covariant} with our change of coordinate system, just like those of a 1-form (and unlike those of a vector field). 
	
	A general 1-form associates a first-order function behavior $\omega_i(p)~ dq^i$ to every point $p$ in space. Just because $\omega$ is some differential behavior of a function at a point $p$ doesn't mean that $\omega$ actually \emph{is} the differential of a function. That is, it doesn't mean there exists a function so that $\partial f/\partial q^i = \omega_i$ at every point $p$ so that $\omega = \mathrm df$.
	
	It rarely true that $\omega = \mathrm df$. In fact, this happens \emph{exactly} when $\omega$ actually \emph{does} correspond to a gradient. This is exactly what we called a conservative vector field in introductory physics and in multivariable calculus. In this language, we call $\omega$ an \textbf{exact form}\index{Differential Form!Exact} when it is the differential of a function.
	
	
	
	This is what we care about when integrating. It is more fundamental than $\mathbf{F}$, but what does it mean \emph{physically}? If $\mathbf{F}$ was a force field, then since we know $\mathbf{F} \cdot d \mathbf{r} = dW$, this form $\omega$ represents all possible infinitesimal changes in work $dW$ at a given point, depending on what changes $dx,dy,dz$ we do.
	
	If we were actually \emph{given} the changes in each of the coordinates $dx,dy,dz$, we could plug them in to $\omega$ and get the first-order approximation of the amount of work done over that distance. This is a point that has been said before: $\omega$ does not represent a specific change in work, but rather the \emph{relationship} between the changes in coordinate and the change in work. If you \emph{give it} an infinitesimal displacement, it will tell you the associated work. When integrating along a curve, the displacement is simply the tangent vector to the curve.

	Even simpler than one-forms are the \textbf{zero forms}, with no differentials appearing. A zero-form precisely a scalar function at $f(p)$ each point $p$. Regardless of how we change our coordinate system, the value of the \emph{function} at point $p$ is the same.
	
	We are now in a good place to define $\mathrm d$, at least for going from functions (zero-forms) to one-forms. Given a function $f$, $\mathrm d f$ will produce a form representing the local change in $f$ depending on the displacement. We call $\mathrm d$ the \textbf{exterior derivative} \index{Exterior Derivative} operator.
	
	For example, for a potential energy function $\phi$, $\mathrm d \phi$ can be written as 
	
	\begin{equation}
		\mathrm d \phi = \sum_{i=1}^n \frac{\partial \phi}{\partial x^i} dx^i
	\end{equation}
	
	because of $\mathrm d$, we will no longer have to use the gradient at all. This is more important than simply meaning that we'll grow to stop using $\mathbf{\hat i}, \mathbf{\hat j},\mathbf{\hat k}$. It is something much deeper. In in two-dimensional motion, if you have some potential $\phi$ at a point $p$, then of course the value of $\phi$ at $p$ is independent of any coordinate system you use. If you have two cartesian coordinates, say $x,y$, then you can define the $x,y$ components of force by 
	\begin{equation*}
		\mathrm d \phi= \frac{\partial \phi}{\partial x} dx + \frac{\partial \phi}{\partial y} dy= F_x ~ dx + F_y ~dy 
	\end{equation*}
	If our coordinates were $r,\theta$, then the analogous force would be the covariant components of the same \emph{form}, in a different coordinate system:
	\begin{equation*}
		\mathrm d \phi = \frac{\partial \phi}{\partial \theta} d\theta + \frac{\partial \phi}{\partial r} dr = F_\theta ~ d\theta + F_r ~ dr
	\end{equation*}
	Note that the first component has units not of force, but of force times distance. It is precisely the torque that the potential induces. In this sense, quantities like torque are precisely just generalizations of force to non-cartesian coordinate systems (polar, in this case). The second component is just radial force, plain and simple.
	
	To hammer the point across: these two ``forces'' have components that mean completely different things, and cannot easily be compared. On the other hand, since $\mathrm d \phi$ is independent of coordinate system, we get:
	
	\begin{equation}
		\mathrm d \phi = F_x ~ dx + F_y ~ dy = F_\theta ~ d\theta + F_r ~ dr 
	\end{equation}
	
	Because we know how to go from $x,y$ to $r, \theta$ and because this nonlinear change of coordinates is \emph{linear} at every point on the differentials, this would allow us to go between the language of ``x-y force'' and the language of ``torque + radial force about the origin''  at any point.
	
	All forces (including the generalized forces, like torque) are covariant coefficients of the invariant differential form for work. If you're working in a coordinate system $q^i$, whether it be cartesian $x,y,z$ or polar $r, \theta$, then the coefficient corresponding to $dq^i$ is precisely the generalized force associated with that coordinate in your system.\\
	
	
	% \begin{concept}[One-Forms Relate Change to Direction]
	% 	For a function $\phi$, the one-form $\omega = \mathrm d \phi$ gives the first-order change in $\phi$ along a given direction $(dx^1,\dots, dx^n)$. In general, for a one-form $\omega$ that is not exact, $\omega$ along a given direction $(dx^1,\dots, dx^n)$ gives the change in a quantity that cannot be represented by a function of the coordinates. This occurs, for example, with non-conservative forces such as friction or when calculating heat added to a system.
	% \end{concept}
	
	
	\section{The Exterior Algebra and the Wedge} % (fold)
	\label{sec:the_exterior_algebra_and_the_wedge}
	
	If a $1$-form must be fed a vector of some associated change of coordinates $dq^i$, then what about a $2$ form? A 2-form, $\omega$, should associate a ``flux" out of a plane, so we need $\omega$ to be given a plane associated with \emph{two} directions $v^i, u^i$, and then $\omega$ acting on these two directions would give the associated ``flux'' out of the infinitesimal parallelogram gained by varying $q^i$ in both directions. So if $\omega$ were a 1-form, it would act on one vector as $\omega(\mathbf v)=\omega_i v^i$, but now $\omega$ as a 2-form acts on two vectors:
	\begin{equation*}
		\omega(\mathbf u, \mathbf v) \longleftrightarrow \text{Flux Coming out of $\mathbf u$ and $\mathbf v$s Parallelogram}
	\end{equation*}
	This is an intuitive \emph{geometric idea}. This is what we want to be true, and the following observations will be \emph{algebraic properties of $\omega$} based on our geometric notions of flux.\\
	\textbf{DRAW SOMETHING HERE}

	\begin{obs}
		$\omega(\mathbf v, \mathbf v) = 0$
	\end{obs}
	The parallelogram generated by $\mathbf v$ and itself has no second dimension, so it has no area. Therefore there isn't room for any flux to come out of it.
	\begin{obs}
		$\omega(2\mathbf u, \mathbf v) = 2 \omega(\mathbf u, \mathbf v)$ and $\omega(\mathbf u,2 \mathbf v) = 2 \omega(\mathbf u, \mathbf v)$.\\
		Moreover, in general $\omega(a \mathbf u, \mathbf v) = \omega(\mathbf u,a \mathbf v)= a \omega(\mathbf u, \mathbf v)$ for $a>0$.
	\end{obs}
	Of course, if we scaled the parallelogram by some positive amount $a$ along one of the sides, then its total area scales by $a$, so that $\omega$ gives $a$ times as much ``stuff'' coming out of the scaled parallelogram. This observation, together with our linear algebraic ideas, suggest that this should naturally extend beyond positive $a$ so that $\omega(-\mathbf u,\mathbf v) = -\omega(\mathbf u, \mathbf v)$, giving us negative flux through the parallelogram. But what does that mean? This means that if one of the vectors gets scaled negatively, the \emph{orientation} of the new parallelogram reverses. 
	\textbf{DRAW THIS HERE}
	
	So now even though the pair $-\mathbf u, \mathbf v$ are on the same plane, the notion of ``out" through their parallelogram has reversed. This makes physical sense, 
	\begin{concept}
		What matters, when finding the flux associated with $\omega$ along two directions $\mathbf u, \mathbf v$
		\begin{enumerate}
			\item The \emph{plane} that $\mathbf u, \mathbf v$ span, through which the flux is going
			\item That scaling $\mathbf u$ or $\mathbf v$ also scales the flux.
			\item The \emph{orientation} for which direction is in and which is out.
		\end{enumerate}
	\end{concept}
	This is exactly what we've seen before with the \textbf{Right-Hand Rule}\index{Right-Hand Rule}. Because of this, and from what we've seen before with objects like cross products, we would \emph{expect} that $\omega(\mathbf u, \mathbf v)$ represents one orientation, and $\omega(\mathbf v, \mathbf u)$ represents the opposite one so that 
	 \begin{equation*}
	 	\omega(\mathbf u, \mathbf v) = -\omega(\mathbf v, \mathbf u)
	 \end{equation*}
	we can add this as another property \emph{but} we can instead actually prove it if we make just one more geometric observation
	\begin{obs}
		$\omega(\mathbf u+\mathbf v, \mathbf w) = \omega(\mathbf u, \mathbf w) + \omega(\mathbf v, \mathbf w)$
	\end{obs}
	If $\mathbf u = a \mathbf v$ are linearly dependent then this is just the scaling observation. If $\mathbf v$ is linearly dependent with $\mathbf w, \mathbf u = a \mathbf w$, then this is just the observation that the parallelogram of $(\mathbf u, \mathbf w)$ would have the same amount of associated area if you were to add vectors in the direction of $\mathbf w$ to $\mathbf u$ so $\omega(\mathbf u + a \mathbf w, \mathbf w) = \omega(\mathbf u, \mathbf w)$. It's the same idea if $\mathbf u$ is linearly dependent with $\mathbf w$. Now let's assume all vectors are linearly independent. Geometrically, the two planes associated with $(\mathbf u,\mathbf w)$ and $(\mathbf v,\mathbf w)$, and the plane associated with the sum $(\mathbf u + \mathbf v, \mathbf w)$ can look like:
	
	\textbf{FIGURES TO ILLUSTRATE MY GEOMETRIC IDEA, DISCUSS WITH AARON}

	Now $\omega$ represents a constant flux in space. Enclosing a region by these three planes should mean that the flux that goes through the first two is the flux that comes out of the third.
	
	The same exact argument can be applied to show this holds for the second slot in $\omega(-,-)$.
	\begin{cor}
		The above observations imply that $\omega$ is linear and \emph{\textbf{antisymmetric}} in its arguments so that $\omega(\mathbf u, \mathbf v) = -\omega(\mathbf v, \mathbf u)$. That is, as we expected, reversing the order of $\mathbf u$ and $\mathbf v$ reverses the orientation of the plane.
	\end{cor}
	\begin{proof}
		We have shown that $\omega$ is compatible with scaling and addition in each argument, so it is linear in both.
		
		Now consider $\omega(\mathbf u + \mathbf v, \mathbf u + \mathbf v)$. By our first observation, such a parallelogram has no area, so this is zero. On the other hand, by the linearity of $\omega$ in both arguments:
		\begin{align*}
			0 &= \omega(\mathbf u, \mathbf u) + \omega(\mathbf u, \mathbf v) + \omega(\mathbf v, \mathbf u) + \omega( \mathbf v , \mathbf v) 
			\\
			&= \omega(\mathbf u, \mathbf v) + \omega( \mathbf v, \mathbf u) \\
			&\Rightarrow \omega(\mathbf u, \mathbf v) = - \omega(\mathbf v, \mathbf u)
		\end{align*}
	\end{proof}
	
	2-forms are bilinear operators that act on pairs of vectors that represent coordinate changes $(\mathbf u, \mathbf v)$. They associate a flux to a given plane defined by such coordinate changes. 
	In $\mathbb{R}^3$, the space of 1-forms is spanned by $dx, dy, dz$. We can also know the flux through any plane if we knew the flux on the $xy, yz$, and $zx$ planes, so our basis for our set of 2-forms should also be three dimensional. It is spanned by three elements that we will write as
	\begin{equation*}
		dx \wedge dy, ~ dy \wedge dz, ~ dx \wedge dz
	\end{equation*}
	The first element represents a flux of magnitude $|dx ~ dy|$ through the $xy$ plane an no flux through the other two. On the other hand $dy \wedge dx$ would represent a flux in the OTHER direction so that $dx \wedge dy = - dy \wedge dx$. We have invented something called the \textbf{wedge product}\index{Wedge Product}.
	\begin{align*}
		\text{1-form in $\mathbb R^3$} &= \omega_x dx + \omega_y dy + \omega_z dz\\
		\text{2-form in $\mathbb R^3$} 
		&=
		\omega_{xy} (dx \wedge dy) + \omega_{yz} (dy \wedge dz) + \omega_{xz} (dx \wedge dz) 
	\end{align*}
	
	
	In multivariable calculus, we would define planes, together with orientations, by specifying a normal vector to those planes. In a general dimension, we aren't able to associate a normal vector to a plane, so we should talk about the plane \emph{itself}. For this reason we define the \textbf{wedge product} 
	
	\begin{defn}[The Wedge Product]\label{def:wedge}
		The product $\wedge$ on a real vector space $V$ defined so as to satisfy the following properties
		\begin{itemize}
			\item $ \forall \mathbf a,\mathbf b, \mathbf c \in V, ~ (a \mathbf a + b \mathbf b) \wedge \mathbf c$ = $ a (\mathbf a \wedge \mathbf c) + b (\mathbf b \wedge \mathbf c)$
			\item $ \forall \mathbf a,\mathbf b, \mathbf c \in V, ~ \mathbf c \wedge (a \mathbf a + b \mathbf b) = a \mathbf c \wedge \mathbf b + b \mathbf c \wedge \mathbf a$
			\item $\forall \mathbf a\in V, ~ \mathbf a \wedge \mathbf a = 0$
		\end{itemize}
	\end{defn}
	As before, this last condition, together with bi-linearity, implies anti-symmetry of $\wedge$.
	
	In our case, when we have a basis $dq^i$ for our cotangent space of 1-forms, the basis for our space of 2-forms can be written as $dq^i \wedge dq^j$ for $i<j$. The 2-form $dq^3 \wedge dq^4$, for example, represents a flux of magnitude $|dq^3 dq^4|$ out of the $q^3q^4$ plane and no flux out of $q^iq^j$ planes for any other $i,j$. On the other hand $dq^4 \wedge dq^3$ would be the same flux in the opposite direction (and again, no flux out of any of the other $q^i q^j$ planes).
	
	It may be frustrating to see this new product without any previous background. Let's do an example of wedging two 1-forms in 3D: $\alpha$ and $\beta$, and see what happens.
	\begin{align*}
		\alpha \wedge \beta &=(\alpha_x dx + \alpha_y dy + \alpha_z dz) \wedge (\beta_x dx + \beta_y dy + \beta_z dz) \\
		\\
		& =  ~~~ \alpha_x \beta_x (dx \wedge dx) + \alpha_x \beta_y (dx \wedge dy) + \alpha _x \beta_z (dx \wedge dz) \\
		 & ~~~ + \alpha_y \beta_x (dy \wedge dx) + \alpha_y \beta_y (dy \wedge dy) + \alpha_y \beta_z (dy \wedge dz) \\
		 & ~~~ + \alpha_z \beta_x (dy \wedge dx) + \alpha_z \beta_y (dy \wedge dy) + \alpha_z \beta_z (dy \wedge dz)\\
		 \\
		&=(\alpha_x \beta_y - \alpha_y \beta_x)(dx \wedge dy)+ (\alpha_y \beta_z - \alpha_z \beta_y)(dy \wedge dz)\\
		&  ~~ +(\alpha_z \beta_x - \alpha_x \beta_z)(dz \wedge dx) 
	\end{align*}
	
	\textbf{OK AARON FORMAT THIS IDK HOW}
	but if we were back in multivariable calculus world, not caring about vectors and forms and writing everything in terms of $\hat{\mathbf{i}}, \hat{\mathbf{j}}, \hat{\mathbf{k}}$, and we identified 
	\begin{align*}
		&\hat{\mathbf{i}} \leftrightarrow dx, \hat{\mathbf{j}} \leftrightarrow dy, \hat{\mathbf{k}} \leftrightarrow dz 
	\end{align*}
	as well as
	\begin{align*}
		&\hat{\mathbf{i}} \leftrightarrow dy \wedge dz, \hat{\mathbf{j}} \leftrightarrow dz \wedge dx, \hat{\mathbf{k}} \leftrightarrow dx \wedge dy
	\end{align*}
	then the wedge product becomes \emph{exactly} the cross product. Why wedge then, when we already know the cross product? Because the cross product, going from vectors to vectors, only works in three dimensions. The wedge product taking us from 1-forms to 2-forms, is universally valid.
	
	Moreover, now we can go beyond just 2-forms and form higher wedges, $k$-forms\index{Differential Form!$k$-Form}. The wedge of two forms $\alpha \wedge \beta$ will always be linear in both arguments and antisymmetric. For example, we can make 3-forms. In 3-D, the space of 3-forms is one dimensional spanned by $dx \wedge dy \wedge dz$. Any other wedge of these differentials will either be the same, or a negative of this one. It corresponds to the one true infinitesimal 3D volume. What about the sign? It is the distinction between flow ``into'' the volume.
	
	\textbf{GRAPHIC of 2-form wedge a 1-form giving ``inward'' orientation, and then wedge the negative of that 1-form to give the ``outward'' orientation}
	
	This is more general, in $n$ dimensions the space of $n$-forms is one dimensional, spanned by the form $\bigwedge_{i=1}^n dq^i$. For some terminology:
	\begin{defn}[\textbf{Exterior Power}\index{Exterior!Power}] For a given vector space $V$, the vector space of of $k$ forms spanned by wedging elements of $V$ with themselves $k$ times is called the $k$th exterior power of $V$, and is denoted by $\Lambda^kV$.
	\end{defn}
	Our $V$ in this case is the cotangent space at a point $T^*_p M$: the space of 1-forms and $V = \Lambda^1 V$ always. The space of zero forms $\Lambda^0 (T^*_p M)$ at a point is the set of possible function values so is just $\mathbb R$ in our case, since we are working over the reals. The space of $k$ forms at $p$ is the $k$th exterior power of the cotangent space at $p$: $\Lambda^k T^*_p M$. If we consider all $k$-forms at $p$, then we get the \emph{exterior algebra} of the cotangent space at $p$. 
	\begin{defn}[\textbf{Exterior Algebra}\index{Exterior!Algebra}]
		The vector space of \emph{all} $k$-forms is called the exterior algebra of $V$ and is denoted $\Lambda V$.
	\end{defn}
	What about the tangent space of vectors? What does the exterior algebra mean there? If $dq^1 \wedge dq^2$ is the form that associates an oriented flux to the $q^1 q^2$ plane, then $\partial_1 \wedge \partial_2$ is the oriented plane \emph{itself}.
	
	In this way 
	\begin{equation*}
		(\text{Flux} ~ dq^1 \wedge dq^2)(\text{Coordinate Area} ~ \partial_1 \wedge \partial_2) = (\text{Flux}) (\text{Coordinate Area}).
	\end{equation*} 
	This is the invariant \emph{total flux} coming out from varying $dq^1$ and $dq^2$ together to sweep out a coordinate area. In general, $k$-wedges of vectors $v^i \partial_i$ represent the oriented $k$-volumes \emph{themselves}, on which $k$-forms act. In Einstein's convention, you can show that in for a general 2-forms and 2-vectors whose coordinates are doubly covariant and contravariant, respectively, we get the invariant value:
	\begin{align*}
		(\omega_{ab} dx^a \wedge dx^b) (v^{cd} \partial_c \wedge \partial_d) &= \omega_{ab} v^{cd} (\delta^{a}_c \delta^{b}_d - \delta^{a}_d \delta^{b}_c)\\
		& = \omega_{ij}v^{ij} - \omega_{ij}v^{ji}.
	\end{align*}
	\textbf{It is an exercise to generalize this, and also to check that it makes sense in 3D when our wedges are cross products and areas are normal vectors}
	
	As a last note, philosophically, \emph{where does this antisymmetry come from?} We've already seen it in the cross product, and now we have it in this wedge. Geometrically, what is happening? That the wedge product of a form with itself is zero is easy to understand: you cannot geometrically extend an object to higher dimensions without introducing new directions. Antisymmetry, on the other hand, is less obvious.
	
	 When we extend a geometric $k$-volume to a $k+1$-volume, there is a notion of orientation. Going from the line to the plane, we need to know ``which direction is out for flux?" Similarly, for the plane to the volume, we have basically the same question ``which direction is in/out for flux?'', and the antisymmetry of the wedge reflects that orientation will always exist for higher $k$ volumes.
	
	% section the_exterior_algebra_and_the_wedge (end)
	
	\section{Stokes' Theorem} % (fold)
	\label{sec:stokes_theorem}
	
	% section stokes_theorem (end)
	
	To go from a 0-form $f$ to a 1-form $\omega$, we applied the exterior derivative operator, which could just be written as:
	\begin{equation}
		\mathrm df = dq^i \frac{\partial f}{\partial q^i}.
	\end{equation}
	Going further, perhaps we could write the exterior derivative operator explicitly asa the invariant: 
	\begin{equation}\label{eq:exterior_derivative}
		\mathrm d = dq^i \frac{\partial}{\partial q^i}.
	\end{equation}
	We can view this $\mathrm d$ operator as a 1-form whose coefficients on each $dq^i$, rather than being numbers, are derivative operators $\partial/\partial q^i$ with respect to the corresponding coordinates. 
	
	Now for a 1-form $\omega = \omega_i dq^i$, we want the exterior derivative to take us to a 2-form. If this $\mathrm d$ operator can be thought of as a 1-form, the obvious way to go to a form one step higher is by wedging, meaning that we would define:
	\begin{equation}\label{eq:exterior_derivative2}
		\mathrm d \omega := (dq^i \frac{\partial}{\partial q^i}) \wedge \omega= dq^i \wedge \frac{\partial \omega}{\partial q^i}.
	\end{equation}
	Note that the only reason we did this is because of what the \emph{algebra} seemed to tell us to do, independent of any geometric intuition beforehand. This is powerful, but is this right? Is this the derivative operator that will generalize the gradient, divergence, curl, and \emph{everything else}?
	
	Let us first check that for a 1-form $\omega$ $\mathrm d \omega$ gives us Stokes' theorem, as we want:
	
	\begin{theorem}[Stokes' Theorem for 1-forms]
		If $\omega$ is a 1-form, then with $\partial$ defined as the boundary operator of a manifold and $\mathrm d$ defined as in Equation~\eqref{eq:exterior_derivative2}, we have
		\begin{equation*} 
			\int_\Omega \mathrm d \omega = \int_{\partial \Omega} \omega.
		\end{equation*}
	\end{theorem}
	\begin{proof}
		Take $\Omega$, and as in all of our proofs in the section Section~\ref{sec:the_derivative_and_the_boundary}, let us cut $\Omega$ into a mesh of infinitesimal parallelograms. If we integrate $\omega$ over the boundary $\partial \Omega$, this is the same as integrating $\omega$ over every single individual parallelogram on the interior, as \emph{BECAUSE OF ORIENTATION}, the integrals over the boundaries of these parallelograms will cancel between neighbors, leaving us with only the boundary, as always.
		
		It remains to show that for an arbitrary small parallelogram, Stokes' theorem holds. This parallelogram is obtained by varying $q^i$ along two vectors $u^i \partial_i$ and $v^i \partial_i$. After a suitable linear transformation of coordinates, we can assume WLOG that this parallelogram is obtained by changing $q^1$ by some fixed small amount $dq^1$ and $q^2$ by $dq^2$. Let's integrate $\omega$ on the boundary:
		
		Because the parallelogram is small, we can approximate these integrals as:
		\begin{equation*}
			(\partial_1 \omega_2 - \partial_2 \omega_1) dq^1 dq^2
		\end{equation*}
		On the other hand the exterior derivative is:
		\begin{align*}
			\mathrm d \omega 
			&= dq^i \wedge (\partial_i \omega) \\
			&= (dq^1 \wedge dq^2) \partial_1 \omega_2 + (dq^2 \wedge dq^1)  \partial_2 \omega_1 + \text{other} \\
			&= (\partial_1 \omega_2 - \partial_2 \omega_1) dq^1 \wedge dq^2 + \text{other}
		\end{align*}
		where the other terms involve wedges that aren't of $q^1,q^2$ and will vanish along integration of this specific parallelogram. Since integrating $(\partial_1 \omega_2 - \partial_2 \omega_1) dq^1 \wedge dq^2$ gives exactly $(\partial_1 \omega_2 - \partial_2 \omega_1) dq^1 dq^2$, we have proven it for this parallelogram, and by linearity and coordinate change for \emph{any} paralellogram. Because adding these parallelograms together forms the bulk of $\Omega$ and they cancel when integrated on neighboring boundaries (if one boundary is associated with $dq^i$, the neighboring one is associated with $-dq^i$), this gives the desired result. 
	\end{proof}
	\textbf{WE DONT NEED ANY OTHER COORDINATES}
	
	
	Note that if $q^i = (x,y,z)$ then this is exactly Stokes' theorem in 3-D for the curl! More than this, it generalizes Stokes' theorem in $\mathbb{R}^3$: for any 2-D surface, the circulation of $\omega$ over the boundary is exactly the sum total of the curl $\mathrm d \omega$ over the interior.
	
	From this let us prove what we set out to prove in the most general case:
	{
	\renewcommand{\thetheorem}{\ref{thm:GeneralStokes}}
	\begin{theorem}[General Stokes' Theorem]
		With $\partial$ defined as the boundary operator of a manifold and $\mathrm d$ defined as in Equation~\eqref{eq:exterior_derivative2}, we have for a general differential $k$-form $\omega$ that
		\begin{equation*} 
			\int_\Omega \mathrm d \omega = \int_{\partial \Omega} \omega.
		\end{equation*}
	\end{theorem}
	\addtocounter{theorem}{-1}
	}
	First, a lemma:
	
	\begin{lemma}[Restriction of a Form]
		If $\omega$ is a $k$ form of $n$ variables that is being integrated over some $k$-dimensional manifold associated with changing only the first $k$ variables, then for the integration, we can work with the restricted $\omega_{\text{res}}$ associated with setting the last $n-k$ $dq^i$ equal to zero, and eliminating any wedge terms holding those $dq^i$.
	\end{lemma}
	\begin{proof}
		This follows from
	\end{proof}
	\textbf{IM NOT SURE WE NEED THIS NOW}
	
	Now to prove the General Stokes' Theorem:
	\begin{proof}
		For a given manifold $\Omega$, divide it's bulk into $k$-volumes that are generalizations of parallelograms to $k$-dimensions. Just like a line's boundary has two points, a parallelogram's has 4 lines, a parallelepiped's has $6$ parallelograms, a $k$-volume's boundary is $2k$ $k-1$ volumes. Again, we can set up local coordinates so that this $k$ volume has each $k-1$ obtained by holding some $q^i$ constant and letting the others vary. For each $q^i$ there are exactly two opposing $k-1$ volumes obtained by holding that coordinate constant, and they have opposite orientation.
		
		Now let us perform the integration of $\omega$ over the $k-1$ boundaries. In terms of these coordinates, $\omega$ can be written as a linear combination of wedges of $k-1$ of the $dq^i$, meaning that each such wedge misses exactly one $dq^i$.
	\end{proof}
	\textbf{THE PROBLEM WITH THE ABOVE IS THERE COULD BE $n$ $q^i$ and }
	\begin{prop}
		$\mathrm d^2 = 0$
	\end{prop}
	\begin{proof}
		For a general form $\omega$, consider
		\begin{align*}
			\mathrm d (\mathrm d\omega) &= (dq^j \partial_j)\wedge (dq^i \partial_i \omega) \\
					& = dq^j \wedge dq^i (\partial_j \partial_i \omega)
		\end{align*}
		This is a summation over $i$ and $j$ running from $1$ to $n$. Now pick any specific term in the sum with \emph{specific} indices $a, b$. This corresponds to a term:
		\begin{equation*}
			dq^a \wedge dq^b (\partial_{a} \partial_{b} \omega)
		\end{equation*}
		in the sum. We can assume $a \neq b$ as otherwise that'd mean $dq^a \wedge dq^b = 0$.
		But that means we will have another distinct term with those indices reversed $(b,a)$ equal to
		\begin{equation*}
			dq^b \wedge dq^a (\partial_{b} \partial_{a} \omega)
		\end{equation*}
		Since partials commute but the wedge products anti-commute, this $(b,a)$ term is equal to the \emph{negative} of that first $(a,b)$ term, meaning they will cancel. The whole double-sum will then become a sum of cancelling terms, giving zero.
	\end{proof}
	\begin{cor}
		$\partial^2 = 0$: The boundary of a boundary is nothing. 
	\end{cor}
	\begin{proof}
		For any $k$-form $\omega$, assume we are integrating a form $\mathrm d \omega$ on a $(k+1)$-boundary. By Stokes' Theorem (twice):
		\begin{equation*}
			\int_{\partial^2 \Omega} \omega = \int_{\partial \Omega} \mathrm d \omega = \int_{\Omega} \mathrm d^2 \omega = \int_\Omega 0 = 0.
		\end{equation*}
		Since $\omega$ was any arbitrary form, we must have $\partial^2 \Omega = 0$.
	\end{proof}
	This is an amazing geometric fact that we have gotten, via Stokes' theorem, from the \emph{purely algebraically derived} fact that $\mathrm d^2 = 0$. The duality between forms and the manifolds we integrate them over is a gorgeous duality between algebra and geometry that extends very deeply and profoundlys (c.f. Hodge Theory \textbf{INSERT TEXTS HERE}).
	
	We have already seen that differential forms that are the exterior derivatives $\mathrm d \omega$ of some other form $\omega$ are called exact. We know exact 1-forms correspond to conservative vector fields. \textbf{It will be an exercise to show} exact 2-forms in $\mathbb R^3$ correspond to solenoidal vector fields. On the other hand, forms $\omega$ that have $\mathrm d \omega = 0$ are called \textbf{closed}\index{Differential Form!Closed}. Why this language? It is taken for the corresponding geometric language for the boundary operator. If a region has no boundary, it is called closed, so if a form has zero exterior derivative, it will also be called closed. Clearly exact forms are closed, since $\mathrm d^2 = 0$, but are \emph{all} the closed forms exact? In $\mathbf{R}^3$, the answer is yes, but consider this:
	
	\begin{example}
		$d\theta$ is a closed form defined on the punctured plane that is not exact. 
	\end{example}
	
	We know $\mathrm d (d\theta) = 0$, so it is indeed closed. Although locally, a function $\theta$ (that this form represents the change of) can be defined just by calculating the angle from the $x$ axis, if you go around counterclockwise in a circle containing the origin, then $\theta$ continuously increases. At the end of the revolution, even though you are at the same point, $\theta$ has increased by $2\pi$. So although $d\theta$ makes sense locally as a differential form everywhere in the plane minus the origin, we cannot define a global smooth function representing $\theta$ without a discontinuity. The existence of a closed form that is not exact happens because the manifold on which $d\theta$ is defined is \emph{not} $\mathbb R^2$, but is instead defined on $\mathbb{R}^2$ without the origin (where $d\theta$ would not be well-defined). This change in global geometric structure gives rise to these interesting closed, inexact forms. 
	
	The study of the closed forms that are not exact on a manifold is called the \textbf{De-Rham cohomology}\index{De-Rham Cohomology} of a manifold. \\
	
	\textbf{Insert Graphic Here}
	
	
	\section{Distance, a Metric}	
	
	\index{Metric Tensor|(}\index{Tensor!Metric|(}\index{Manifold!Metric|(}
	
	
	\emph{Why have we been making such a difference between vectors and forms?! In multivariable calculus I could have done all the 3-D calculation working only with my regular vector fields using $\hat{\mathbf{i}}$ and the rest. Why can't I just turn these forms into vector fields?}.\\
	
	These are the types of questions you might ask, were this book written in dialogue. It's a fair point. For some reason, in multivariable calculus, there seemed to be no problem just using the language of vector fields for gradients, curls, and integration. What are all these forms doing here? The reason, as we've said before, is because we've no longer assumed that we were working in an ``orthogonal reference frame''. In fact, up until now we have been working in vector spaces of tangent vectors and forms that had \emph{no notion of distance whatsoever}. Accordingly, we made no assumptions that our manifolds themselves had any notion of distance and length either. Even though we talked about wedge products as associated with area, it was not true area of euclidean space but merely a (contravariant) number obtained by multiplying the changes of coordinates $dq^i$ together. 
	
	
	But we know that in the physical world there \emph{is} a notion of length. It is not just affine space. There is in fact a notion of \emph{being perpendicular}. If we had this notion, something powerful happens. Every 1-form, as we know from the previous chapter, can be viewed of as local function behavior, and therefore visualized in terms of level curves of that function. If we had a notion of being perpendicular, then we have a unique direction \emph{perpendicular to the level curves of the first order behavior} along which $\omega$ increases. We can then associate a specific, invariant vector $\mathbf v = v^i \partial_i$ that is perpendicular to the level curves of $\omega$, and along which omega increases by exactly $|\mathbf v|^2$. 
	
	Conversely, if we have a notion of being perpendicular, then to a specific vector, we can pick a unique 1-form $\omega = \omega_i dx^i$ whose level curves are perpendicular to the direction of $\mathbf v$ and such that the increase of $\omega$ along $\mathbf v$ is $|\mathbf v|^2$.
	
	What we want is to associate an invariant \emph{physical length element} to a small local change of coordinates $v^i$ around any point. In 3-dimensional Euclidean space $\mathbf E^3$, with an orthogonal coordinate system $x,y,z$ we know how to obtain a physical change in length from the changes of the coordinates
	\begin{equation}
		ds^2 = dx^2 + dy^2 + dz^2
	\end{equation}
		%
	% Since such a change is a vector, to get something invariant the first guess might be to have a 1-form $\omega^*$ that takes in a direction $\mathbf v$ and associates the invariant magnitude  $\omega^* \mathbf v$ to it. But then if $\omega^* \mathbf v$ were positive then $\omega^* (-\mathbf v)$ would be negative, and we want distance to be a positive quantity.
	
	
	
	We are let by Pythogoras: In euclidean space $\mathbb E^n$, for an orthogonal frame, the length associated with a change of coordinates is 
	\begin{equation}
		|x| = \sqrt{\sum x_i^2} \Rightarrow |x|^2 = x_i x_i
	\end{equation}
	where we've used lower indices for coordinates because in an orthogonal frame there's no big deal between covariance and contravariance. More generally, in an orthogonal frame, $\mathbf E^n$ has a dot product producing a scalar $\mathbf x \cdot \mathbf y = x_i y_i$. 
	In general coordinates, though, we've already seen $v^i v^i$ is not invariant just by noting the two upper indices, and also by analyzing how it trasforms directly in Section~\ref{sec:the_notion_of_length_on_vector_spaces}. 
	
	We want to define an \emph{invariant} \textbf{inner product}\index{Inner Product} $\left< \mathbf v | \mathbf w \right>$ on our tangent space, bilinear in both arguments, just like the one in $\mathbf E^n$. Then we will write:
	\begin{equation}
		\begin{aligned}
			\left< \mathbf v | \mathbf w \right> &= \left< v^i \partial_i | w^j \partial_j \right> \\
			&= v^i w^i \left< \partial_i | \partial_j \right>
		\end{aligned}
	\end{equation}
	This object, $\left< \partial_i | \partial_j \right>$, is the inner product between just the basis vectors themselves. Its double co-variance cancels out the double contra-variance of the coordinates themselves to give an invariant quantity to this inner product. In an orthonormal frame, since the vectors are orthonormal we would have $\left< \partial_i | \partial_j \right> = \delta_{ij}$ so that the inner product $\delta_{ij} v^i w^j$ is exactly the same as the dot product we are used to. 
	
	Now we can get the length of a vector:
	\begin{equation}
		|\mathbf v|^2 = \left< \mathbf v | \mathbf v \right> = v^i v^j \left< \partial_i | \partial_j \right> 
	\end{equation}
	in general, for an \emph{arbitrary} change of coordinates $dq^i$, we can finally associate a \emph{physical length} in space by:
	\begin{equation}
		ds^2 = dq^i dq^j  \left< \partial_i | \partial_j \right> 
	\end{equation}
	Because of the significance of this object $\left< \partial_i | \partial_j \right>$, we will them by $g_{ij}$. All together, this invariant inner product that takes in two coordinate changes:
	\begin{equation}
		ds^2 = g_{ij} {(dq^1)}^i {(dq^2)}^j 
	\end{equation}
	will be called the \textbf{metric} on our tangent space. 
	
	At a very far glance, you might ask if this thing is some sort of 2-form, but it certainly is not. There are no wedges between the differentials, and in fact since the dot product (which is the inner product in orthonormal basis) is symmetric, $\mathbf u \cdot \mathbf v = \mathbf v \cdot \mathbf u$, this holds true in general, so in fact $g_{ij} = g_{ji}$ is symmetric as a matrix. This is a different type of product of 1-forms, that will be discussed in the next section. For now, it is the tool by which we can associate a length with a coordinate change $dq^i$, and how we can take scalar products of two vectors together. 
	
	To avoid abstraction without geometric concreteness, let's do a simple but nontrivial example. In cartesian coordinates on Euclidean space, $g_{ij} = \delta_{ij}$, but in polar coordinates, we now have $\partial_r, \partial_\theta$. At each point, these vectors are orthogonal, and $\partial_r$ corresponds to a unit increase in $r$, meaning that it has magnitude $1$. On the other hand, $\partial_\theta$ corresponds to a unit increase in theta. Increasing $\theta$ by $d\theta$ on a circle of radius $r$ gives an infinitesimal length of $r d\theta$. The total line element then is
	\begin{equation}
		ds^2 = dr^2 + r^2 d\theta^2 \Rightarrow g_{ij} = 
		\begin{pmatrix}
			1 & 0 \\
			0 & r^2 
		\end{pmatrix}.
	\end{equation}
	At every point, this gives the \emph{physical} length associated with any local infinitesimal change of coordinates $(dr, d\theta) = v^i$ that form the components of a vector.
	
	Immediately, being able to associate an invariant length to a tangent vector on our space allows us to be able to define the length of any curve on our manifold. For a curve $\gamma(t) \in M$ parameterized by $t$. For a coordinate patch, we get $q^i = \varphi^i(\gamma)$. On each tangent space we have the tangent vector to $\gamma$:
	\begin{equation}
		\mathbf v_{\gamma,t} = \frac{d}{dt} \left[ \varphi^i(\gamma(t)) \right] \frac{\partial}{\partial q^i}
	\end{equation}
	for which we can now calculate an associated length: 
	\begin{equation}
		|\mathbf v_{\gamma,t}|^2 = g_{ij} ~ \dot \varphi^i(\gamma) \dot \varphi^j(\gamma).
	\end{equation}
	\begin{prop}[The Metric Defines the Length of Curves]
		The length of a curve $\gamma$ parameterized by $t$ on a manifold $M$ with metric $g$ is
		\begin{equation}
			\int_{t_i}^{t_f} \sqrt{g_{ij} ~ \dot \varphi^i(\gamma(t)) \dot \varphi^j(\gamma(t))} ~ dt
		\end{equation}
		and can be more clearly seen as
		\begin{equation}
			\int_{\gamma} \sqrt{g_{ij} \frac{dq^i}{dt} \frac{dq^j}{dt}} dt = \int_{\gamma} \sqrt{g_{ij} dq^i dq^j}
		\end{equation}
		where $dq^i$ is the infinitesimal coordinate change along the curve at that point.
	\end{prop}
	We said that the integrals over curves should be one-forms, but the integrand doesn't look like it is. Note, however, that $\sqrt{g_{ij} dq^i dq^j}$ is exactly the ``square root of a 1-form product'', which could be viewed as the ``norm'' of a 1-form in some sense. This object is not a 1-form, it is a \textbf{measure}\index{Measure}. The difference between the two is that while a 1-form will give a negative of its original value if integrated in the opposite direction of the curve, a measure will \emph{always} associate a positive value to any region it is integrated over. These are centrally important to calculating positive geometric invariants like length, volume, etc. 
	
	In the polar example, this automatically gives us the equation for arc-length:
	\begin{equation}
		\int_{\gamma} \sqrt{dr^2 + r^2 d\theta^2} 
		= \int_{t_i}^{t_f} \sqrt{\left( \frac{dr}{dt}\right)^2+ r^2 \left( \frac{d\theta}{dt} \right)^2 } dt
	\end{equation}
	and indeed, once $g$ is known, we can find the equation for arc length in \emph{any} coordinate system. \textbf{THERE WILL BE EXERCISES ON THIS}.
	
	
	Now note, the metric tensor was not something that we had before on a general manifold $M$. It is added information. A manifold doesn't need a metric or any notion of distance to be well-defined. After all, its definition is just topological. When $M$ is endowed with with a metric $g$ at every tangent space $T_p M$, we say it is a \textbf{Riemannian manifold}\index{Manifold!Riemannian}. 
	\begin{defn}[Riemannian Manifold]
		A Riemannian manifold is a smooth manifold $M$ with a positive definite bilinear form $g(\mathbf u, \mathbf v)$ on each tangent space. This form is called a \emph{\textbf{metric}} or a \emph{\textbf{Riemannian metric}}.
		
		$g$ is positive definite in the sense that $g(\mathbf u, \mathbf u) \geq 0$ always, and equals zero iff $\mathbf u = 0$. This defines our positive-definite inner product. 
	\end{defn}
	This condition ensures that every nonzero vector has positive length. Now, endowed with a metric, we can measure lengths of curves, and much more. 
	\begin{prop}[The Metric Defines Angles]
		If two curves $\gamma_1, \gamma_2$ intersect at a point $p$ on a Riemannian manifold, the angle between them can be calculated in terms of their respective tangent vectors, $\mathbf u, \mathbf v$ by
		\begin{equation}
			\cos(\theta)=\frac{g_{ij} u^i v^i}{|u| |v|}
		\end{equation}
	\end{prop}
	This is the obvious analogue of the Euclidean definition, now we've just extended the dot product of vectors to general coordinates. This is how we will \emph{define} the notion of angle in a manifold. Note under general linear transformations, angles between two vectors can appear sheared and stretched. It is the \emph{metric} that holds us steady and tells us ``this frame is orthogonal, this one is not, it's distorted and so the dot product here should be $g_{ij}$ instead to recover the right notion of angle''.
	
	It is also worth noting a simple fact
	\begin{lemma}
		The metric tensor $g_{ij}$ is invertible as a matrix.
	\end{lemma}
	\begin{proof}
		Assume $g$ isn't invertible. Then there is a nontrivial null space for $g$. Then there exists a nonzero vector $\mathbf u$ with $g \mathbf u = 0$, but that means $g(\mathbf u, \mathbf u) = 0$ without $u=0$, contradicting the positive definiteness of the metric.
	\end{proof}
	Now since a metric gives us an inner product, to each vector $\mathbf v$ is associated a corresponding $\left< \mathbf v | - \right>$ operation that wants to take in a vector. But this is \emph{exactly} what a 1-form is. We then have:
	\begin{prop}[Metric Defines an Isomorphism Between Vectors and Covectors]
		A given metric $g$ associates to each vector $\mathbf u$ a corresponding 1-form $\omega_\mathbf{u}$ in the sense that for all vectors, $\mathbf v$, $\omega_\mathbf u$ acts according to inner product
		\begin{equation}
			\omega_\mathbf{u} \mathbf v =  \left< \mathbf u | \mathbf v \right>, \omega = g \mathbf u
		\end{equation}
		In coordinate notation, we have that for $\mathbf u = u^i \partial_i$ give rise to 
		\begin{equation}
			(\omega_\mathbf u)_i dx^i = (g_{ij} dx^i dx^j)(u^j \partial_j) = (g_{ij} u^j) dx^i
		\end{equation}
	\end{prop}
	so the tuple of coefficients for the form corresponding is the matrix $g_{ij}$ multiplying the tuple $u^j$ for the vector (note we could have had it multiplying $u^i$ since $g_{ij}$ is symmetric so we can switch $i$ and $j$ above). 
	
	Of course, in an orthonormal frame, where $g_{ij}=\delta_{ij}$ is just the identity, this means that the form corresponding to $\partial_i$ is just $dx^i$ and things are simple. It is for this reason that forms and vectors have been interchangeable in our orthonormal frames with trivial metric. In any interesting coordinate system, however, this breaks immediately. 
	
	Note, then, that if $g \mathbf v = \omega$ gives a form associated to a vector, and this is just a matrix-vector equation then we would expect that 
	\begin{equation}
		g^{-1} \omega = \mathbf v
	\end{equation}
	gives a \emph{vector} associated to every 1-form. In index notation
	\begin{equation}
		(g^{-1})^{ij} \omega_j = v^i
	\end{equation}
	For this reason, $g$ and its inverse\footnote{Note that often $(g^{-1})^{ij}$ is often just written as $g^{ij}$ in the literature simply because upper indices automatically mean that we have inverted the covariant $g_{ij}$, and so no confusion ensues.} are said to ``raise and lower indices''. This inverse $g$ then gives us an inner product on the cotangent space:
	\begin{equation}
		\left< \alpha | \beta \right> = g^{ij} \alpha_i \beta_j
	\end{equation} 
	its easy (almost immediate) to check that the inner product of 1-forms gives the same number as the inner product of their corresponding vectors.
	
	Mathematicians, who often write vectors and forms just as $\mathbf v$ and $\omega$, without ever resorting to index notation have invented a very creative notation for the way that $g$ associates vectors to their 1-forms.
	\begin{prop}[Musical Isomorphisms]
		At each point $p$ on a Riemannian manifold $M$, the metric $g$ induces an isomorphism between the tangent space $T_pM$ and the cotangent space $T^*_pM$:
		\begin{equation}
			\flat: T_pM \rightarrow T^*_pM
		\end{equation}
		so that for each vector $\mathbf v \in T_pM$, $\omega = \mathbf v^\flat$ is the associated 1-form. The inverse of the metric gives the opposite direction
		\begin{equation}
			\sharp: T^*_pM \rightarrow T_pM
		\end{equation}
		so that $\mathbf v = \omega^\sharp$ is the associated vector to the 1-form $\omega$.
	\end{prop}
	\textbf{LOL DRAW A DIAGRAM OF THIS}

	As mentioned in the beginning of this section, since the metric gives the notion of ``orthogonality'' to the tangent space, geometrically the form corresponding to a vector $\mathbf v$ is the local linear behavior of a function with level curves perpendicular to $\mathbf v$, and increasing in the direction of $\mathbf v$ so that $\omega_{\mathbf v}(\mathbf v) = |\mathbf v|^2$. $\mathbf v$ would then be exactly the \emph{gradient vector} to this local behavior, pointing in the direction of greatest ascent. 
	
		This means we can define the gradient\index{Gradient Operator} vector to a function, finally.
		\begin{defn}[The Gradient Vector]
			For a function $f$, we define $\nabla f$ as the vector field 
			\begin{equation}
				\nabla f = (\mathrm df)^\sharp.
			\end{equation}
			In a coordinate system $q^i$ this is:
			\begin{equation}
				(\nabla f)^k \partial_k = \left(g^{ij} \frac{\partial f}{\partial q^j} \right) \partial_i
			\end{equation}
		\end{defn}
		That is, we take the form corresponding to the first-order behavior of $f$ and because of the metric we now have the vector field corresponding to the \emph{direction of greatest ascent}.
		Comparing components, we have that 
		\begin{equation}
			\nabla^i f = g^{ij} (\partial_j f) \Rightarrow \nabla^i = g^{ij} \partial_j
		\end{equation}
		So the gradient operator is the ``contravariant" version of the partial derivative. Often it is just written as the ``raised partial" $\partial^i$ to avoid confusion with what $\nabla$ will denote in later sections. The powerful thing about knowing the metric is that it allows us to define the gradient operator in \emph{any} coordinate system. In the polar example, this is
		\begin{align*}
			g^{ij} = 
			\begin{pmatrix}
				1 & 0 \\
				0 & 1/r^2 
			\end{pmatrix}
			\Rightarrow \nabla f = \frac{\partial f}{\partial r} \partial_r + \frac{1}{r^2} \frac{\partial f}{\partial \theta} \partial_\theta.
		\end{align*}
		Often, engineers want to define this in terms of the ``normalized basis vectors'' of length 1: $\hat {\mathbf r} = \partial_r$, $\hat \theta = \partial_\theta/r$ so that 
		\begin{equation*}
			\nabla f = \frac{\partial f}{\partial r} \hat{\mathbf r} + \frac{1}{r} \frac{\partial f}{\partial \theta} \hat{\mathbf \theta}.
		\end{equation*}
		
	
	\index{Metric Tensor|)}\index{Tensor!Metric|)}\index{Manifold!Metric|)}

	\section[Multilinear Algebra: $\oplus, \otimes$ and Tensors]{Multilinear Algebra: $\oplus, \otimes$ and\\ Tensors}
	
	\index{Tensor|(}
	
	\textbf{INSERT SOME QUOTE ABOUT HOW LIKE 97\% OF MATH IS JUST TERMINOLOGY}
	
	Before moving on to completing the study of differential geometry, it is worth reflecting on the mathematical machinery that we have developed. 
	
	We began differential geometry seriously in Section~\ref{sec:vectors_reimagined} by noting that there were two first order behaviors that were dual: vectors, which represented the first order behavior of curves and gave rise to direction, and forms, which represented the first order behavior of functions, and represented an operator that took a direction and gave us an associated change.
	
	Both sets of first order behavior had the property of being vector spaces: their elements could be added together and scaled. From this we defined the tangent and cotangent spaces. If we changed our coordinate system $q^i$, on both spaces this induced a linear transformation (since nonlinear coordinate transforms are locally linear, c.f. Section~\ref{sec:nonlinear_coordinate_systems_are_locally_linear}) on the  \emph{components} of the vectors. The tangent space would have its components transform one way and the cotangent space would have the components transform the other way. As a result, these two spaces were not naturally isomorphic.
	
	With the introduction of the metric $g_{ij}$, whose components are ``doubly'' covariant, we get a linear map that lets us go from the contra-variant components $v^i$ of a vector to the covariant $\omega_i$ of a form. This also gives us an inner product. 
	
	While $v_i$ and $\omega_i$ could both be represented by a tuple, a list of numbers that changed either with or against the basis $\partial_i$ when we changed coordinates, $g_{ij}$ is represented by a \emph{matrix} of numbers. In fact this matrix is symmetric. The metric, $g$, is and object that's neither a vector nor a covector but something ``higher''.
	% OH MASTERRRRRR 
	% SHOW ME SOMETHING HIGHGHGHGHGHGHEERRRR
	
	We will work towards making intuitive and rigorous the concept of a \textbf{tensor}, which generalizes that of a vector and covector from before:
	\begin{concept}[Tensor]
		A tensor is a physical quantity that can be represented by a multidimensional array of $k$ indices. Because the numbers within the tensor depend on the coordinate system, they will change (either co-variantly or contra-variantly). Components associated with some indices can be covariant, coordinates associated with other indices can be contravariant. 
	\end{concept}
	We've already seen before: a $k$-form, the wedge product of $k$ $1$-forms is a tensor with $k$ components. For a set of indices $i_k$ (which we will call a \textbf{multi-index}\index{Multi-Index}), we associate the flux that is going through the $k$-volume $dq^{i_1} \wedge dq^{i_2} \wedge \dots \wedge dq^{i_k}$. This tensor has the property that if we flip any two neighboring $i_k$, say $i_1$ with $i_2$, then the value at that new multi-index is the negative of the value of the old, because $\wedge$ is anti-symmetric. This tensor is thus called anti-symmetric: flipping neighboring indices flips the sign.
	
	In order to appreciate this all better, however, we need to go back and better our understanding of the language of vector spaces. \\
	
	Consider an $n$-dimensional vector space, $V$ with a basis $\{ \mathbf v_i \}$ and an $m$-dimensional vector space $W$ with basis $\{ \mathbf w_i \}$. 
	\begin{defn}
		The \emph{\textbf{direct sum}}\index{Direct Sum} $V \oplus W$ is the $n+m$-dimensional vector space of ordered pairs $(\mathbf v, \mathbf w)$ with $\mathbf v \in V$, $\mathbf w \in W$, with addition defined component-wise.
	\end{defn}
	
	\textbf{GRAPHIC: DIRECT SUM, R with R, and R with $\mathbf R^2$}
	
	The direct sum is one of the easiest concepts to grasp. It is exactly what it seems, at first glance, a addition of new directions to $V$. The only possible uncertainty is ``what if the two bases overlap?''. But when taking the direct sum, $V$ and $W$ are assumed to be entirely different vector spaces from the start, and we assume they share no elements. For this reason, we can take $\mathbb{R} \oplus \mathbb{R} = \mathbb{R}^2$ even though the two vector spaces are the same.
	
	The direct sum is simply the addition of new possibilities, new directions to $V$ by $W$. It is clearly commutative $V \oplus W = W \oplus V$. We can write a direct sum of the individual elements as:
	
	\begin{equation}
		\begin{pmatrix}
			v_1 \\ \vdots \\v_n 
		\end{pmatrix}
		\oplus
		\begin{pmatrix}
			w_1 \\ \vdots \\w_m
		\end{pmatrix}
		=
		\begin{pmatrix}
			v_1 \\ \vdots \\v_n \\
			w_1 \\ \vdots \\w_m
		\end{pmatrix}
	\end{equation}
	
	An element in the direct sum $V \oplus W$ corresponds uniquely to a vector in $V$ paired with a vector in $W$. If a given vector space can be written as a direct sum of two nontrivial sub-spaces, we say it can be \textbf{decomposed}\index{Direct Sum!Decomposition}. 
	\begin{example}
		A plane in $\mathbb R^3$ can be \textbf{decomposed} as a direct sum of any two distinct lines going through it. On the other hand $\mathbb R^3$ clearly cannot be decomposed into a direct sum of that plane and a line going through that same plane. Even though the plane and the line are isomorphic to $\mathbb R^2$ and $\mathbb R$, in context, since their embedding in this 3-space is linearly dependent, their combined span is not $\mathbb{R}^3$.
		
		On the other hand, there \emph{is} a way to pick a plane and a line so that their combined span is $\mathbb R^3$ so $\mathbb R^3$ \emph{is} decomposable into $\mathbb R^2$ and $\mathbb R$. It depends on \emph{which} $\mathbb R^2$ and \emph{which} $\mathbb R$ we choose inside $\mathbb R^3$.
	\end{example}
	
	The notion of a direct sum is so broad and general that it will be seen for the remainder of this book. It is the idea that $V$ and $W$ are two entirely different worlds, and $V \oplus W$ is their combination. 
	
	A very intuitive way to understand the direct product further comes, oddly enough, from a quantum mechanical-type language. In quantum mechanics, the various states that something can have are represented by vectors. Say something can be one of three shapes
	\begin{equation*}
		\{ \square, ~ \triangle,  ~ \bigcirc \}
	\end{equation*}
	Now in quantum mechanics we can mix states so something could look like $\frac{1}{\sqrt 2} \square + \frac{1}{\sqrt 2} \triangle$, so these form a vector space $V$ with vectors $a \square + b \triangle + c \bigcirc$. If we had another two states, say $\ddot \smile$ and $\ddot \frown$ that formed a space of vectors $a \ddot \smile + b \ddot \frown$ labelled $W$ then the direct sum $V \oplus W$ would be spanned by the basis:
	\begin{equation*}
		\{ \square, ~ \triangle,  ~ \bigcirc, ~ \ddot \smile, \ddot \frown \}
	\end{equation*}
	The idea of the direct sum should resemble exactly how you learned addition as a toddler.
	
	Within \emph{this} scheme, it is easy to explain the \textbf{direct product}\index{Tensor!Tensor Product}\index{Direct Product} also known as the \textbf{tensor product}. Say we have the same two vector spaces $V$ and $W$, spanned by $\{ \square, ~ \triangle,  ~ \bigcirc \}$ and $\{ \ddot \smile, \ddot \frown \}$, respectively. Then the tensor product $V \otimes W$ has a basis the represents all the states of a \emph{composite system} of an object that has a state in $V$ and another object that has a state in $W$:
	\begin{equation}
		\begin{Bmatrix}
		 \square \otimes \ddot \smile, & \triangle \otimes \ddot \smile,  & \bigcirc  \otimes \ddot \smile, \\
		\square \otimes \ddot \frown, & \triangle \otimes \ddot \frown,  & \bigcirc  \otimes \ddot \frown~	
		\end{Bmatrix}
	\end{equation}
	this is a completely different space from $V$ or $W$, and unlike the direct sum, neither of the original vector space are a part of this new space. The dimension of this tensor product space is the product of the two original dimensions.
	
	\textbf{IDK HOW TO FINISH THIS, BUT I WANNA MOTIVATE THE TENSOR PRODUCT INTUITIVELY AND WHY PRODUCTS OF CHANGES $dq^i \otimes dq^j$ cant just be represented in the direct sum by $(dq^i, dq^j)$}
	
	\textbf{Then maybe we wanna say: }

	\begin{obs}
		A metric $g$ is an element of the space $T^*_p M \otimes T^*_p M$
	\end{obs}

	Going back to studying transformation laws: if we change from $q^i$ to $q'^i$, then the basis vectors/covectors at a point $p$ change according to Equations~\eqref{eq:covariant_einstein},\eqref{eq:contravariant_einstein} and their components change in the respectively opposite ways depending on the Jacobian matrix $J = \frac{\partial q^j}{\partial q^i}$ and its inverse, $J^{-1} = \frac{\partial q^i}{\partial q^j}$ at $p$,
	\begin{equation}
		v'^i = \frac{\partial q'^i}{\partial q^j} v^j, ~~ \omega'_i = \frac{\partial q^j}{\partial q'^i} \omega_j.
	\end{equation}
	On the other hand, the metric tensor would change like:
	\begin{equation}
		g'_{ij} = \frac{\partial q^k}{\partial q'^i} \frac{\partial q^l}{\partial q'^j} g_{kl}
	\end{equation}
	That is, because the invariant length of a vector $g_{ij} v^i v^j$ involves the contra-variant components twice, $g$ must co-variantly change twice (once for each index) to keep that inner product invariant. We'd say that $g$ is a tensor with two covariant indices, or a $(0,2)$-tensor. 
	
	\begin{defn}[An $(a,b)$-Tensor]
		A tensor of $a$ contra-variant indices and $b$ covariant indices is called an $(a,b)$-tensor, and can be written in local coordinates at a point $p$ on $M$ as:
		\begin{equation*}
			T_{j_1 \dots j_b}^{i_1 \dots i_a}  \partial_{i_1} \otimes \dots \otimes \partial_{i_a} \otimes dx^{j_1} \otimes \dots \otimes dx^{j_b}  \in (T_p M)^{\otimes a} \otimes (T_p^* M)^{\otimes b} 
		\end{equation*}
	\end{defn}
	and we know how $T_{i_1, \dots, i_a}^{j_1, \dots, j_b}$ linearly transforms on each index. Moments like this are when we should be grateful to Einstein's summation convention for saving us $a+b$ summation signs out front. 

	
	\index{Tensor|)}

	\section{The Hodge Star and the Laplacian} % (fold)
	\label{sec:the_hodge_star_and_the_laplacian}
	
	Plan for this section:
	\begin{itemize}
		\item Get volume form Vol from the metric. 
		\item Then: We saw metric defines inner product on forms, this extends to the tensor products of forms as well (what does this mean physically?). Then this \emph{descends} down to the exterior algebra, giving us the ability to take products of k forms. We'll define $\star \alpha$ on a $k$ form to be the $n-k$ form so that $(\star \alpha) \wedge \beta = \left< \alpha | \beta \right> \mathrm{Vol}$
		\item Use this to define a divergence on vectors by
		\begin{equation}
			\text{div} ~ \mathbf v = \star \mathrm d \star (\mathbf v^\flat)
		\end{equation}
		\item Use that to get the Laplacian
		\begin{equation}
			\triangle f = \text{div} ~ \text{grad} ~ f 
			= \star \mathrm d \star \mathrm d f
			= (\star \mathrm d)^2 f
		\end{equation}
		\item Fun aside: from the metric giving us the Hodge star, we get a conjugated $\mathrm d$ operator that takes us UP from $k$ forms to $k-1$ forms:
		\begin{equation}
			\delta := \star \mathrm d \star
		\end{equation}
		so that $\triangle f = \delta \mathrm d f$, and on general forms: $\triangle = \delta \mathrm d + \mathrm d \delta$. 
		
		And like that, we've literally exhausted every human representation of the letter $D$ in this chapter: $\mathrm D, \mathrm d, d, \nabla, \partial, \delta, \triangle$ to mean some janky-ass derivative operator. 
		
	\end{itemize}
	 
	
	\textbf{GOTTA GET TO HERE}
	
	Let's go further and see what the metric gives us when we work with $k$-forms. For a manifold of dimension $n$, not only does the metric give us a notion of length, but it also gives us a \emph{volume form}. That is, for a coordinate system of $n$ $q^i$, we can associate a volume with a change $dq^i$. This form must be top-dimensional, i.e. it must have the maximal degree, $n$. 
	

	
	\begin{theorem}[Volume Form]
		On an oriented\footnote{This is a big subtlety. On manifolds like the Mobius band that are not oriented, it's not possible to say which way is out vs in. On local patches, we can always chose a volume form, but globally we may not be able to.} Riemannian manifold, the metric gives rise to the invariant volume form 
		\begin{equation}
			\mathrm{Vol} = \sqrt{|g|} ~ dq^1 \wedge \dots \wedge dq^n 
		\end{equation}
	\end{theorem}
	
	
	% section the_hodge_star_and_the_laplacian (end)

	\section{Movement, Lie's Ideas}

		To end this chapter, lets begin with something beautiful. 
		\begin{equation*}
			e^{i \pi} + 1 = 0
		\end{equation*}
		Hopefully this equation is familiar to the reader. More generally, though manipulating Taylor series in calculus class, it is known that
		\begin{equation}
			e^{i \theta} = \cos(\theta) + i \sin(\theta).
		\end{equation}
		
		Now from here lets talk about
		\begin{itemize}
			\item $e^{\partial_x}$ on the real line and what that means (its translation)
			\item $e^{\partial_i}$ in general is flow along a coordinate
			\item Vector fields can be exponentiated to give the flow along that field $e^{\mathbf v}$! WOWWW (and that is literally what the exponential map should intuitively mean).
			\item For coordinates, flow along $\partial_i + \partial_j$ is just $e^{\partial_i + \partial_j} = e^{\partial_i} e^{\partial_j}$. Not all fields are just coordinate fields. We don't have this nice property. What's going wrong?? 
			\item Obtain commutator from taylor expansion
			\item Explain commutator intuitively (I got a good idea for this)
			\item This is a derivative of one field along another without moving tangent spaces, so now we have $L_X Y$
			\item IMPORTANT POINT: NO NEED FOR A METRIC TO DEFINE THIS DERIVATIVE
		\end{itemize}
		
	\section{Exercises}
	
	% We denote the 2-form representing the infinitesimal area formed by one-forms $dx^i$ and $dx^j$ by $dx^i \wedge dx^j$. This is called the wedge product between $dx^i$ and $dx^j$.
%
%
% 	Note that it is not as easy as just defining the area to be $dx dy$, like a simple scalar. This two-form is a vector-like object. Indeed, the set of all two forms in some dimension form a vector space: we can add them, we can scale them by functions, and we have $0$ to be a trivial two form of no area.
%
% 	What properties does this wedge product have?
%
% 	\begin{prop}[Properties of $\wedge$]
% 		The wedge product\index{Wedge Product} satisfies:
% 		\begin{enumerate}
% 			\item $dx^i \wedge dx^i = 0$
% 			\item $(\alpha dx^i) \wedge dx^k = \alpha (dx^i \wedge dx^j)$
% 			\item $(dx^i + dx^j) \wedge dx^k = dx^i \wedge dx^k + dx^j \wedge dx^k$
% 		\end{enumerate}
% 	\end{prop}
%
% 	Three forms? Infinitesimal parallelepipeds. Past that, it gets difficult to visualize, but you get the idea. Moreover, the formalism does not change.
%
% 	\textbf{Talk about coordinate independence of the FTOC and now how we get it for the proof in the divergence theorem}
%
% 	\textbf{Example in 1-D, 2-D, and 3-D}
%
%
%
%
%
% 	If we have the vectors $\mathbf u, \mathbf v$ then there is an associated area between them. In multivariable calculus, to \emph{represent} this area, we would have taken the \textbf{cross product}\index{Cross Product}
% 	\begin{equation}
% 		\mathbf u \times \mathbf v = A \hat {\mathbf n}
% 	\end{equation}
% 	where $A$ is the area of the parallelogram they generate, and $\mathbf{n}$ defines the normal to that plane. This only worked in three dimensions, where we could take about ``normal vectors to planes'' instead of planes, because every 2-D plane defined just two unique 1D normal vectors (because 3 = 2 + 1).
% 	\begin{equation}
% 		\mathbf u \wedge \mathbf v
% 	\end{equation}
% 	to be not the normal vector to the plane spanned by $\mathbf u, \mathbf v$ but an area on the plane \emph{itself}. For this reason, just like with the cross product $\mathbf u \wedge \mathbf u = 0$, because we haven't introduced a new direction aside from $u$ to form a plane. Similarly just like the cross product, we want it to be linear in both arguments.
%
%
% 	\begin{cor}[Antisymmetry of $\wedge$]
% 		From the above properties, it follows that $\mathbf u \wedge \mathbf v = - \mathbf v \wedge \mathbf u$.
% 	\end{cor}
% 	\begin{proof}
% 		Consider $(\mathbf u + \mathbf v) \wedge (\mathbf u + \mathbf v)$. This is a vector wedged with itself, so is zero, on the other hand it expands out to:
% 		\begin{align*}
% 			0 &= \mathbf u \wedge \mathbf u + \mathbf u \wedge \mathbf v + \mathbf v \wedge \mathbf u + \mathbf v \wedge \mathbf v \\&= \mathbf u \wedge \mathbf v + \mathbf v \wedge \mathbf u \\
% 			&\Rightarrow \mathbf u \wedge \mathbf v = - \mathbf v \wedge \mathbf u
% 		\end{align*}
% 	\end{proof}
% 	What does this antisymmetry mean? We had it with the cross product as well, $\mathbf u \times \mathbf v = - \mathbf v \times \mathbf u$. This corresponded to the direction in which the normal vector pointed. It means that not only do we associate an area to the plane which $\mathbf u$ and $\mathbf v$ span, but we associate a \emph{signed area} to it, corresponding to an \textbf{orientation}\index{Orientation}. What does this mean? In multivariable calculus we asked ``How much of our vector field $\mathbf F$ is flowing through an infinitesimal parallelogram?'', but we need to know \emph{which side means ``out''}.
%
% 	So if we can take wedge products of elements in a vector space $V$, the resulting space of wedge products is denoted $\Lambda^2 V$ and is called the second \textbf{Exterior Power}\index{Exterior Algebra!Exterior Power} of $V$. The first exterior power is just $\Lambda^1 V = V$, and the zeroth is just the underlying $\Lambda^0 V = \mathbb{R}$.
%
% 	 If these coordinates are in the same direction, there
% 	\begin{equation}
% 		\omega = \omega_{ij} dx^i \wedge dx^j
% 	\end{equation}
%
% 	So now what is a 2-form? It is an object that is meant to be integrated over a 2D surface $\Omega$, but in multivariable calculus, the formula for $2$-dimensional integration was
% 	\begin{equation}
% 		\int_\Omega \mathbf F \cdot d\mathbf S
% 	\end{equation}
% 	this calculates the flux of $\mathbf F$ through the surface. The 2-form should then be $\omega = \mathbf F \cdot d\mathbf S$. Now $d\mathbf S$ represents an infinitesimal parallelogram obtained by varying, say, $dq^1$ and $dq^2$ independently.
%
% 	Say we are integrating a 1-form $\omega$ along a small line segment where (without loss of generality) $q^1$ is changing by some very small length $dq^1$. All other coordinates are held fixed, so $dq^i = 0 ~ \forall i \neq 1$. This infinitesimal line segment could be turned into a 2-D parallelogram by adding another \emph{different} $q^i$ into the mix, say (again without loss of generality) $q^2$. Then in multivariable calculus this would be an infinitesimal area element of size $dA = dq^1 dq^2$. This is what we want to do: we want to be able to multiply two one-forms to get a two-form that is meant to be integrated along areas.
%
% 	This multiplication is not just as straightforward as multiplying the differentials as if they were numbers $dx^i dx^j$ like we did when integrating. The differentials are elements of the vector space of one forms at each point. We want to multiply two vectors (that give values when given a specific direction or infinitesimal line segment $dx^i$) and obtain a new type of object that gives a value when given a specific \emph{plane element}, or infinitesimal parallelogram $dx^i, dx^j$. Extending a length in its own direction would not give rise to an area. We need to define a product $\wedge$ of 1-forms so that $dq^1 \wedge dq^1 = 0$
%
%
% 	When we visualize any vector space, whether physical or abstract algebraic, we can view each component as being an axis, and the vector as being a magnitude and direction along this space.
%
% 	%% Need to work on this
% 	A physical vector takes specific direction (a line passing through the origin) and associates to it a magnitude. The vector's information is the direction of the line, together with ``how far we go along this line". Algebraic vectors are anything that can be added and scaled. Physical vectors $v^i \partial/\partial q^i$ are examples of algebraic vectors, as are 1-forms. 1-forms can also be seen
% 	%%
%

	


